{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DQN for Space Invaders\n",
    "\n",
    "Minimal implementation of Deep Q-Network for ALE/SpaceInvaders-v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/space_invaders_basic_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari,accept-rom-license] ale-py torch scipy numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DQN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert frame to grayscale, resize to 84x84, and normalize\"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = np.dot(frame[..., :3], [0.299, 0.587, 0.114])\n",
    "    # Resize to 84x84\n",
    "    resized = zoom(gray, (84/210, 84/160), order=1)\n",
    "    # Normalize\n",
    "    normalized = resized / 255.0\n",
    "    return normalized.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.00025\n",
    "GAMMA = 0.99\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE = 1000\n",
    "N_FRAMES = 4\n",
    "N_EPISODES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment and Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\")\n",
    "n_actions = 6  # All 6 actions for Space Invaders\n",
    "\n",
    "# Networks\n",
    "policy_net = DQN((N_FRAMES, 84, 84), n_actions).to(device)\n",
    "target_net = DQN((N_FRAMES, 84, 84), n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "print(f\"Action space: {n_actions}\")\n",
    "print(f\"Policy Network initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon):\n",
    "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"Perform one step of optimization\"\"\"\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Next Q values from target network\n",
    "    next_q = target_net(next_states).max(1)[0].detach()\n",
    "    target_q = rewards + (1 - dones) * GAMMA * next_q\n",
    "    \n",
    "    # Loss\n",
    "    loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "steps = 0\n",
    "\n",
    "for episode in range(N_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    state = preprocess_frame(state)\n",
    "    state_stack = deque([state] * N_FRAMES, maxlen=N_FRAMES)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Epsilon decay\n",
    "        epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
    "                  np.exp(-1. * steps / EPSILON_DECAY)\n",
    "        \n",
    "        # Select action\n",
    "        state_array = np.array(state_stack)\n",
    "        action = select_action(state_array, epsilon)\n",
    "        \n",
    "        # Take step\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        next_state = preprocess_frame(next_state)\n",
    "        next_state_stack = state_stack.copy()\n",
    "        next_state_stack.append(next_state)\n",
    "        \n",
    "        # Store transition\n",
    "        replay_buffer.push(\n",
    "            np.array(state_stack),\n",
    "            action,\n",
    "            reward,\n",
    "            np.array(next_state_stack),\n",
    "            float(done)\n",
    "        )\n",
    "        \n",
    "        state_stack = next_state_stack\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        # Optimize\n",
    "        optimize_model()\n",
    "        \n",
    "        # Update target network\n",
    "        if steps % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward:.2f}, Avg Reward (100): {avg_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "env.close()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "\n",
    "# Calculate moving average\n",
    "window = 100\n",
    "moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, len(episode_rewards)), moving_avg, label=f'Moving Average ({window})', linewidth=2)\n",
    "\n",
    "plt.axhline(y=500, color='r', linestyle='--', label='Target (500)')\n",
    "plt.axhline(y=400, color='orange', linestyle='--', label='Minimum (400)')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('DQN Training Progress on Space Invaders')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "final_avg = np.mean(episode_rewards[-100:])\n",
    "print(f\"\\nFinal average reward (last 100 episodes): {final_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'policy_net_state_dict': policy_net.state_dict(),\n",
    "    'target_net_state_dict': target_net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'episode_rewards': episode_rewards,\n",
    "}, 'dqn_space_invaders_basic.pth')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
