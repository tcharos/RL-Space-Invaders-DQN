{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/advanced_dqn_space_invaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced DQN Variants for Space Invaders\n",
    "\n",
    "Implementation of DQN, Double DQN, Dueling DQN, and Prioritized Experience Replay for ALE/SpaceInvaders-v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari,accept-rom-license]\n",
    "!pip install ale-py\n",
    "!pip install torch scipy numpy psutil\n",
    "\n",
    "# Import and verify ALE is available\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Register ALE environments\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GDRIVE = False  # Set to True to enable Google Drive integration\n",
    "\n",
    "if USE_GDRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/DQN_SpaceInvaders_Checkpoints'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved locally to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Standard DQN Network\"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"Dueling DQN Network with separate value and advantage streams\"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        value = self.value_stream(conv_out)\n",
    "        advantage = self.advantage_stream(conv_out)\n",
    "        \n",
    "        # Combine value and advantage using the aggregation formula\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Standard Experience Replay Buffer\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized Experience Replay Buffer\"\"\"\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # How much prioritization to use (0 = uniform, 1 = full prioritization)\n",
    "        self.beta_start = beta_start  # Importance sampling weight\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.pos = 0\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        \"\"\"Linearly increase beta from beta_start to 1.0\"\"\"\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        N = len(self.buffer)\n",
    "        if N == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.pos]\n",
    "        \n",
    "        # Calculate sampling probabilities\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        \n",
    "        # Sample indices based on priorities\n",
    "        indices = np.random.choice(N, batch_size, p=probabilities, replace=False)\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (N * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        # Get samples\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        state, action, reward, next_state, done = zip(*samples)\n",
    "        \n",
    "        return np.array(state), action, reward, np.array(next_state), done, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions\"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert frame to grayscale, resize to 84x84, and normalize\"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = np.dot(frame[..., :3], [0.299, 0.587, 0.114])\n",
    "    # Resize to 84x84\n",
    "    resized = zoom(gray, (84/210, 84/160), order=1)\n",
    "    # Normalize\n",
    "    normalized = resized / 255.0\n",
    "    return normalized.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Configuration\n",
    "BASE_CONFIG = {\n",
    "    # Environment\n",
    "    'ENV_ID': 'ALE/SpaceInvaders-v5',\n",
    "    'SEED': 7,\n",
    "    \n",
    "    # Network\n",
    "    'N_FRAMES': 4,\n",
    "    'N_ACTIONS': 6,\n",
    "    \n",
    "    # Training\n",
    "    'N_EPISODES': 1000,\n",
    "    'LEARNING_RATE': 0.00025,\n",
    "    'GAMMA': 0.99,\n",
    "    'BATCH_SIZE': 32,\n",
    "    \n",
    "    # Exploration\n",
    "    'EPSILON_START': 1.0,\n",
    "    'EPSILON_END': 0.1,\n",
    "    'EPSILON_DECAY': 10000,\n",
    "    \n",
    "    # Memory\n",
    "    'BUFFER_SIZE': 10000,\n",
    "    'TARGET_UPDATE': 1000,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'CHECKPOINT_EVERY': 200,  # Save checkpoint every N episodes\n",
    "    'USE_GDRIVE': USE_GDRIVE,\n",
    "    'CHECKPOINT_DIR': CHECKPOINT_DIR,\n",
    "    \n",
    "    # DQN Type\n",
    "    'DQN_TYPE': 'DQN',  # Options: 'DQN', 'DoubleDQN', 'DuelingDQN'\n",
    "    'USE_PER': False,  # Use Prioritized Experience Replay\n",
    "    \n",
    "    # PER hyperparameters (if USE_PER=True)\n",
    "    'PER_ALPHA': 0.6,\n",
    "    'PER_BETA_START': 0.4,\n",
    "    'PER_BETA_FRAMES': 100000,\n",
    "    'PER_EPSILON': 1e-6  # Small constant to prevent zero priority\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, policy_net, n_actions, device):\n",
    "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "\n",
    "def optimize_model_dqn(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device):\n",
    "    \"\"\"Standard DQN optimization\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Next Q values from target network\n",
    "    next_q = target_net(next_states).max(1)[0].detach()\n",
    "    target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Loss\n",
    "    loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def optimize_model_double_dqn(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device):\n",
    "    \"\"\"Double DQN optimization\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Double DQN: use policy net to select actions, target net to evaluate them\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        next_q = target_net(next_states).gather(1, next_actions).squeeze()\n",
    "    \n",
    "    target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Loss\n",
    "    loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def optimize_model_per(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device, dqn_type='DQN'):\n",
    "    \"\"\"Optimization with Prioritized Experience Replay\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones, indices, weights = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    weights = torch.FloatTensor(weights).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    \n",
    "    # Calculate target Q values based on DQN type\n",
    "    with torch.no_grad():\n",
    "        if dqn_type == 'DoubleDQN':\n",
    "            next_actions = policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "            next_q = target_net(next_states).gather(1, next_actions).squeeze()\n",
    "        else:  # Standard DQN or Dueling DQN\n",
    "            next_q = target_net(next_states).max(1)[0]\n",
    "        \n",
    "        target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Calculate TD errors for priority update\n",
    "    td_errors = torch.abs(current_q - target_q).detach().cpu().numpy()\n",
    "    \n",
    "    # Weighted loss\n",
    "    loss = (weights * F.mse_loss(current_q, target_q, reduction='none')).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update priorities\n",
    "    new_priorities = td_errors + 1e-6  # Add small epsilon to prevent zero priority\n",
    "    replay_buffer.update_priorities(indices, new_priorities)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def print_config(config):\n",
    "    \"\"\"Print configuration in a formatted way\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  DQN TYPE: {config['DQN_TYPE']}\")\n",
    "    if config['USE_PER']:\n",
    "        print(f\"  Using Prioritized Experience Replay (PER)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"-\"*70)\n",
    "    for key, value in config.items():\n",
    "        if key not in ['CHECKPOINT_DIR']:  # Skip long paths\n",
    "            print(f\"  {key:20s}: {value}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "def save_checkpoint(config, policy_net, target_net, optimizer, episode, avg_score, \n",
    "                   episode_rewards, is_best=False):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dqn_type = config['DQN_TYPE']\n",
    "    per_suffix = \"_PER\" if config['USE_PER'] else \"\"\n",
    "    \n",
    "    if is_best:\n",
    "        filename = f\"{dqn_type}{per_suffix}_best.pth\"\n",
    "    else:\n",
    "        filename = f\"{dqn_type}{per_suffix}_ep{episode}_{timestamp}.pth\"\n",
    "    \n",
    "    filepath = os.path.join(config['CHECKPOINT_DIR'], filename)\n",
    "    \n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'config': config,\n",
    "        'policy_net_state_dict': policy_net.state_dict(),\n",
    "        'target_net_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'avg_score': avg_score,\n",
    "        'timestamp': timestamp\n",
    "    }, filepath)\n",
    "    \n",
    "    print(f\"Checkpoint saved: {filename}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(config, policy_net, target_net, optimizer, replay_buffer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generic DQN training function that works with all variants.\n",
    "    \n",
    "    Supports:\n",
    "    - Standard DQN\n",
    "    - Double DQN\n",
    "    - Dueling DQN\n",
    "    - Prioritized Experience Replay (PER)\n",
    "    \"\"\"\n",
    "    # Print configuration\n",
    "    print_config(config)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(config['ENV_ID'])\n",
    "    if config.get('SEED') is not None:\n",
    "        env.reset(seed=config['SEED'])\n",
    "    \n",
    "    n_actions = config['N_ACTIONS']\n",
    "    episode_rewards = []\n",
    "    steps = 0\n",
    "    best_avg_score = -float('inf')\n",
    "    \n",
    "    # Select optimization function based on config\n",
    "    if config['USE_PER']:\n",
    "        optimize_fn = lambda: optimize_model_per(\n",
    "            policy_net, target_net, optimizer, replay_buffer,\n",
    "            config['BATCH_SIZE'], config['GAMMA'], device, config['DQN_TYPE']\n",
    "        )\n",
    "    elif config['DQN_TYPE'] == 'DoubleDQN':\n",
    "        optimize_fn = lambda: optimize_model_double_dqn(\n",
    "            policy_net, target_net, optimizer, replay_buffer,\n",
    "            config['BATCH_SIZE'], config['GAMMA'], device\n",
    "        )\n",
    "    else:  # Standard DQN or Dueling DQN\n",
    "        optimize_fn = lambda: optimize_model_dqn(\n",
    "            policy_net, target_net, optimizer, replay_buffer,\n",
    "            config['BATCH_SIZE'], config['GAMMA'], device\n",
    "        )\n",
    "    \n",
    "    print(\"Starting training...\\n\")\n",
    "    \n",
    "    for episode in range(config['N_EPISODES']):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_frame(state)\n",
    "        state_stack = deque([state] * config['N_FRAMES'], maxlen=config['N_FRAMES'])\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon decay\n",
    "            epsilon = config['EPSILON_END'] + (config['EPSILON_START'] - config['EPSILON_END']) * \\\n",
    "                      np.exp(-1. * steps / config['EPSILON_DECAY'])\n",
    "            \n",
    "            # Select action\n",
    "            state_array = np.array(state_stack)\n",
    "            action = select_action(state_array, epsilon, policy_net, n_actions, device)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_state = preprocess_frame(next_state)\n",
    "            next_state_stack = state_stack.copy()\n",
    "            next_state_stack.append(next_state)\n",
    "            \n",
    "            # Store transition\n",
    "            replay_buffer.push(\n",
    "                np.array(state_stack),\n",
    "                action,\n",
    "                reward,\n",
    "                np.array(next_state_stack),\n",
    "                float(done)\n",
    "            )\n",
    "            \n",
    "            state_stack = next_state_stack\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Optimize\n",
    "            optimize_fn()\n",
    "            \n",
    "            # Update target network\n",
    "            if steps % config['TARGET_UPDATE'] == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Print progress every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            \n",
    "            # Get memory info\n",
    "            mem = psutil.virtual_memory()\n",
    "            gpu_mem = 0.0\n",
    "            gpu_mem_str = \"N/A\"\n",
    "            if device.type == 'cuda':\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_mem_str = f\"{gpu_mem:.2f}GB\"\n",
    "            elif device.type == 'mps':\n",
    "                gpu_mem_str = \"Active\"  # MPS doesn't expose memory stats\n",
    "            else:\n",
    "                gpu_mem_str = \"N/A\"\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
    "            \n",
    "            print(f'Episode {episode}\\tScore: {episode_reward:.1f}\\tAvg: {avg_score:.2f}\\tEps: {epsilon:.3f}\\tSteps: {steps}')\n",
    "            print(f'RAM: {mem.percent:.1f}% | GPU: {gpu_mem_str} | Buffer: {len(replay_buffer)}/{config[\"BUFFER_SIZE\"]}')\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_score > best_avg_score:\n",
    "                best_avg_score = avg_score\n",
    "                save_checkpoint(config, policy_net, target_net, optimizer, episode, \n",
    "                              avg_score, episode_rewards, is_best=True)\n",
    "        \n",
    "        # Save checkpoint every N episodes\n",
    "        if episode > 0 and episode % config['CHECKPOINT_EVERY'] == 0:\n",
    "            avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            save_checkpoint(config, policy_net, target_net, optimizer, episode, \n",
    "                          avg_score, episode_rewards, is_best=False)\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best average score: {best_avg_score:.2f}\")\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Standard DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Standard DQN\n",
    "CONFIG_DQN = BASE_CONFIG.copy()\n",
    "CONFIG_DQN['DQN_TYPE'] = 'DQN'\n",
    "CONFIG_DQN['USE_PER'] = False\n",
    "\n",
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (will be slower)\")\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DQN['SEED'])\n",
    "np.random.seed(CONFIG_DQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DQN['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_dqn = DQN((CONFIG_DQN['N_FRAMES'], 84, 84), CONFIG_DQN['N_ACTIONS']).to(device)\n",
    "target_net_dqn = DQN((CONFIG_DQN['N_FRAMES'], 84, 84), CONFIG_DQN['N_ACTIONS']).to(device)\n",
    "target_net_dqn.load_state_dict(policy_net_dqn.state_dict())\n",
    "\n",
    "optimizer_dqn = optim.Adam(policy_net_dqn.parameters(), lr=CONFIG_DQN['LEARNING_RATE'])\n",
    "replay_buffer_dqn = ReplayBuffer(CONFIG_DQN['BUFFER_SIZE'])\n",
    "\n",
    "# Train\n",
    "rewards_dqn = train_dqn(CONFIG_DQN, policy_net_dqn, target_net_dqn, \n",
    "                        optimizer_dqn, replay_buffer_dqn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Double DQN\n",
    "CONFIG_DDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DDQN['DQN_TYPE'] = 'DoubleDQN'\n",
    "CONFIG_DDQN['USE_PER'] = False\n",
    "CONFIG_DDQN['SEED'] = 42  # Different seed\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DDQN['SEED'])\n",
    "np.random.seed(CONFIG_DDQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DDQN['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_ddqn = DQN((CONFIG_DDQN['N_FRAMES'], 84, 84), CONFIG_DDQN['N_ACTIONS']).to(device)\n",
    "target_net_ddqn = DQN((CONFIG_DDQN['N_FRAMES'], 84, 84), CONFIG_DDQN['N_ACTIONS']).to(device)\n",
    "target_net_ddqn.load_state_dict(policy_net_ddqn.state_dict())\n",
    "\n",
    "optimizer_ddqn = optim.Adam(policy_net_ddqn.parameters(), lr=CONFIG_DDQN['LEARNING_RATE'])\n",
    "replay_buffer_ddqn = ReplayBuffer(CONFIG_DDQN['BUFFER_SIZE'])\n",
    "\n",
    "# Train\n",
    "rewards_ddqn = train_dqn(CONFIG_DDQN, policy_net_ddqn, target_net_ddqn, \n",
    "                         optimizer_ddqn, replay_buffer_ddqn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Dueling DQN\n",
    "CONFIG_DuelDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DuelDQN['DQN_TYPE'] = 'DuelingDQN'\n",
    "CONFIG_DuelDQN['USE_PER'] = False\n",
    "CONFIG_DuelDQN['SEED'] = 123  # Different seed\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DuelDQN['SEED'])\n",
    "np.random.seed(CONFIG_DuelDQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DuelDQN['SEED'])\n",
    "\n",
    "# Networks - Use DuelingDQN architecture\n",
    "policy_net_dueling = DuelingDQN((CONFIG_DuelDQN['N_FRAMES'], 84, 84), CONFIG_DuelDQN['N_ACTIONS']).to(device)\n",
    "target_net_dueling = DuelingDQN((CONFIG_DuelDQN['N_FRAMES'], 84, 84), CONFIG_DuelDQN['N_ACTIONS']).to(device)\n",
    "target_net_dueling.load_state_dict(policy_net_dueling.state_dict())\n",
    "\n",
    "optimizer_dueling = optim.Adam(policy_net_dueling.parameters(), lr=CONFIG_DuelDQN['LEARNING_RATE'])\n",
    "replay_buffer_dueling = ReplayBuffer(CONFIG_DuelDQN['BUFFER_SIZE'])\n",
    "\n",
    "# Train\n",
    "rewards_dueling = train_dqn(CONFIG_DuelDQN, policy_net_dueling, target_net_dueling, \n",
    "                            optimizer_dueling, replay_buffer_dueling, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - DQN with PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for DQN with Prioritized Experience Replay\n",
    "CONFIG_PER = BASE_CONFIG.copy()\n",
    "CONFIG_PER['DQN_TYPE'] = 'DQN'\n",
    "CONFIG_PER['USE_PER'] = True\n",
    "CONFIG_PER['SEED'] = 456  # Different seed\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_PER['SEED'])\n",
    "np.random.seed(CONFIG_PER['SEED'])\n",
    "torch.manual_seed(CONFIG_PER['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_per = DQN((CONFIG_PER['N_FRAMES'], 84, 84), CONFIG_PER['N_ACTIONS']).to(device)\n",
    "target_net_per = DQN((CONFIG_PER['N_FRAMES'], 84, 84), CONFIG_PER['N_ACTIONS']).to(device)\n",
    "target_net_per.load_state_dict(policy_net_per.state_dict())\n",
    "\n",
    "optimizer_per = optim.Adam(policy_net_per.parameters(), lr=CONFIG_PER['LEARNING_RATE'])\n",
    "replay_buffer_per = PrioritizedReplayBuffer(\n",
    "    CONFIG_PER['BUFFER_SIZE'],\n",
    "    alpha=CONFIG_PER['PER_ALPHA'],\n",
    "    beta_start=CONFIG_PER['PER_BETA_START'],\n",
    "    beta_frames=CONFIG_PER['PER_BETA_FRAMES']\n",
    ")\n",
    "\n",
    "# Train\n",
    "rewards_per = train_dqn(CONFIG_PER, policy_net_per, target_net_per, \n",
    "                        optimizer_per, replay_buffer_per, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidated Results Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {\n",
    "    'DQN': rewards_dqn,\n",
    "    'DoubleDQN': rewards_ddqn,\n",
    "    'DuelingDQN': rewards_dueling,\n",
    "    'DQN_PER': rewards_per\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consolidated_results(results_dict, window=100, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Plot consolidated training progress for multiple DQN runs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    for idx, (name, rewards) in enumerate(results_dict.items()):\n",
    "        # Calculate moving average\n",
    "        if len(rewards) >= window:\n",
    "            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            episodes = range(window-1, len(rewards))\n",
    "            final_avg = np.mean(rewards[-100:])\n",
    "            \n",
    "            # Plot moving average\n",
    "            plt.plot(episodes, moving_avg, \n",
    "                    label=f'{name} (Avg={final_avg:.2f})',\n",
    "                    color=colors[idx % len(colors)],\n",
    "                    linewidth=2)\n",
    "    \n",
    "    # Add goal lines\n",
    "    plt.axhline(y=500, color='green', linestyle='--', linewidth=2, label='Goal: 500', alpha=0.7)\n",
    "    plt.axhline(y=400, color='red', linestyle='--', linewidth=2, label='Goal: 400', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Episode #', fontsize=12)\n",
    "    plt.ylabel(f'Average Score ({window}-Game Window)', fontsize=12)\n",
    "    plt.title(f'Consolidated DQN Training Progress ({window}-Episode Moving Average)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_individual_results(rewards, name, window=100, figsize=(12, 6)):\n",
    "    \"\"\"Plot individual run results\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), moving_avg, label=f'Moving Average ({window})', linewidth=2)\n",
    "    \n",
    "    plt.axhline(y=500, color='r', linestyle='--', label='Target (500)')\n",
    "    plt.axhline(y=400, color='orange', linestyle='--', label='Minimum (400)')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'{name} - Training Progress on Space Invaders')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    final_avg = np.mean(rewards[-100:])\n",
    "    print(f\"\\n{name} - Final average reward (last 100 episodes): {final_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot consolidated results\n",
    "plot_consolidated_results(all_results, window=100)\n",
    "\n",
    "# Plot individual results\n",
    "for name, rewards in all_results.items():\n",
    "    plot_individual_results(rewards, name, window=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to a single file\n",
    "import pickle\n",
    "\n",
    "results_file = os.path.join(CHECKPOINT_DIR, 'all_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"All results saved to: {results_file}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for name, rewards in all_results.items():\n",
    "    final_avg = np.mean(rewards[-100:])\n",
    "    max_reward = max(rewards)\n",
    "    print(f\"{name:20s} - Avg (last 100): {final_avg:6.2f} | Max: {max_reward:6.1f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
