{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/advanced_dqn_space_invaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced DQN Variants for Space Invaders\n",
    "\n",
    "Implementation of DQN, Double DQN, Dueling DQN, and Prioritized Experience Replay for ALE/SpaceInvaders-v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[atari,accept-rom-license]\n",
      "Requirement already satisfied: ale-py in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>1.20 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (1.26.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (7.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->ale-py) (3.18.1)\n",
      "Requirement already satisfied: torch in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: scipy in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: numpy in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: psutil in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (5.9.8)\n",
      "Requirement already satisfied: filelock in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[atari,accept-rom-license]\n",
    "!pip install ale-py\n",
    "!pip install torch scipy numpy psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and verify ALE is available\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Register ALE environments\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved locally to: ./checkpoints\n",
      "Best models will be saved locally to: ./best_models\n"
     ]
    }
   ],
   "source": [
    "USE_GDRIVE = False  # Set to True to enable Google Drive integration\n",
    "\n",
    "if USE_GDRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/DQN_SpaceInvaders_Checkpoints'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    BEST_MODELS = './best_models'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(BEST_MODELS, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved locally to: {CHECKPOINT_DIR}\")\n",
    "    print(f\"Best models will be saved locally to: {BEST_MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Standard DQN Network\"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"Dueling DQN Network with separate value and advantage streams\"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        value = self.value_stream(conv_out)\n",
    "        advantage = self.advantage_stream(conv_out)\n",
    "        \n",
    "        # Combine value and advantage using the aggregation formula\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Standard Experience Replay Buffer\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized Experience Replay Buffer\"\"\"\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # How much prioritization to use (0 = uniform, 1 = full prioritization)\n",
    "        self.beta_start = beta_start  # Importance sampling weight\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.pos = 0\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        \"\"\"Linearly increase beta from beta_start to 1.0\"\"\"\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        N = len(self.buffer)\n",
    "        if N == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.pos]\n",
    "        \n",
    "        # Calculate sampling probabilities\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        \n",
    "        # Sample indices based on priorities\n",
    "        indices = np.random.choice(N, batch_size, p=probabilities, replace=False)\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (N * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        # Get samples\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        state, action, reward, next_state, done = zip(*samples)\n",
    "        \n",
    "        return np.array(state), action, reward, np.array(next_state), done, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions\"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Standard DQN preprocessing: \n",
    "    1. Grayscale\n",
    "    2. Resize/Crop to 84x84\n",
    "    3. Normalize\n",
    "    \"\"\"\n",
    "    # 1. Convert to grayscale (The frame should be H x W x 3)\n",
    "    # Use standard cv2 conversion for simplicity\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 2. Crop/Resize to 84x84\n",
    "    # Standard practice is to crop to the relevant game area, then resize.\n",
    "    # A simplified, but effective, approach is direct resizing:\n",
    "    # Use INTER_AREA for downsampling (best quality)\n",
    "    resized_frame = cv2.resize(\n",
    "        gray_frame, \n",
    "        (84, 84), \n",
    "        interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "\n",
    "    # 3. Normalize\n",
    "    normalized_frame = resized_frame.astype(np.float32) / 255.0\n",
    "    \n",
    "    # DQN expects a channel dimension, even if it's 1, often added during stacking\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, policy_net, n_actions, device):\n",
    "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "\n",
    "def optimize_model_dqn(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device):\n",
    "    \"\"\"Standard DQN optimization\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Next Q values from target network\n",
    "    next_q = target_net(next_states).max(1)[0].detach()\n",
    "    target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Loss\n",
    "    loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def optimize_model_double_dqn(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device):\n",
    "    \"\"\"Double DQN optimization\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Double DQN: use policy net to select actions, target net to evaluate them\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        next_q = target_net(next_states).gather(1, next_actions).squeeze()\n",
    "    \n",
    "    target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Loss\n",
    "    loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def optimize_model_per(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device, dqn_type='DQN'):\n",
    "    \"\"\"Optimization with Prioritized Experience Replay\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones, indices, weights = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    weights = torch.FloatTensor(weights).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    \n",
    "    # Calculate target Q values based on DQN type\n",
    "    with torch.no_grad():\n",
    "        if dqn_type == 'DoubleDQN':\n",
    "            next_actions = policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "            next_q = target_net(next_states).gather(1, next_actions).squeeze()\n",
    "        else:  # Standard DQN or Dueling DQN\n",
    "            next_q = target_net(next_states).max(1)[0]\n",
    "        \n",
    "        target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Calculate TD errors for priority update\n",
    "    td_errors = torch.abs(current_q - target_q).detach().cpu().numpy()\n",
    "    \n",
    "    # Weighted loss\n",
    "    loss = (weights * F.mse_loss(current_q, target_q, reduction='none')).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update priorities\n",
    "    new_priorities = td_errors + 1e-6  # Add small epsilon to prevent zero priority\n",
    "    replay_buffer.update_priorities(indices, new_priorities)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def print_config(config):\n",
    "    \"\"\"Print configuration in a formatted way\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  DQN TYPE: {config['DQN_TYPE']}\")\n",
    "    if config['USE_PER']:\n",
    "        print(f\"  Using Prioritized Experience Replay (PER)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"-\"*70)\n",
    "    for key, value in config.items():\n",
    "        if key not in ['CHECKPOINT_DIR']:  # Skip long paths\n",
    "            print(f\"  {key:20s}: {value}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "def save_checkpoint(config, policy_net, target_net, optimizer, episode, avg_score, \n",
    "                   episode_rewards, is_best=False):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dqn_type = config['DQN_TYPE']\n",
    "    per_suffix = \"_PER\" if config['USE_PER'] else \"\"\n",
    "    \n",
    "    if is_best:\n",
    "        filename = f\"{dqn_type}{per_suffix}_{timestamp}_best.pth\"\n",
    "        filepath = os.path.join(config['BEST_MODELS_DIR'], filename)\n",
    "        print(f'\\t\\tNew best avg: {avg_score:.2f} - saved to {filename}')\n",
    "    else:\n",
    "        filename = f\"{dqn_type}{per_suffix}_ep{episode}_{timestamp}.pth\"\n",
    "        filepath = os.path.join(config['CHECKPOINT_DIR'], filename)\n",
    "        print(f'\\t\\tCheckpoint saved: {filename}')\n",
    "\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'config': config,\n",
    "        'policy_net_state_dict': policy_net.state_dict(),\n",
    "        'target_net_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'avg_score': avg_score,\n",
    "        'timestamp': timestamp\n",
    "    }, filepath)\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(config, policy_net, target_net, optimizer, replay_buffer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train DQN with graceful interruption support and LR scheduling.\n",
    "    Press Ctrl+C anytime to stop and save progress.\n",
    "    \n",
    "    Supports:\n",
    "    - Standard DQN\n",
    "    - Double DQN\n",
    "    - Dueling DQN\n",
    "    - Prioritized Experience Replay (PER)\n",
    "    - Learning Rate Scheduling\n",
    "    \"\"\"\n",
    "    import psutil\n",
    "    from datetime import datetime\n",
    "    from torch.optim.lr_scheduler import ExponentialLR\n",
    "    import os\n",
    "    import numpy as np # Ensure numpy is available for printing\n",
    "    import torch.nn.functional as F # Ensure F is available for optimize functions\n",
    "    \n",
    "    # Print configuration\n",
    "    print_config(config)\n",
    "\n",
    "    # Create environment\n",
    "    env = gym.make(config['ENV_ID'])\n",
    "    if config.get('SEED') is not None:\n",
    "        env.reset(seed=config['SEED'])\n",
    "    \n",
    "    n_actions = config['N_ACTIONS']\n",
    "    episode_rewards = []\n",
    "    steps = 0\n",
    "    best_avg_score = -float('inf')\n",
    "    \n",
    "    # Initialize LR scheduler (optional - only if enabled in config)\n",
    "    scheduler = None\n",
    "    if config.get('LR_SCHEDULER', False):\n",
    "        scheduler = ExponentialLR(optimizer, gamma=config.get('LR_GAMMA', 0.9995))\n",
    "        print(f\"‚úÖ LR Scheduler enabled: ExponentialLR (gamma={config.get('LR_GAMMA', 0.9995)})\")\n",
    "    \n",
    "    # Select optimization function based on CONFIG\n",
    "    if config.get('USE_PER', False):\n",
    "        dqn_type = config.get('DQN_TYPE', 'DQN')\n",
    "        def optimize_fn():\n",
    "            return optimize_model_per(\n",
    "                policy_net, target_net, optimizer, replay_buffer,\n",
    "                config['BATCH_SIZE'], config['GAMMA'], device, \n",
    "                dqn_type\n",
    "            )\n",
    "        print(f\"Optimization: {dqn_type} + PER\")\n",
    "    elif config.get('DQN_TYPE', 'DQN') == 'DoubleDQN':\n",
    "        def optimize_fn():\n",
    "            return optimize_model_double_dqn(\n",
    "                policy_net, target_net, optimizer, replay_buffer,\n",
    "                config['BATCH_SIZE'], config['GAMMA'], device\n",
    "            )\n",
    "        print(\"Optimization: Double DQN\")\n",
    "    else:\n",
    "        def optimize_fn():\n",
    "            return optimize_model_dqn(\n",
    "                policy_net, target_net, optimizer, replay_buffer,\n",
    "                config['BATCH_SIZE'], config['GAMMA'], device\n",
    "            )\n",
    "        dqn_type = config.get('DQN_TYPE', 'DQN')\n",
    "        if dqn_type == 'DuelingDQN':\n",
    "            print(\"Optimization: Dueling DQN (uses Standard DQN optimization)\")\n",
    "        else:\n",
    "            print(\"Optimization: Standard DQN\")\n",
    "    \n",
    "    print(\"\\nüí° Press Ctrl+C anytime to stop training and save progress\\n\")\n",
    "    \n",
    "    # --- START TIME LOGGING ---\n",
    "    start_time = datetime.now()\n",
    "    print(f\"--- TRAINING STARTED: {start_time.strftime('%Y-%m-%d %H:%M:%S')} ---\\n\")\n",
    "    # --------------------------\n",
    "    \n",
    "    try:\n",
    "        for episode in range(config['N_EPISODES']):\n",
    "            state, _ = env.reset()\n",
    "            # Ensure preprocess_frame is defined or imported globally\n",
    "            # For this context, we assume preprocess_frame is defined elsewhere\n",
    "            from collections import deque\n",
    "            state = preprocess_frame(state)\n",
    "            state_stack = deque([state] * config['N_FRAMES'], maxlen=config['N_FRAMES'])\n",
    "            \n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                epsilon = config['EPSILON_END'] + (config['EPSILON_START'] - config['EPSILON_END']) * \\\n",
    "                          np.exp(-1. * steps / config['EPSILON_DECAY'])\n",
    "                \n",
    "                state_array = np.array(state_stack)\n",
    "                # Ensure select_action is defined or imported globally\n",
    "                action = select_action(state_array, epsilon, policy_net, n_actions, device)\n",
    "                \n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                next_state = preprocess_frame(next_state)\n",
    "                next_state_stack = state_stack.copy()\n",
    "                next_state_stack.append(next_state)\n",
    "                \n",
    "                replay_buffer.push(\n",
    "                    np.array(state_stack),\n",
    "                    action,\n",
    "                    reward,\n",
    "                    np.array(next_state_stack),\n",
    "                    float(done)\n",
    "                )\n",
    "                \n",
    "                state_stack = next_state_stack\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                optimize_fn()\n",
    "                \n",
    "                if steps % config['TARGET_UPDATE'] == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            # Step LR scheduler after each episode\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Print every 10 episodes the progress\n",
    "            if episode % 10 == 0:\n",
    "                avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f'Episode {episode} | Score: {episode_reward:.1f} | Avg: {avg_score:.2f} | Eps: {epsilon:.3f} | LR: {current_lr:.6f} | Steps: {steps}')\n",
    "            \n",
    "            if episode % config['CHECKPOINT_EVERY'] == 0:\n",
    "                # save_checkpoint(\n",
    "                #         config, policy_net, target_net, optimizer, \n",
    "                #         episode, avg_score, episode_rewards, is_best=False)\n",
    "                if avg_score > best_avg_score:\n",
    "                    best_avg_score = avg_score\n",
    "                    save_checkpoint(\n",
    "                        config, policy_net, target_net, optimizer, \n",
    "                        episode, avg_score, episode_rewards, is_best=True)\n",
    "                    \n",
    "            if episode % 40 == 0:\n",
    "                mem = psutil.virtual_memory()\n",
    "                gpu_mem_str = \"N/A\"\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                    gpu_mem_str = f\"{gpu_mem:.2f}GB\"\n",
    "                elif torch.backends.mps.is_available() and device.type == 'mps':\n",
    "                    gpu_mem_str = \"Active\"\n",
    "                print(f'\\t\\tRAM: {mem.percent:.1f}% | GPU: {gpu_mem_str} | Buffer: {len(replay_buffer)}/{config[\"BUFFER_SIZE\"]}')\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"‚ö†Ô∏è  TRAINING INTERRUPTED BY USER\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if len(episode_rewards) > 0:\n",
    "            current_episode = len(episode_rewards) - 1\n",
    "            avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            \n",
    "            print(f\"\\nüìä Training Statistics at Interruption:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"  Episodes completed: {current_episode + 1} / {config['N_EPISODES']}\")\n",
    "            print(f\"  Total steps: {steps:,}\")\n",
    "            print(f\"  Last episode score: {episode_rewards[-1]:.1f}\")\n",
    "            print(f\"  Average score (last {min(100, len(episode_rewards))} episodes): {avg_score:.2f}\")\n",
    "            print(f\"  Best average score: {best_avg_score:.2f}\")\n",
    "            print(f\"  Max episode score: {max(episode_rewards):.1f}\")\n",
    "            print(f\"  Min episode score: {min(episode_rewards):.1f}\")\n",
    "            \n",
    "            # --- INTERRUPTED TIME LOGGING ---\n",
    "            print(f\"\\n--- INTERRUPTED AT: {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
    "            print(f\"--- DURATION: {duration} ---\")\n",
    "            # --------------------------------\n",
    "            \n",
    "            save_checkpoint(\n",
    "                config, policy_net, target_net, optimizer, \n",
    "                current_episode, avg_score, episode_rewards, is_best=False)\n",
    "            \n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "                if len(episode_rewards) >= 10:\n",
    "                    window = min(100, len(episode_rewards))\n",
    "                    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "                    plt.plot(range(window-1, len(episode_rewards)), moving_avg, \n",
    "                            label=f'Moving Avg ({window})', linewidth=2, color='red')\n",
    "                plt.axhline(y=avg_score, color='green', linestyle='--', \n",
    "                           label=f'Current Avg: {avg_score:.2f}')\n",
    "                plt.xlabel('Episode')\n",
    "                plt.ylabel('Score')\n",
    "                plt.title(f'Training Progress (Interrupted at Episode {current_episode})')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                plot_filename = f'interrupted_plot_{timestamp}.png'\n",
    "                plot_filepath = os.path.join(config['BEST_MODELS_DIR'], plot_filename)\n",
    "                plt.savefig(plot_filepath, dpi=100)\n",
    "                \n",
    "                print(f\"üìà Plot saved: {plot_filepath}\")\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not generate plot: {e}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"‚úÖ All progress saved successfully!\")\n",
    "            print(\"üí° You can resume training by loading the checkpoint\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "    \n",
    "    if len(episode_rewards) == config['N_EPISODES']:\n",
    "        # --- END TIME LOGGING ---\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\n--- TRAINING FINISHED: {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
    "        print(f\"--- DURATION: {duration} ---\")\n",
    "        # ------------------------\n",
    "        \n",
    "        print(\"\\n‚úÖ Training completed successfully!\")\n",
    "        print(f\"Best average score: {best_avg_score:.2f}\\n\")\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Standard DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (will be slower)\")\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Configuration\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    'ENV_ID': 'ALE/SpaceInvaders-v5',\n",
    "    'SEED': 18,\n",
    "    'N_FRAMES': 4,\n",
    "    'N_ACTIONS': 6,\n",
    "    'N_EPISODES': 4000,\n",
    "    'LEARNING_RATE': 0.00001,\n",
    "    'GAMMA': 0.99,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPSILON_START': 1.0,\n",
    "    'EPSILON_END': 0.1,\n",
    "    'EPSILON_DECAY': 100000,\n",
    "    'BUFFER_SIZE': 100000,\n",
    "    'TARGET_UPDATE': 5000,\n",
    "    'CHECKPOINT_EVERY': 400,\n",
    "    'USE_GDRIVE': USE_GDRIVE,\n",
    "    'CHECKPOINT_DIR': CHECKPOINT_DIR,\n",
    "    'BEST_MODELS_DIR': BEST_MODELS,\n",
    "    'DQN_TYPE': 'DQN',  # Options: 'DQN', 'DoubleDQN', 'DuelingDQN'\n",
    "    'USE_PER': False, \n",
    "    'LR_SCHEDULER': True,\n",
    "    'LR_GAMMA': 0.999, \n",
    "    \n",
    "    # PER hyperparameters (if USE_PER=True)\n",
    "    'PER_ALPHA': 0.6,\n",
    "    'PER_BETA_START': 0.4,\n",
    "    'PER_BETA_FRAMES': 100000,\n",
    "    'PER_EPSILON': 1e-6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Standard DQN\n",
    "CONFIG_DQN = BASE_CONFIG.copy()\n",
    "CONFIG_DQN['DQN_TYPE'] = 'DQN'\n",
    "CONFIG_DQN['USE_PER'] = False\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DQN['SEED'])\n",
    "np.random.seed(CONFIG_DQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DQN['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_dqn = DQN((CONFIG_DQN['N_FRAMES'], 84, 84), CONFIG_DQN['N_ACTIONS']).to(device)\n",
    "target_net_dqn = DQN((CONFIG_DQN['N_FRAMES'], 84, 84), CONFIG_DQN['N_ACTIONS']).to(device)\n",
    "target_net_dqn.load_state_dict(policy_net_dqn.state_dict())\n",
    "\n",
    "optimizer_dqn = optim.Adam(policy_net_dqn.parameters(), lr=CONFIG_DQN['LEARNING_RATE'])\n",
    "replay_buffer_dqn = ReplayBuffer(CONFIG_DQN['BUFFER_SIZE'])\n",
    "\n",
    "# Train\n",
    "rewards_dqn = train_dqn(CONFIG_DQN, policy_net_dqn, target_net_dqn, \n",
    "                        optimizer_dqn, replay_buffer_dqn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Double DQN (PER/no PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Using Standard Uniform Replay Buffer\n",
      "\n",
      "======================================================================\n",
      "  DQN TYPE: DoubleDQN\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "----------------------------------------------------------------------\n",
      "  ENV_ID              : ALE/SpaceInvaders-v5\n",
      "  SEED                : 18\n",
      "  N_FRAMES            : 4\n",
      "  N_ACTIONS           : 6\n",
      "  N_EPISODES          : 4000\n",
      "  LEARNING_RATE       : 1e-05\n",
      "  GAMMA               : 0.99\n",
      "  BATCH_SIZE          : 64\n",
      "  EPSILON_START       : 1.0\n",
      "  EPSILON_END         : 0.1\n",
      "  EPSILON_DECAY       : 100000\n",
      "  BUFFER_SIZE         : 100000\n",
      "  TARGET_UPDATE       : 5000\n",
      "  CHECKPOINT_EVERY    : 400\n",
      "  USE_GDRIVE          : False\n",
      "  BEST_MODELS_DIR     : ./best_models\n",
      "  DQN_TYPE            : DoubleDQN\n",
      "  USE_PER             : False\n",
      "  LR_SCHEDULER        : True\n",
      "  LR_GAMMA            : 0.999\n",
      "  PER_ALPHA           : 0.6\n",
      "  PER_BETA_START      : 0.4\n",
      "  PER_BETA_FRAMES     : 120000\n",
      "  PER_EPSILON         : 1e-06\n",
      "======================================================================\n",
      "\n",
      "‚úÖ LR Scheduler enabled: ExponentialLR (gamma=0.999)\n",
      "Optimization: Double DQN\n",
      "\n",
      "üí° Press Ctrl+C anytime to stop training and save progress\n",
      "\n",
      "--- TRAINING STARTED: 2025-11-30 10:12:13 ---\n",
      "\n",
      "Episode 0 | Score: 55.0 | Avg: 55.00 | Eps: 0.996 | LR: 0.000010 | Steps: 409\n",
      "\t\tNew best avg: 55.00 - saved to DoubleDQN_20251130_101219_best.pth\n",
      "\t\tRAM: 72.3% | GPU: Active | Buffer: 409/100000\n",
      "Episode 10 | Score: 110.0 | Avg: 132.27 | Eps: 0.953 | LR: 0.000010 | Steps: 5393\n",
      "Episode 20 | Score: 235.0 | Avg: 125.71 | Eps: 0.912 | LR: 0.000010 | Steps: 10322\n",
      "Episode 30 | Score: 45.0 | Avg: 134.68 | Eps: 0.873 | LR: 0.000010 | Steps: 15256\n",
      "Episode 40 | Score: 180.0 | Avg: 141.10 | Eps: 0.826 | LR: 0.000010 | Steps: 21423\n",
      "\t\tRAM: 71.2% | GPU: Active | Buffer: 21423/100000\n",
      "Episode 50 | Score: 135.0 | Avg: 145.88 | Eps: 0.781 | LR: 0.000010 | Steps: 27838\n",
      "Episode 60 | Score: 210.0 | Avg: 148.20 | Eps: 0.743 | LR: 0.000009 | Steps: 33596\n",
      "Episode 70 | Score: 295.0 | Avg: 155.49 | Eps: 0.706 | LR: 0.000009 | Steps: 39519\n",
      "Episode 80 | Score: 240.0 | Avg: 159.75 | Eps: 0.673 | LR: 0.000009 | Steps: 45199\n",
      "\t\tRAM: 74.9% | GPU: Active | Buffer: 45199/100000\n",
      "Episode 90 | Score: 105.0 | Avg: 156.81 | Eps: 0.643 | LR: 0.000009 | Steps: 50588\n",
      "Episode 100 | Score: 155.0 | Avg: 152.95 | Eps: 0.617 | LR: 0.000009 | Steps: 55416\n",
      "Episode 110 | Score: 305.0 | Avg: 153.95 | Eps: 0.593 | LR: 0.000009 | Steps: 60288\n",
      "Episode 120 | Score: 175.0 | Avg: 161.00 | Eps: 0.565 | LR: 0.000009 | Steps: 66040\n",
      "\t\tRAM: 77.7% | GPU: Active | Buffer: 66040/100000\n",
      "Episode 130 | Score: 100.0 | Avg: 159.90 | Eps: 0.539 | LR: 0.000009 | Steps: 71709\n",
      "Episode 140 | Score: 470.0 | Avg: 163.05 | Eps: 0.517 | LR: 0.000009 | Steps: 76880\n",
      "Episode 150 | Score: 5.0 | Avg: 162.15 | Eps: 0.498 | LR: 0.000009 | Steps: 81604\n",
      "Episode 160 | Score: 210.0 | Avg: 164.80 | Eps: 0.473 | LR: 0.000009 | Steps: 87994\n",
      "\t\tRAM: 78.8% | GPU: Active | Buffer: 87994/100000\n",
      "Episode 170 | Score: 125.0 | Avg: 166.25 | Eps: 0.449 | LR: 0.000008 | Steps: 94640\n",
      "Episode 180 | Score: 85.0 | Avg: 162.25 | Eps: 0.431 | LR: 0.000008 | Steps: 100152\n",
      "Episode 190 | Score: 50.0 | Avg: 166.75 | Eps: 0.413 | LR: 0.000008 | Steps: 105773\n",
      "Episode 200 | Score: 490.0 | Avg: 169.45 | Eps: 0.399 | LR: 0.000008 | Steps: 110242\n",
      "\t\tRAM: 80.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 210 | Score: 90.0 | Avg: 168.60 | Eps: 0.385 | LR: 0.000008 | Steps: 115055\n",
      "Episode 220 | Score: 265.0 | Avg: 169.55 | Eps: 0.367 | LR: 0.000008 | Steps: 121377\n",
      "Episode 230 | Score: 165.0 | Avg: 172.80 | Eps: 0.351 | LR: 0.000008 | Steps: 127651\n",
      "Episode 240 | Score: 50.0 | Avg: 169.00 | Eps: 0.338 | LR: 0.000008 | Steps: 133002\n",
      "\t\tRAM: 80.3% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 250 | Score: 250.0 | Avg: 166.15 | Eps: 0.326 | LR: 0.000008 | Steps: 138402\n",
      "Episode 260 | Score: 90.0 | Avg: 160.85 | Eps: 0.314 | LR: 0.000008 | Steps: 143497\n",
      "Episode 270 | Score: 65.0 | Avg: 149.00 | Eps: 0.306 | LR: 0.000008 | Steps: 147446\n",
      "Episode 280 | Score: 160.0 | Avg: 154.80 | Eps: 0.294 | LR: 0.000008 | Steps: 153201\n",
      "\t\tRAM: 80.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 290 | Score: 165.0 | Avg: 160.85 | Eps: 0.281 | LR: 0.000007 | Steps: 160422\n",
      "Episode 300 | Score: 370.0 | Avg: 168.75 | Eps: 0.270 | LR: 0.000007 | Steps: 166895\n",
      "Episode 310 | Score: 250.0 | Avg: 174.90 | Eps: 0.259 | LR: 0.000007 | Steps: 173381\n",
      "Episode 320 | Score: 150.0 | Avg: 178.30 | Eps: 0.249 | LR: 0.000007 | Steps: 179555\n",
      "\t\tRAM: 80.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 330 | Score: 105.0 | Avg: 181.70 | Eps: 0.242 | LR: 0.000007 | Steps: 184937\n",
      "Episode 340 | Score: 225.0 | Avg: 186.30 | Eps: 0.233 | LR: 0.000007 | Steps: 191101\n",
      "Episode 350 | Score: 45.0 | Avg: 187.20 | Eps: 0.227 | LR: 0.000007 | Steps: 196061\n",
      "Episode 360 | Score: 325.0 | Avg: 194.30 | Eps: 0.219 | LR: 0.000007 | Steps: 201929\n",
      "\t\tRAM: 80.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 370 | Score: 210.0 | Avg: 205.65 | Eps: 0.212 | LR: 0.000007 | Steps: 208109\n",
      "Episode 380 | Score: 175.0 | Avg: 201.50 | Eps: 0.207 | LR: 0.000007 | Steps: 212913\n",
      "Episode 390 | Score: 150.0 | Avg: 191.40 | Eps: 0.202 | LR: 0.000007 | Steps: 217764\n",
      "Episode 400 | Score: 230.0 | Avg: 189.90 | Eps: 0.196 | LR: 0.000007 | Steps: 223724\n",
      "\t\tNew best avg: 189.90 - saved to DoubleDQN_20251130_120234_best.pth\n",
      "\t\tRAM: 80.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 410 | Score: 265.0 | Avg: 190.95 | Eps: 0.190 | LR: 0.000007 | Steps: 230027\n",
      "Episode 420 | Score: 135.0 | Avg: 185.95 | Eps: 0.185 | LR: 0.000007 | Steps: 235950\n",
      "Episode 430 | Score: 245.0 | Avg: 182.85 | Eps: 0.180 | LR: 0.000006 | Steps: 241482\n",
      "Episode 440 | Score: 105.0 | Avg: 184.35 | Eps: 0.175 | LR: 0.000006 | Steps: 248162\n",
      "\t\tRAM: 79.5% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 450 | Score: 125.0 | Avg: 188.35 | Eps: 0.171 | LR: 0.000006 | Steps: 253864\n",
      "Episode 460 | Score: 115.0 | Avg: 180.75 | Eps: 0.167 | LR: 0.000006 | Steps: 259061\n",
      "Episode 470 | Score: 80.0 | Avg: 174.55 | Eps: 0.164 | LR: 0.000006 | Steps: 264680\n",
      "Episode 480 | Score: 575.0 | Avg: 173.25 | Eps: 0.161 | LR: 0.000006 | Steps: 269731\n",
      "\t\tRAM: 80.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 490 | Score: 155.0 | Avg: 173.05 | Eps: 0.158 | LR: 0.000006 | Steps: 274887\n",
      "Episode 500 | Score: 60.0 | Avg: 166.35 | Eps: 0.155 | LR: 0.000006 | Steps: 279678\n",
      "Episode 510 | Score: 300.0 | Avg: 160.95 | Eps: 0.152 | LR: 0.000006 | Steps: 284746\n",
      "Episode 520 | Score: 175.0 | Avg: 162.15 | Eps: 0.150 | LR: 0.000006 | Steps: 289795\n",
      "\t\tRAM: 80.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 530 | Score: 430.0 | Avg: 168.15 | Eps: 0.146 | LR: 0.000006 | Steps: 296860\n",
      "Episode 540 | Score: 125.0 | Avg: 170.30 | Eps: 0.144 | LR: 0.000006 | Steps: 302958\n",
      "Episode 550 | Score: 105.0 | Avg: 177.15 | Eps: 0.141 | LR: 0.000006 | Steps: 308830\n",
      "Episode 560 | Score: 165.0 | Avg: 178.85 | Eps: 0.139 | LR: 0.000006 | Steps: 314125\n",
      "\t\tRAM: 79.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 570 | Score: 210.0 | Avg: 181.65 | Eps: 0.137 | LR: 0.000006 | Steps: 319600\n",
      "Episode 580 | Score: 105.0 | Avg: 184.75 | Eps: 0.135 | LR: 0.000006 | Steps: 325341\n",
      "Episode 590 | Score: 315.0 | Avg: 191.40 | Eps: 0.132 | LR: 0.000006 | Steps: 332270\n",
      "Episode 600 | Score: 285.0 | Avg: 204.00 | Eps: 0.130 | LR: 0.000005 | Steps: 339305\n",
      "\t\tRAM: 79.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 610 | Score: 475.0 | Avg: 209.60 | Eps: 0.128 | LR: 0.000005 | Steps: 345838\n",
      "Episode 620 | Score: 80.0 | Avg: 202.20 | Eps: 0.127 | LR: 0.000005 | Steps: 350807\n",
      "Episode 630 | Score: 160.0 | Avg: 193.30 | Eps: 0.126 | LR: 0.000005 | Steps: 355735\n",
      "Episode 640 | Score: 75.0 | Avg: 184.30 | Eps: 0.124 | LR: 0.000005 | Steps: 360901\n",
      "\t\tRAM: 79.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 650 | Score: 380.0 | Avg: 180.45 | Eps: 0.123 | LR: 0.000005 | Steps: 367231\n",
      "Episode 660 | Score: 95.0 | Avg: 189.75 | Eps: 0.121 | LR: 0.000005 | Steps: 373907\n",
      "Episode 670 | Score: 200.0 | Avg: 187.45 | Eps: 0.120 | LR: 0.000005 | Steps: 379003\n",
      "Episode 680 | Score: 125.0 | Avg: 184.00 | Eps: 0.119 | LR: 0.000005 | Steps: 383915\n",
      "\t\tRAM: 79.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 690 | Score: 140.0 | Avg: 178.95 | Eps: 0.118 | LR: 0.000005 | Steps: 388958\n",
      "Episode 700 | Score: 210.0 | Avg: 171.90 | Eps: 0.117 | LR: 0.000005 | Steps: 394766\n",
      "Episode 710 | Score: 200.0 | Avg: 166.65 | Eps: 0.117 | LR: 0.000005 | Steps: 399780\n",
      "Episode 720 | Score: 225.0 | Avg: 173.95 | Eps: 0.116 | LR: 0.000005 | Steps: 406028\n",
      "\t\tRAM: 79.5% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 730 | Score: 195.0 | Avg: 174.00 | Eps: 0.115 | LR: 0.000005 | Steps: 411377\n",
      "Episode 740 | Score: 165.0 | Avg: 182.05 | Eps: 0.114 | LR: 0.000005 | Steps: 418061\n",
      "Episode 750 | Score: 245.0 | Avg: 179.30 | Eps: 0.113 | LR: 0.000005 | Steps: 423443\n",
      "Episode 760 | Score: 125.0 | Avg: 172.05 | Eps: 0.112 | LR: 0.000005 | Steps: 428929\n",
      "\t\tRAM: 80.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 770 | Score: 120.0 | Avg: 176.95 | Eps: 0.112 | LR: 0.000005 | Steps: 435114\n",
      "Episode 780 | Score: 110.0 | Avg: 184.15 | Eps: 0.111 | LR: 0.000005 | Steps: 441972\n",
      "Episode 790 | Score: 140.0 | Avg: 186.40 | Eps: 0.110 | LR: 0.000005 | Steps: 447457\n",
      "Episode 800 | Score: 180.0 | Avg: 185.30 | Eps: 0.110 | LR: 0.000004 | Steps: 453127\n",
      "\t\tRAM: 80.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 810 | Score: 680.0 | Avg: 189.85 | Eps: 0.109 | LR: 0.000004 | Steps: 459382\n",
      "Episode 820 | Score: 155.0 | Avg: 188.65 | Eps: 0.109 | LR: 0.000004 | Steps: 465372\n",
      "Episode 830 | Score: 110.0 | Avg: 197.25 | Eps: 0.108 | LR: 0.000004 | Steps: 471943\n",
      "Episode 840 | Score: 210.0 | Avg: 195.55 | Eps: 0.108 | LR: 0.000004 | Steps: 477718\n",
      "\t\tRAM: 80.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 850 | Score: 110.0 | Avg: 196.40 | Eps: 0.107 | LR: 0.000004 | Steps: 483527\n",
      "Episode 860 | Score: 110.0 | Avg: 202.30 | Eps: 0.107 | LR: 0.000004 | Steps: 490662\n",
      "Episode 870 | Score: 50.0 | Avg: 200.00 | Eps: 0.106 | LR: 0.000004 | Steps: 496096\n",
      "Episode 880 | Score: 105.0 | Avg: 194.65 | Eps: 0.106 | LR: 0.000004 | Steps: 501747\n",
      "\t\tRAM: 80.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 890 | Score: 190.0 | Avg: 201.45 | Eps: 0.106 | LR: 0.000004 | Steps: 508088\n",
      "Episode 900 | Score: 160.0 | Avg: 198.30 | Eps: 0.105 | LR: 0.000004 | Steps: 514130\n",
      "Episode 910 | Score: 110.0 | Avg: 197.85 | Eps: 0.105 | LR: 0.000004 | Steps: 520076\n",
      "Episode 920 | Score: 195.0 | Avg: 210.20 | Eps: 0.105 | LR: 0.000004 | Steps: 527206\n",
      "\t\tRAM: 80.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 930 | Score: 295.0 | Avg: 204.75 | Eps: 0.104 | LR: 0.000004 | Steps: 533321\n",
      "Episode 940 | Score: 180.0 | Avg: 201.10 | Eps: 0.104 | LR: 0.000004 | Steps: 539500\n",
      "Episode 950 | Score: 85.0 | Avg: 197.05 | Eps: 0.104 | LR: 0.000004 | Steps: 544413\n",
      "Episode 960 | Score: 535.0 | Avg: 195.75 | Eps: 0.104 | LR: 0.000004 | Steps: 550499\n",
      "\t\tRAM: 79.6% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 970 | Score: 110.0 | Avg: 195.20 | Eps: 0.103 | LR: 0.000004 | Steps: 555866\n",
      "Episode 980 | Score: 55.0 | Avg: 196.65 | Eps: 0.103 | LR: 0.000004 | Steps: 561486\n",
      "Episode 990 | Score: 95.0 | Avg: 188.10 | Eps: 0.103 | LR: 0.000004 | Steps: 566560\n",
      "Episode 1000 | Score: 220.0 | Avg: 187.25 | Eps: 0.103 | LR: 0.000004 | Steps: 571737\n",
      "\t\tRAM: 80.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1010 | Score: 380.0 | Avg: 180.90 | Eps: 0.103 | LR: 0.000004 | Steps: 576247\n",
      "Episode 1020 | Score: 110.0 | Avg: 166.70 | Eps: 0.103 | LR: 0.000004 | Steps: 582225\n",
      "Episode 1030 | Score: 110.0 | Avg: 166.55 | Eps: 0.103 | LR: 0.000004 | Steps: 588108\n",
      "Episode 1040 | Score: 135.0 | Avg: 162.50 | Eps: 0.102 | LR: 0.000004 | Steps: 593356\n",
      "\t\tRAM: 79.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1050 | Score: 75.0 | Avg: 159.95 | Eps: 0.102 | LR: 0.000003 | Steps: 598551\n",
      "Episode 1060 | Score: 280.0 | Avg: 153.30 | Eps: 0.102 | LR: 0.000003 | Steps: 603350\n",
      "Episode 1070 | Score: 135.0 | Avg: 147.60 | Eps: 0.102 | LR: 0.000003 | Steps: 607482\n",
      "Episode 1080 | Score: 155.0 | Avg: 145.15 | Eps: 0.102 | LR: 0.000003 | Steps: 612731\n",
      "\t\tRAM: 79.6% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1090 | Score: 120.0 | Avg: 149.15 | Eps: 0.102 | LR: 0.000003 | Steps: 619037\n",
      "Episode 1100 | Score: 120.0 | Avg: 151.95 | Eps: 0.102 | LR: 0.000003 | Steps: 624166\n",
      "Episode 1110 | Score: 90.0 | Avg: 155.45 | Eps: 0.102 | LR: 0.000003 | Steps: 629335\n",
      "Episode 1120 | Score: 115.0 | Avg: 154.25 | Eps: 0.102 | LR: 0.000003 | Steps: 634202\n",
      "\t\tRAM: 80.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1130 | Score: 85.0 | Avg: 151.95 | Eps: 0.101 | LR: 0.000003 | Steps: 640214\n",
      "Episode 1140 | Score: 705.0 | Avg: 160.65 | Eps: 0.101 | LR: 0.000003 | Steps: 646957\n",
      "Episode 1150 | Score: 210.0 | Avg: 171.05 | Eps: 0.101 | LR: 0.000003 | Steps: 653525\n",
      "Episode 1160 | Score: 110.0 | Avg: 174.30 | Eps: 0.101 | LR: 0.000003 | Steps: 659128\n",
      "\t\tRAM: 79.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1170 | Score: 265.0 | Avg: 181.65 | Eps: 0.101 | LR: 0.000003 | Steps: 665798\n",
      "Episode 1180 | Score: 270.0 | Avg: 192.15 | Eps: 0.101 | LR: 0.000003 | Steps: 672769\n",
      "Episode 1190 | Score: 150.0 | Avg: 188.45 | Eps: 0.101 | LR: 0.000003 | Steps: 678305\n",
      "Episode 1200 | Score: 120.0 | Avg: 195.35 | Eps: 0.101 | LR: 0.000003 | Steps: 684523\n",
      "\t\tNew best avg: 195.35 - saved to DoubleDQN_20251130_152200_best.pth\n",
      "\t\tRAM: 80.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1210 | Score: 50.0 | Avg: 197.85 | Eps: 0.101 | LR: 0.000003 | Steps: 689884\n",
      "Episode 1220 | Score: 140.0 | Avg: 203.35 | Eps: 0.101 | LR: 0.000003 | Steps: 696366\n",
      "Episode 1230 | Score: 240.0 | Avg: 208.30 | Eps: 0.101 | LR: 0.000003 | Steps: 702354\n",
      "Episode 1240 | Score: 110.0 | Avg: 203.85 | Eps: 0.101 | LR: 0.000003 | Steps: 708112\n",
      "\t\tRAM: 80.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1250 | Score: 155.0 | Avg: 199.50 | Eps: 0.101 | LR: 0.000003 | Steps: 714011\n",
      "Episode 1260 | Score: 240.0 | Avg: 201.20 | Eps: 0.101 | LR: 0.000003 | Steps: 720001\n",
      "Episode 1270 | Score: 110.0 | Avg: 211.90 | Eps: 0.101 | LR: 0.000003 | Steps: 727951\n",
      "Episode 1280 | Score: 160.0 | Avg: 202.60 | Eps: 0.101 | LR: 0.000003 | Steps: 734010\n",
      "\t\tRAM: 80.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1290 | Score: 820.0 | Avg: 207.40 | Eps: 0.101 | LR: 0.000003 | Steps: 739831\n",
      "Episode 1300 | Score: 90.0 | Avg: 206.45 | Eps: 0.101 | LR: 0.000003 | Steps: 746410\n",
      "Episode 1310 | Score: 110.0 | Avg: 201.60 | Eps: 0.100 | LR: 0.000003 | Steps: 751970\n",
      "Episode 1320 | Score: 345.0 | Avg: 202.10 | Eps: 0.100 | LR: 0.000003 | Steps: 758048\n",
      "\t\tRAM: 80.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1330 | Score: 120.0 | Avg: 199.40 | Eps: 0.100 | LR: 0.000003 | Steps: 764229\n",
      "Episode 1340 | Score: 355.0 | Avg: 203.80 | Eps: 0.100 | LR: 0.000003 | Steps: 770791\n",
      "Episode 1350 | Score: 90.0 | Avg: 204.65 | Eps: 0.100 | LR: 0.000003 | Steps: 776264\n",
      "Episode 1360 | Score: 80.0 | Avg: 209.40 | Eps: 0.100 | LR: 0.000003 | Steps: 783057\n",
      "\t\tRAM: 79.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1370 | Score: 55.0 | Avg: 197.30 | Eps: 0.100 | LR: 0.000003 | Steps: 789605\n",
      "Episode 1380 | Score: 185.0 | Avg: 202.15 | Eps: 0.100 | LR: 0.000003 | Steps: 796495\n",
      "Episode 1390 | Score: 545.0 | Avg: 207.50 | Eps: 0.100 | LR: 0.000002 | Steps: 802934\n",
      "Episode 1400 | Score: 115.0 | Avg: 206.05 | Eps: 0.100 | LR: 0.000002 | Steps: 808951\n",
      "\t\tRAM: 80.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1410 | Score: 75.0 | Avg: 211.40 | Eps: 0.100 | LR: 0.000002 | Steps: 815289\n",
      "Episode 1420 | Score: 260.0 | Avg: 210.30 | Eps: 0.100 | LR: 0.000002 | Steps: 821586\n",
      "Episode 1430 | Score: 180.0 | Avg: 208.80 | Eps: 0.100 | LR: 0.000002 | Steps: 827145\n",
      "Episode 1440 | Score: 205.0 | Avg: 208.95 | Eps: 0.100 | LR: 0.000002 | Steps: 833727\n",
      "\t\tRAM: 80.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1450 | Score: 100.0 | Avg: 215.15 | Eps: 0.100 | LR: 0.000002 | Steps: 839793\n",
      "Episode 1460 | Score: 140.0 | Avg: 212.25 | Eps: 0.100 | LR: 0.000002 | Steps: 845191\n",
      "Episode 1470 | Score: 140.0 | Avg: 215.00 | Eps: 0.100 | LR: 0.000002 | Steps: 851337\n",
      "Episode 1480 | Score: 230.0 | Avg: 212.80 | Eps: 0.100 | LR: 0.000002 | Steps: 857432\n",
      "\t\tRAM: 80.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1490 | Score: 180.0 | Avg: 214.70 | Eps: 0.100 | LR: 0.000002 | Steps: 864275\n",
      "Episode 1500 | Score: 165.0 | Avg: 214.00 | Eps: 0.100 | LR: 0.000002 | Steps: 870880\n",
      "Episode 1510 | Score: 180.0 | Avg: 214.75 | Eps: 0.100 | LR: 0.000002 | Steps: 876555\n",
      "Episode 1520 | Score: 140.0 | Avg: 216.50 | Eps: 0.100 | LR: 0.000002 | Steps: 882811\n",
      "\t\tRAM: 81.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1530 | Score: 410.0 | Avg: 221.25 | Eps: 0.100 | LR: 0.000002 | Steps: 888570\n",
      "Episode 1540 | Score: 270.0 | Avg: 220.90 | Eps: 0.100 | LR: 0.000002 | Steps: 894283\n",
      "Episode 1550 | Score: 230.0 | Avg: 212.05 | Eps: 0.100 | LR: 0.000002 | Steps: 899568\n",
      "Episode 1560 | Score: 210.0 | Avg: 210.35 | Eps: 0.100 | LR: 0.000002 | Steps: 905583\n",
      "\t\tRAM: 81.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1570 | Score: 105.0 | Avg: 212.00 | Eps: 0.100 | LR: 0.000002 | Steps: 912254\n",
      "Episode 1580 | Score: 210.0 | Avg: 207.20 | Eps: 0.100 | LR: 0.000002 | Steps: 917506\n",
      "Episode 1590 | Score: 220.0 | Avg: 203.90 | Eps: 0.100 | LR: 0.000002 | Steps: 925003\n",
      "Episode 1600 | Score: 185.0 | Avg: 206.35 | Eps: 0.100 | LR: 0.000002 | Steps: 931749\n",
      "\t\tNew best avg: 206.35 - saved to DoubleDQN_20251130_170933_best.pth\n",
      "\t\tRAM: 81.5% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1610 | Score: 190.0 | Avg: 205.85 | Eps: 0.100 | LR: 0.000002 | Steps: 937421\n",
      "Episode 1620 | Score: 160.0 | Avg: 213.45 | Eps: 0.100 | LR: 0.000002 | Steps: 943989\n",
      "Episode 1630 | Score: 90.0 | Avg: 209.80 | Eps: 0.100 | LR: 0.000002 | Steps: 949825\n",
      "Episode 1640 | Score: 110.0 | Avg: 218.20 | Eps: 0.100 | LR: 0.000002 | Steps: 956611\n",
      "\t\tRAM: 82.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1650 | Score: 215.0 | Avg: 219.05 | Eps: 0.100 | LR: 0.000002 | Steps: 962733\n",
      "Episode 1660 | Score: 130.0 | Avg: 219.00 | Eps: 0.100 | LR: 0.000002 | Steps: 968874\n",
      "Episode 1670 | Score: 600.0 | Avg: 223.65 | Eps: 0.100 | LR: 0.000002 | Steps: 976289\n",
      "Episode 1680 | Score: 230.0 | Avg: 227.40 | Eps: 0.100 | LR: 0.000002 | Steps: 982480\n",
      "\t\tRAM: 82.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1690 | Score: 155.0 | Avg: 224.50 | Eps: 0.100 | LR: 0.000002 | Steps: 990375\n",
      "Episode 1700 | Score: 155.0 | Avg: 223.80 | Eps: 0.100 | LR: 0.000002 | Steps: 996637\n",
      "Episode 1710 | Score: 105.0 | Avg: 224.70 | Eps: 0.100 | LR: 0.000002 | Steps: 1003605\n",
      "Episode 1720 | Score: 215.0 | Avg: 216.75 | Eps: 0.100 | LR: 0.000002 | Steps: 1010157\n",
      "\t\tRAM: 82.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1730 | Score: 360.0 | Avg: 221.40 | Eps: 0.100 | LR: 0.000002 | Steps: 1017526\n",
      "Episode 1740 | Score: 185.0 | Avg: 209.50 | Eps: 0.100 | LR: 0.000002 | Steps: 1023911\n",
      "Episode 1750 | Score: 195.0 | Avg: 211.35 | Eps: 0.100 | LR: 0.000002 | Steps: 1030756\n",
      "Episode 1760 | Score: 180.0 | Avg: 216.05 | Eps: 0.100 | LR: 0.000002 | Steps: 1038514\n",
      "\t\tRAM: 82.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1770 | Score: 180.0 | Avg: 216.65 | Eps: 0.100 | LR: 0.000002 | Steps: 1045907\n",
      "Episode 1780 | Score: 180.0 | Avg: 223.95 | Eps: 0.100 | LR: 0.000002 | Steps: 1053456\n",
      "Episode 1790 | Score: 595.0 | Avg: 234.10 | Eps: 0.100 | LR: 0.000002 | Steps: 1062559\n",
      "Episode 1800 | Score: 140.0 | Avg: 246.30 | Eps: 0.100 | LR: 0.000002 | Steps: 1071602\n",
      "\t\tRAM: 81.3% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1810 | Score: 210.0 | Avg: 251.00 | Eps: 0.100 | LR: 0.000002 | Steps: 1079563\n",
      "Episode 1820 | Score: 235.0 | Avg: 250.80 | Eps: 0.100 | LR: 0.000002 | Steps: 1086127\n",
      "Episode 1830 | Score: 180.0 | Avg: 252.40 | Eps: 0.100 | LR: 0.000002 | Steps: 1093896\n",
      "Episode 1840 | Score: 215.0 | Avg: 258.00 | Eps: 0.100 | LR: 0.000002 | Steps: 1100943\n",
      "\t\tRAM: 81.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1850 | Score: 185.0 | Avg: 262.20 | Eps: 0.100 | LR: 0.000002 | Steps: 1107737\n",
      "Episode 1860 | Score: 180.0 | Avg: 257.55 | Eps: 0.100 | LR: 0.000002 | Steps: 1114194\n",
      "Episode 1870 | Score: 210.0 | Avg: 255.30 | Eps: 0.100 | LR: 0.000002 | Steps: 1120708\n",
      "Episode 1880 | Score: 270.0 | Avg: 254.25 | Eps: 0.100 | LR: 0.000002 | Steps: 1128299\n",
      "\t\tRAM: 81.6% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1890 | Score: 210.0 | Avg: 246.65 | Eps: 0.100 | LR: 0.000002 | Steps: 1135844\n",
      "Episode 1900 | Score: 285.0 | Avg: 234.70 | Eps: 0.100 | LR: 0.000001 | Steps: 1143251\n",
      "Episode 1910 | Score: 170.0 | Avg: 235.80 | Eps: 0.100 | LR: 0.000001 | Steps: 1152051\n",
      "Episode 1920 | Score: 260.0 | Avg: 236.95 | Eps: 0.100 | LR: 0.000001 | Steps: 1159646\n",
      "\t\tRAM: 82.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1930 | Score: 345.0 | Avg: 239.30 | Eps: 0.100 | LR: 0.000001 | Steps: 1167910\n",
      "Episode 1940 | Score: 110.0 | Avg: 241.80 | Eps: 0.100 | LR: 0.000001 | Steps: 1175341\n",
      "Episode 1950 | Score: 180.0 | Avg: 241.50 | Eps: 0.100 | LR: 0.000001 | Steps: 1182652\n",
      "Episode 1960 | Score: 380.0 | Avg: 250.95 | Eps: 0.100 | LR: 0.000001 | Steps: 1190942\n",
      "\t\tRAM: 81.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 1970 | Score: 180.0 | Avg: 252.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1197893\n",
      "Episode 1980 | Score: 380.0 | Avg: 253.30 | Eps: 0.100 | LR: 0.000001 | Steps: 1205261\n",
      "Episode 1990 | Score: 315.0 | Avg: 260.55 | Eps: 0.100 | LR: 0.000001 | Steps: 1213146\n",
      "Episode 2000 | Score: 240.0 | Avg: 269.80 | Eps: 0.100 | LR: 0.000001 | Steps: 1220962\n",
      "\t\tNew best avg: 269.80 - saved to DoubleDQN_20251130_191620_best.pth\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2010 | Score: 180.0 | Avg: 269.90 | Eps: 0.100 | LR: 0.000001 | Steps: 1229146\n",
      "Episode 2020 | Score: 210.0 | Avg: 271.60 | Eps: 0.100 | LR: 0.000001 | Steps: 1236721\n",
      "Episode 2030 | Score: 180.0 | Avg: 271.00 | Eps: 0.100 | LR: 0.000001 | Steps: 1244687\n",
      "Episode 2040 | Score: 130.0 | Avg: 267.05 | Eps: 0.100 | LR: 0.000001 | Steps: 1251830\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2050 | Score: 460.0 | Avg: 273.40 | Eps: 0.100 | LR: 0.000001 | Steps: 1259207\n",
      "Episode 2060 | Score: 285.0 | Avg: 269.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1266110\n",
      "Episode 2070 | Score: 180.0 | Avg: 268.70 | Eps: 0.100 | LR: 0.000001 | Steps: 1274240\n",
      "Episode 2080 | Score: 310.0 | Avg: 266.00 | Eps: 0.100 | LR: 0.000001 | Steps: 1281548\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2090 | Score: 365.0 | Avg: 255.90 | Eps: 0.100 | LR: 0.000001 | Steps: 1288375\n",
      "Episode 2100 | Score: 210.0 | Avg: 251.55 | Eps: 0.100 | LR: 0.000001 | Steps: 1297038\n",
      "Episode 2110 | Score: 240.0 | Avg: 246.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1303871\n",
      "Episode 2120 | Score: 210.0 | Avg: 244.70 | Eps: 0.100 | LR: 0.000001 | Steps: 1311134\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2130 | Score: 285.0 | Avg: 242.05 | Eps: 0.100 | LR: 0.000001 | Steps: 1318609\n",
      "Episode 2140 | Score: 210.0 | Avg: 242.65 | Eps: 0.100 | LR: 0.000001 | Steps: 1325684\n",
      "Episode 2150 | Score: 260.0 | Avg: 238.75 | Eps: 0.100 | LR: 0.000001 | Steps: 1333202\n",
      "Episode 2160 | Score: 180.0 | Avg: 238.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1340872\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2170 | Score: 240.0 | Avg: 234.10 | Eps: 0.100 | LR: 0.000001 | Steps: 1347631\n",
      "Episode 2180 | Score: 260.0 | Avg: 234.25 | Eps: 0.100 | LR: 0.000001 | Steps: 1355205\n",
      "Episode 2190 | Score: 105.0 | Avg: 234.60 | Eps: 0.100 | LR: 0.000001 | Steps: 1362799\n",
      "Episode 2200 | Score: 265.0 | Avg: 229.20 | Eps: 0.100 | LR: 0.000001 | Steps: 1369971\n",
      "\t\tRAM: 82.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2210 | Score: 180.0 | Avg: 227.30 | Eps: 0.100 | LR: 0.000001 | Steps: 1377157\n",
      "Episode 2220 | Score: 380.0 | Avg: 235.00 | Eps: 0.100 | LR: 0.000001 | Steps: 1384872\n",
      "Episode 2230 | Score: 315.0 | Avg: 237.60 | Eps: 0.100 | LR: 0.000001 | Steps: 1393007\n",
      "Episode 2240 | Score: 265.0 | Avg: 242.05 | Eps: 0.100 | LR: 0.000001 | Steps: 1400608\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2250 | Score: 260.0 | Avg: 239.20 | Eps: 0.100 | LR: 0.000001 | Steps: 1407675\n",
      "Episode 2260 | Score: 425.0 | Avg: 238.60 | Eps: 0.100 | LR: 0.000001 | Steps: 1414995\n",
      "Episode 2270 | Score: 265.0 | Avg: 238.60 | Eps: 0.100 | LR: 0.000001 | Steps: 1422173\n",
      "Episode 2280 | Score: 210.0 | Avg: 238.60 | Eps: 0.100 | LR: 0.000001 | Steps: 1430177\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2290 | Score: 180.0 | Avg: 235.95 | Eps: 0.100 | LR: 0.000001 | Steps: 1436776\n",
      "Episode 2300 | Score: 230.0 | Avg: 235.95 | Eps: 0.100 | LR: 0.000001 | Steps: 1444976\n",
      "Episode 2310 | Score: 260.0 | Avg: 240.55 | Eps: 0.100 | LR: 0.000001 | Steps: 1452841\n",
      "Episode 2320 | Score: 210.0 | Avg: 229.75 | Eps: 0.100 | LR: 0.000001 | Steps: 1459315\n",
      "\t\tRAM: 80.6% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2330 | Score: 210.0 | Avg: 226.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1467182\n",
      "Episode 2340 | Score: 220.0 | Avg: 225.70 | Eps: 0.100 | LR: 0.000001 | Steps: 1474786\n",
      "Episode 2350 | Score: 210.0 | Avg: 226.40 | Eps: 0.100 | LR: 0.000001 | Steps: 1481912\n",
      "Episode 2360 | Score: 155.0 | Avg: 223.90 | Eps: 0.100 | LR: 0.000001 | Steps: 1488578\n",
      "\t\tRAM: 82.5% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2370 | Score: 180.0 | Avg: 224.55 | Eps: 0.100 | LR: 0.000001 | Steps: 1496223\n",
      "Episode 2380 | Score: 355.0 | Avg: 224.75 | Eps: 0.100 | LR: 0.000001 | Steps: 1503777\n",
      "Episode 2390 | Score: 155.0 | Avg: 225.85 | Eps: 0.100 | LR: 0.000001 | Steps: 1510555\n",
      "Episode 2400 | Score: 180.0 | Avg: 226.05 | Eps: 0.100 | LR: 0.000001 | Steps: 1517328\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2410 | Score: 180.0 | Avg: 226.00 | Eps: 0.100 | LR: 0.000001 | Steps: 1525196\n",
      "Episode 2420 | Score: 180.0 | Avg: 228.05 | Eps: 0.100 | LR: 0.000001 | Steps: 1532660\n",
      "Episode 2430 | Score: 180.0 | Avg: 228.95 | Eps: 0.100 | LR: 0.000001 | Steps: 1539456\n",
      "Episode 2440 | Score: 180.0 | Avg: 222.40 | Eps: 0.100 | LR: 0.000001 | Steps: 1546149\n",
      "\t\tRAM: 81.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2450 | Score: 155.0 | Avg: 220.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1552632\n",
      "Episode 2460 | Score: 210.0 | Avg: 216.30 | Eps: 0.100 | LR: 0.000001 | Steps: 1558940\n",
      "Episode 2470 | Score: 240.0 | Avg: 214.95 | Eps: 0.100 | LR: 0.000001 | Steps: 1566005\n",
      "Episode 2480 | Score: 285.0 | Avg: 215.65 | Eps: 0.100 | LR: 0.000001 | Steps: 1573131\n",
      "\t\tRAM: 81.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2490 | Score: 180.0 | Avg: 217.25 | Eps: 0.100 | LR: 0.000001 | Steps: 1580482\n",
      "Episode 2500 | Score: 210.0 | Avg: 220.50 | Eps: 0.100 | LR: 0.000001 | Steps: 1588964\n",
      "Episode 2510 | Score: 210.0 | Avg: 215.85 | Eps: 0.100 | LR: 0.000001 | Steps: 1596162\n",
      "Episode 2520 | Score: 380.0 | Avg: 216.80 | Eps: 0.100 | LR: 0.000001 | Steps: 1602952\n",
      "\t\tRAM: 82.5% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2530 | Score: 250.0 | Avg: 215.90 | Eps: 0.100 | LR: 0.000001 | Steps: 1610672\n",
      "Episode 2540 | Score: 690.0 | Avg: 220.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1618499\n",
      "Episode 2550 | Score: 210.0 | Avg: 224.70 | Eps: 0.100 | LR: 0.000001 | Steps: 1626099\n",
      "Episode 2560 | Score: 180.0 | Avg: 228.05 | Eps: 0.100 | LR: 0.000001 | Steps: 1633196\n",
      "\t\tRAM: 81.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2570 | Score: 180.0 | Avg: 228.65 | Eps: 0.100 | LR: 0.000001 | Steps: 1640003\n",
      "Episode 2580 | Score: 180.0 | Avg: 227.10 | Eps: 0.100 | LR: 0.000001 | Steps: 1648190\n",
      "Episode 2590 | Score: 230.0 | Avg: 226.75 | Eps: 0.100 | LR: 0.000001 | Steps: 1655624\n",
      "Episode 2600 | Score: 310.0 | Avg: 221.85 | Eps: 0.100 | LR: 0.000001 | Steps: 1662440\n",
      "\t\tRAM: 81.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2610 | Score: 275.0 | Avg: 225.15 | Eps: 0.100 | LR: 0.000001 | Steps: 1669646\n",
      "Episode 2620 | Score: 185.0 | Avg: 228.40 | Eps: 0.100 | LR: 0.000001 | Steps: 1678272\n",
      "Episode 2630 | Score: 180.0 | Avg: 225.40 | Eps: 0.100 | LR: 0.000001 | Steps: 1684934\n",
      "Episode 2640 | Score: 180.0 | Avg: 224.00 | Eps: 0.100 | LR: 0.000001 | Steps: 1692041\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2650 | Score: 180.0 | Avg: 219.95 | Eps: 0.100 | LR: 0.000001 | Steps: 1699317\n",
      "Episode 2660 | Score: 180.0 | Avg: 221.25 | Eps: 0.100 | LR: 0.000001 | Steps: 1706628\n",
      "Episode 2670 | Score: 180.0 | Avg: 221.25 | Eps: 0.100 | LR: 0.000001 | Steps: 1713535\n",
      "Episode 2680 | Score: 210.0 | Avg: 219.10 | Eps: 0.100 | LR: 0.000001 | Steps: 1720258\n",
      "\t\tRAM: 81.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2690 | Score: 260.0 | Avg: 219.55 | Eps: 0.100 | LR: 0.000001 | Steps: 1727990\n",
      "Episode 2700 | Score: 180.0 | Avg: 224.80 | Eps: 0.100 | LR: 0.000001 | Steps: 1735902\n",
      "Episode 2710 | Score: 180.0 | Avg: 220.15 | Eps: 0.100 | LR: 0.000001 | Steps: 1742428\n",
      "Episode 2720 | Score: 180.0 | Avg: 218.00 | Eps: 0.100 | LR: 0.000001 | Steps: 1749624\n",
      "\t\tRAM: 81.5% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2730 | Score: 30.0 | Avg: 220.00 | Eps: 0.100 | LR: 0.000001 | Steps: 1756389\n",
      "Episode 2740 | Score: 240.0 | Avg: 218.55 | Eps: 0.100 | LR: 0.000001 | Steps: 1763419\n",
      "Episode 2750 | Score: 180.0 | Avg: 221.15 | Eps: 0.100 | LR: 0.000001 | Steps: 1771385\n",
      "Episode 2760 | Score: 180.0 | Avg: 219.40 | Eps: 0.100 | LR: 0.000001 | Steps: 1778539\n",
      "\t\tRAM: 81.6% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2770 | Score: 180.0 | Avg: 218.70 | Eps: 0.100 | LR: 0.000001 | Steps: 1785472\n",
      "Episode 2780 | Score: 285.0 | Avg: 223.65 | Eps: 0.100 | LR: 0.000001 | Steps: 1793814\n",
      "Episode 2790 | Score: 180.0 | Avg: 223.70 | Eps: 0.100 | LR: 0.000001 | Steps: 1800774\n",
      "Episode 2800 | Score: 180.0 | Avg: 218.85 | Eps: 0.100 | LR: 0.000001 | Steps: 1807534\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2810 | Score: 180.0 | Avg: 220.75 | Eps: 0.100 | LR: 0.000001 | Steps: 1814969\n",
      "Episode 2820 | Score: 285.0 | Avg: 224.45 | Eps: 0.100 | LR: 0.000001 | Steps: 1823485\n",
      "Episode 2830 | Score: 180.0 | Avg: 222.75 | Eps: 0.100 | LR: 0.000001 | Steps: 1830471\n",
      "Episode 2840 | Score: 180.0 | Avg: 223.40 | Eps: 0.100 | LR: 0.000001 | Steps: 1837725\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2850 | Score: 210.0 | Avg: 221.15 | Eps: 0.100 | LR: 0.000001 | Steps: 1846084\n",
      "Episode 2860 | Score: 180.0 | Avg: 221.80 | Eps: 0.100 | LR: 0.000001 | Steps: 1853258\n",
      "Episode 2870 | Score: 410.0 | Avg: 224.65 | Eps: 0.100 | LR: 0.000001 | Steps: 1860307\n",
      "Episode 2880 | Score: 180.0 | Avg: 220.20 | Eps: 0.100 | LR: 0.000001 | Steps: 1868056\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2890 | Score: 180.0 | Avg: 218.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1874916\n",
      "Episode 2900 | Score: 210.0 | Avg: 223.15 | Eps: 0.100 | LR: 0.000001 | Steps: 1882407\n",
      "Episode 2910 | Score: 240.0 | Avg: 223.20 | Eps: 0.100 | LR: 0.000001 | Steps: 1890053\n",
      "Episode 2920 | Score: 180.0 | Avg: 226.25 | Eps: 0.100 | LR: 0.000001 | Steps: 1898580\n",
      "\t\tRAM: 82.5% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2930 | Score: 210.0 | Avg: 229.45 | Eps: 0.100 | LR: 0.000001 | Steps: 1905437\n",
      "Episode 2940 | Score: 260.0 | Avg: 226.35 | Eps: 0.100 | LR: 0.000001 | Steps: 1911914\n",
      "Episode 2950 | Score: 180.0 | Avg: 222.30 | Eps: 0.100 | LR: 0.000001 | Steps: 1918259\n",
      "Episode 2960 | Score: 260.0 | Avg: 223.40 | Eps: 0.100 | LR: 0.000001 | Steps: 1926140\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 2970 | Score: 345.0 | Avg: 222.25 | Eps: 0.100 | LR: 0.000001 | Steps: 1933200\n",
      "Episode 2980 | Score: 155.0 | Avg: 219.65 | Eps: 0.100 | LR: 0.000001 | Steps: 1939695\n",
      "Episode 2990 | Score: 180.0 | Avg: 221.65 | Eps: 0.100 | LR: 0.000001 | Steps: 1946446\n",
      "Episode 3000 | Score: 180.0 | Avg: 216.20 | Eps: 0.100 | LR: 0.000000 | Steps: 1953235\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3010 | Score: 180.0 | Avg: 216.50 | Eps: 0.100 | LR: 0.000000 | Steps: 1960672\n",
      "Episode 3020 | Score: 180.0 | Avg: 205.55 | Eps: 0.100 | LR: 0.000000 | Steps: 1967063\n",
      "Episode 3030 | Score: 180.0 | Avg: 205.55 | Eps: 0.100 | LR: 0.000000 | Steps: 1974119\n",
      "Episode 3040 | Score: 155.0 | Avg: 207.80 | Eps: 0.100 | LR: 0.000000 | Steps: 1981475\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3050 | Score: 180.0 | Avg: 209.75 | Eps: 0.100 | LR: 0.000000 | Steps: 1988296\n",
      "Episode 3060 | Score: 180.0 | Avg: 208.30 | Eps: 0.100 | LR: 0.000000 | Steps: 1995044\n",
      "Episode 3070 | Score: 180.0 | Avg: 202.95 | Eps: 0.100 | LR: 0.000000 | Steps: 2001319\n",
      "Episode 3080 | Score: 50.0 | Avg: 202.40 | Eps: 0.100 | LR: 0.000000 | Steps: 2007584\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3090 | Score: 155.0 | Avg: 197.65 | Eps: 0.100 | LR: 0.000000 | Steps: 2014000\n",
      "Episode 3100 | Score: 155.0 | Avg: 196.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2020608\n",
      "Episode 3110 | Score: 180.0 | Avg: 193.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2027111\n",
      "Episode 3120 | Score: 180.0 | Avg: 192.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2033715\n",
      "\t\tRAM: 81.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3130 | Score: 180.0 | Avg: 189.45 | Eps: 0.100 | LR: 0.000000 | Steps: 2040444\n",
      "Episode 3140 | Score: 180.0 | Avg: 189.05 | Eps: 0.100 | LR: 0.000000 | Steps: 2047058\n",
      "Episode 3150 | Score: 155.0 | Avg: 187.80 | Eps: 0.100 | LR: 0.000000 | Steps: 2053587\n",
      "Episode 3160 | Score: 180.0 | Avg: 184.35 | Eps: 0.100 | LR: 0.000000 | Steps: 2059934\n",
      "\t\tRAM: 82.6% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3170 | Score: 210.0 | Avg: 185.55 | Eps: 0.100 | LR: 0.000000 | Steps: 2066319\n",
      "Episode 3180 | Score: 220.0 | Avg: 188.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2073616\n",
      "Episode 3190 | Score: 180.0 | Avg: 191.75 | Eps: 0.100 | LR: 0.000000 | Steps: 2080460\n",
      "Episode 3200 | Score: 155.0 | Avg: 192.65 | Eps: 0.100 | LR: 0.000000 | Steps: 2086950\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3210 | Score: 170.0 | Avg: 192.05 | Eps: 0.100 | LR: 0.000000 | Steps: 2093975\n",
      "Episode 3220 | Score: 410.0 | Avg: 194.45 | Eps: 0.100 | LR: 0.000000 | Steps: 2100486\n",
      "Episode 3230 | Score: 180.0 | Avg: 196.30 | Eps: 0.100 | LR: 0.000000 | Steps: 2107348\n",
      "Episode 3240 | Score: 180.0 | Avg: 193.60 | Eps: 0.100 | LR: 0.000000 | Steps: 2113830\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3250 | Score: 155.0 | Avg: 194.90 | Eps: 0.100 | LR: 0.000000 | Steps: 2120674\n",
      "Episode 3260 | Score: 180.0 | Avg: 196.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2127411\n",
      "Episode 3270 | Score: 180.0 | Avg: 199.35 | Eps: 0.100 | LR: 0.000000 | Steps: 2133992\n",
      "Episode 3280 | Score: 180.0 | Avg: 200.30 | Eps: 0.100 | LR: 0.000000 | Steps: 2140949\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3290 | Score: 180.0 | Avg: 199.65 | Eps: 0.100 | LR: 0.000000 | Steps: 2147771\n",
      "Episode 3300 | Score: 180.0 | Avg: 200.40 | Eps: 0.100 | LR: 0.000000 | Steps: 2154141\n",
      "Episode 3310 | Score: 180.0 | Avg: 203.80 | Eps: 0.100 | LR: 0.000000 | Steps: 2161278\n",
      "Episode 3320 | Score: 180.0 | Avg: 204.85 | Eps: 0.100 | LR: 0.000000 | Steps: 2167932\n",
      "\t\tRAM: 82.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3330 | Score: 210.0 | Avg: 203.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2174504\n",
      "Episode 3340 | Score: 180.0 | Avg: 206.20 | Eps: 0.100 | LR: 0.000000 | Steps: 2181127\n",
      "Episode 3350 | Score: 285.0 | Avg: 207.60 | Eps: 0.100 | LR: 0.000000 | Steps: 2188467\n",
      "Episode 3360 | Score: 180.0 | Avg: 210.55 | Eps: 0.100 | LR: 0.000000 | Steps: 2195198\n",
      "\t\tRAM: 82.6% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3370 | Score: 180.0 | Avg: 213.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2202316\n",
      "Episode 3380 | Score: 180.0 | Avg: 212.75 | Eps: 0.100 | LR: 0.000000 | Steps: 2209560\n",
      "Episode 3390 | Score: 155.0 | Avg: 215.30 | Eps: 0.100 | LR: 0.000000 | Steps: 2216404\n",
      "Episode 3400 | Score: 240.0 | Avg: 215.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2223059\n",
      "\t\tRAM: 82.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3410 | Score: 180.0 | Avg: 217.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2229939\n",
      "Episode 3420 | Score: 260.0 | Avg: 214.45 | Eps: 0.100 | LR: 0.000000 | Steps: 2236644\n",
      "Episode 3430 | Score: 180.0 | Avg: 213.15 | Eps: 0.100 | LR: 0.000000 | Steps: 2242857\n",
      "Episode 3440 | Score: 180.0 | Avg: 209.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2249257\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3450 | Score: 260.0 | Avg: 208.05 | Eps: 0.100 | LR: 0.000000 | Steps: 2255922\n",
      "Episode 3460 | Score: 180.0 | Avg: 204.45 | Eps: 0.100 | LR: 0.000000 | Steps: 2262591\n",
      "Episode 3470 | Score: 175.0 | Avg: 199.75 | Eps: 0.100 | LR: 0.000000 | Steps: 2269336\n",
      "Episode 3480 | Score: 180.0 | Avg: 198.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2276241\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3490 | Score: 180.0 | Avg: 195.05 | Eps: 0.100 | LR: 0.000000 | Steps: 2282851\n",
      "Episode 3500 | Score: 180.0 | Avg: 193.95 | Eps: 0.100 | LR: 0.000000 | Steps: 2289525\n",
      "Episode 3510 | Score: 180.0 | Avg: 190.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2296046\n",
      "Episode 3520 | Score: 180.0 | Avg: 189.00 | Eps: 0.100 | LR: 0.000000 | Steps: 2302550\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3530 | Score: 180.0 | Avg: 190.05 | Eps: 0.100 | LR: 0.000000 | Steps: 2309185\n",
      "Episode 3540 | Score: 210.0 | Avg: 193.85 | Eps: 0.100 | LR: 0.000000 | Steps: 2316213\n",
      "Episode 3550 | Score: 180.0 | Avg: 191.90 | Eps: 0.100 | LR: 0.000000 | Steps: 2322708\n",
      "Episode 3560 | Score: 180.0 | Avg: 191.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2329229\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3570 | Score: 180.0 | Avg: 194.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2335958\n",
      "Episode 3580 | Score: 180.0 | Avg: 193.90 | Eps: 0.100 | LR: 0.000000 | Steps: 2342545\n",
      "Episode 3590 | Score: 180.0 | Avg: 195.60 | Eps: 0.100 | LR: 0.000000 | Steps: 2349550\n",
      "Episode 3600 | Score: 110.0 | Avg: 193.25 | Eps: 0.100 | LR: 0.000000 | Steps: 2355846\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3610 | Score: 180.0 | Avg: 196.80 | Eps: 0.100 | LR: 0.000000 | Steps: 2362634\n",
      "Episode 3620 | Score: 180.0 | Avg: 195.35 | Eps: 0.100 | LR: 0.000000 | Steps: 2368898\n",
      "Episode 3630 | Score: 180.0 | Avg: 195.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2375237\n",
      "Episode 3640 | Score: 180.0 | Avg: 192.90 | Eps: 0.100 | LR: 0.000000 | Steps: 2381703\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3650 | Score: 180.0 | Avg: 192.35 | Eps: 0.100 | LR: 0.000000 | Steps: 2387607\n",
      "Episode 3660 | Score: 180.0 | Avg: 194.50 | Eps: 0.100 | LR: 0.000000 | Steps: 2393970\n",
      "Episode 3670 | Score: 235.0 | Avg: 196.95 | Eps: 0.100 | LR: 0.000000 | Steps: 2401482\n",
      "Episode 3680 | Score: 210.0 | Avg: 196.45 | Eps: 0.100 | LR: 0.000000 | Steps: 2408330\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3690 | Score: 180.0 | Avg: 198.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2414932\n",
      "Episode 3700 | Score: 265.0 | Avg: 202.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2421880\n",
      "Episode 3710 | Score: 180.0 | Avg: 199.85 | Eps: 0.100 | LR: 0.000000 | Steps: 2428484\n",
      "Episode 3720 | Score: 105.0 | Avg: 201.25 | Eps: 0.100 | LR: 0.000000 | Steps: 2434783\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3730 | Score: 155.0 | Avg: 201.90 | Eps: 0.100 | LR: 0.000000 | Steps: 2441284\n",
      "Episode 3740 | Score: 180.0 | Avg: 205.45 | Eps: 0.100 | LR: 0.000000 | Steps: 2448243\n",
      "Episode 3750 | Score: 180.0 | Avg: 210.15 | Eps: 0.100 | LR: 0.000000 | Steps: 2455280\n",
      "Episode 3760 | Score: 405.0 | Avg: 209.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2461911\n",
      "\t\tRAM: 82.1% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3770 | Score: 155.0 | Avg: 209.30 | Eps: 0.100 | LR: 0.000000 | Steps: 2468950\n",
      "Episode 3780 | Score: 260.0 | Avg: 208.60 | Eps: 0.100 | LR: 0.000000 | Steps: 2475444\n",
      "Episode 3790 | Score: 210.0 | Avg: 206.35 | Eps: 0.100 | LR: 0.000000 | Steps: 2482394\n",
      "Episode 3800 | Score: 210.0 | Avg: 203.80 | Eps: 0.100 | LR: 0.000000 | Steps: 2489009\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3810 | Score: 105.0 | Avg: 202.60 | Eps: 0.100 | LR: 0.000000 | Steps: 2495375\n",
      "Episode 3820 | Score: 155.0 | Avg: 201.55 | Eps: 0.100 | LR: 0.000000 | Steps: 2501689\n",
      "Episode 3830 | Score: 285.0 | Avg: 201.90 | Eps: 0.100 | LR: 0.000000 | Steps: 2508658\n",
      "Episode 3840 | Score: 155.0 | Avg: 197.65 | Eps: 0.100 | LR: 0.000000 | Steps: 2515192\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3850 | Score: 180.0 | Avg: 193.05 | Eps: 0.100 | LR: 0.000000 | Steps: 2521524\n",
      "Episode 3860 | Score: 180.0 | Avg: 191.80 | Eps: 0.100 | LR: 0.000000 | Steps: 2527976\n",
      "Episode 3870 | Score: 185.0 | Avg: 186.35 | Eps: 0.100 | LR: 0.000000 | Steps: 2534596\n",
      "Episode 3880 | Score: 180.0 | Avg: 189.60 | Eps: 0.100 | LR: 0.000000 | Steps: 2541456\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3890 | Score: 180.0 | Avg: 191.90 | Eps: 0.100 | LR: 0.000000 | Steps: 2548701\n",
      "Episode 3900 | Score: 180.0 | Avg: 192.95 | Eps: 0.100 | LR: 0.000000 | Steps: 2555881\n",
      "Episode 3910 | Score: 180.0 | Avg: 193.80 | Eps: 0.100 | LR: 0.000000 | Steps: 2562521\n",
      "Episode 3920 | Score: 260.0 | Avg: 197.60 | Eps: 0.100 | LR: 0.000000 | Steps: 2568945\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3930 | Score: 620.0 | Avg: 202.10 | Eps: 0.100 | LR: 0.000000 | Steps: 2575843\n",
      "Episode 3940 | Score: 180.0 | Avg: 203.80 | Eps: 0.100 | LR: 0.000000 | Steps: 2582411\n",
      "Episode 3950 | Score: 210.0 | Avg: 205.70 | Eps: 0.100 | LR: 0.000000 | Steps: 2589044\n",
      "Episode 3960 | Score: 290.0 | Avg: 208.75 | Eps: 0.100 | LR: 0.000000 | Steps: 2595856\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 100000/100000\n",
      "Episode 3970 | Score: 180.0 | Avg: 212.45 | Eps: 0.100 | LR: 0.000000 | Steps: 2602212\n",
      "Episode 3980 | Score: 210.0 | Avg: 211.20 | Eps: 0.100 | LR: 0.000000 | Steps: 2608910\n",
      "Episode 3990 | Score: 210.0 | Avg: 210.65 | Eps: 0.100 | LR: 0.000000 | Steps: 2615857\n",
      "\n",
      "--- TRAINING FINISHED: 2025-12-01 05:23:55 ---\n",
      "--- DURATION: 19:11:41.998732 ---\n",
      "\n",
      "‚úÖ Training completed successfully!\n",
      "Best average score: 269.80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Double DQN\n",
    "CONFIG_DDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DDQN['DQN_TYPE'] = 'DoubleDQN'\n",
    "CONFIG_DDQN['LEARNING_RATE'] = 0.00001\n",
    "# CONFIG_DDQN['LEARNING_RATE'] = 0.0005\n",
    "CONFIG_DDQN['TARGET_UPDATE'] = 1000\n",
    "CONFIG_DDQN['EPSILON_DECAY'] = 100000\n",
    "# CONFIG_DDQN['EPSILON_DECAY'] = 25000\n",
    "CONFIG_DDQN['BUFFER_SIZE'] = 150000\n",
    "# CONFIG_DDQN['BUFFER_SIZE'] = 25000\n",
    "CONFIG_DDQN['N_EPISODES'] = 4000\n",
    "CONFIG_DDQN['BATCH_SIZE'] = 32\n",
    "CONFIG_DDQN['PER_BETA_FRAMES'] = 120000\n",
    "CONFIG_DDQN['USE_PER'] = False\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DDQN['SEED'])\n",
    "np.random.seed(CONFIG_DDQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DDQN['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_ddqn = DQN((CONFIG_DDQN['N_FRAMES'], 84, 84), CONFIG_DDQN['N_ACTIONS']).to(device)\n",
    "target_net_ddqn = DQN((CONFIG_DDQN['N_FRAMES'], 84, 84), CONFIG_DDQN['N_ACTIONS']).to(device)\n",
    "target_net_ddqn.load_state_dict(policy_net_ddqn.state_dict())\n",
    "\n",
    "optimizer_ddqn = optim.Adam(policy_net_ddqn.parameters(), lr=CONFIG_DDQN['LEARNING_RATE'])\n",
    "\n",
    "if CONFIG_DDQN['USE_PER']:\n",
    "    replay_buffer = PrioritizedReplayBuffer(\n",
    "        CONFIG_DDQN.get('BUFFER_SIZE', 10000),\n",
    "        alpha=CONFIG_DDQN.get('PER_ALPHA', 0.6),\n",
    "        beta_start=CONFIG_DDQN.get('PER_BETA_START', 0.4),\n",
    "        beta_frames=CONFIG_DDQN.get('PER_BETA_FRAMES', 100000)\n",
    "    )\n",
    "    print(\"   ‚úÖ Using Prioritized Experience Replay (PER)\")\n",
    "    print(f\"      Alpha: {CONFIG_DDQN.get('PER_ALPHA', 0.6)}\")\n",
    "    print(f\"      Beta: {CONFIG_DDQN.get('PER_BETA_START', 0.4)} ‚Üí 1.0\")\n",
    "else:\n",
    "    replay_buffer = ReplayBuffer(CONFIG_DDQN.get('BUFFER_SIZE', 10000))\n",
    "    print(\"   ‚úÖ Using Standard Uniform Replay Buffer\")\n",
    "\n",
    "# Train\n",
    "rewards_ddqn = train_dqn(CONFIG_DDQN, policy_net_ddqn, target_net_ddqn, \n",
    "                         optimizer_ddqn, replay_buffer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Dueling DQN\n",
    "CONFIG_DuelDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DuelDQN['DQN_TYPE'] = 'DuelingDQN'\n",
    "CONFIG_DuelDQN['USE_PER'] = False\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DuelDQN['SEED'])\n",
    "np.random.seed(CONFIG_DuelDQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DuelDQN['SEED'])\n",
    "\n",
    "# Networks - Use DuelingDQN architecture\n",
    "policy_net_dueling = DuelingDQN((CONFIG_DuelDQN['N_FRAMES'], 84, 84), CONFIG_DuelDQN['N_ACTIONS']).to(device)\n",
    "target_net_dueling = DuelingDQN((CONFIG_DuelDQN['N_FRAMES'], 84, 84), CONFIG_DuelDQN['N_ACTIONS']).to(device)\n",
    "target_net_dueling.load_state_dict(policy_net_dueling.state_dict())\n",
    "\n",
    "optimizer_dueling = optim.Adam(policy_net_dueling.parameters(), lr=CONFIG_DuelDQN['LEARNING_RATE'])\n",
    "replay_buffer_dueling = ReplayBuffer(CONFIG_DuelDQN['BUFFER_SIZE'])\n",
    "\n",
    "# Train\n",
    "rewards_dueling = train_dqn(CONFIG_DuelDQN, policy_net_dueling, target_net_dueling, \n",
    "                            optimizer_dueling, replay_buffer_dueling, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - DQN with PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for DQN with Prioritized Experience Replay\n",
    "CONFIG_PER = BASE_CONFIG.copy()\n",
    "CONFIG_PER['DQN_TYPE'] = 'DQN'\n",
    "CONFIG_PER['USE_PER'] = True\n",
    "\n",
    "CONFIG_PER['LEARNING_RATE'] = 0.00035\n",
    "CONFIG_PER['TARGET_UPDATE'] = 2000\n",
    "CONFIG_PER['EPSILON_DECAY'] = 20000\n",
    "\n",
    "CONFIG_PER['BUFFER_SIZE'] = 20000\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_PER['SEED'])\n",
    "np.random.seed(CONFIG_PER['SEED'])\n",
    "torch.manual_seed(CONFIG_PER['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_per = DQN((CONFIG_PER['N_FRAMES'], 84, 84), CONFIG_PER['N_ACTIONS']).to(device)\n",
    "target_net_per = DQN((CONFIG_PER['N_FRAMES'], 84, 84), CONFIG_PER['N_ACTIONS']).to(device)\n",
    "target_net_per.load_state_dict(policy_net_per.state_dict())\n",
    "\n",
    "optimizer_per = optim.Adam(policy_net_per.parameters(), lr=CONFIG_PER['LEARNING_RATE'])\n",
    "replay_buffer_per = PrioritizedReplayBuffer(\n",
    "    CONFIG_PER['BUFFER_SIZE'],\n",
    "    alpha=CONFIG_PER['PER_ALPHA'],\n",
    "    beta_start=CONFIG_PER['PER_BETA_START'],\n",
    "    beta_frames=CONFIG_PER['PER_BETA_FRAMES']\n",
    ")\n",
    "\n",
    "# Train\n",
    "rewards_per = train_dqn(CONFIG_PER, policy_net_per, target_net_per, \n",
    "                        optimizer_per, replay_buffer_per, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidated Results Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {\n",
    "    'DQN': rewards_dqn,\n",
    "    'DoubleDQN': rewards_ddqn,\n",
    "    'DuelingDQN': rewards_dueling,\n",
    "    'DQN_PER': rewards_per\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consolidated_results(results_dict, window=100, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Plot consolidated training progress for multiple DQN runs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    for idx, (name, rewards) in enumerate(results_dict.items()):\n",
    "        # Calculate moving average\n",
    "        if len(rewards) >= window:\n",
    "            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            episodes = range(window-1, len(rewards))\n",
    "            final_avg = np.mean(rewards[-100:])\n",
    "            \n",
    "            # Plot moving average\n",
    "            plt.plot(episodes, moving_avg, \n",
    "                    label=f'{name} (Avg={final_avg:.2f})',\n",
    "                    color=colors[idx % len(colors)],\n",
    "                    linewidth=2)\n",
    "    \n",
    "    # Add goal lines\n",
    "    plt.axhline(y=500, color='green', linestyle='--', linewidth=2, label='Goal: 500', alpha=0.7)\n",
    "    plt.axhline(y=400, color='red', linestyle='--', linewidth=2, label='Goal: 400', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Episode #', fontsize=12)\n",
    "    plt.ylabel(f'Average Score ({window}-Game Window)', fontsize=12)\n",
    "    plt.title(f'Consolidated DQN Training Progress ({window}-Episode Moving Average)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_individual_results(rewards, name, window=100, figsize=(12, 6)):\n",
    "    \"\"\"Plot individual run results\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), moving_avg, label=f'Moving Average ({window})', linewidth=2)\n",
    "    \n",
    "    plt.axhline(y=500, color='r', linestyle='--', label='Target (500)')\n",
    "    plt.axhline(y=400, color='orange', linestyle='--', label='Minimum (400)')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'{name} - Training Progress on Space Invaders')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    final_avg = np.mean(rewards[-100:])\n",
    "    print(f\"\\n{name} - Final average reward (last 100 episodes): {final_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot consolidated results\n",
    "plot_consolidated_results(all_results, window=100)\n",
    "\n",
    "# Plot individual results\n",
    "for name, rewards in all_results.items():\n",
    "    plot_individual_results(rewards, name, window=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to a single file\n",
    "import pickle\n",
    "\n",
    "results_file = os.path.join(CHECKPOINT_DIR, 'all_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"All results saved to: {results_file}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for name, rewards in all_results.items():\n",
    "    final_avg = np.mean(rewards[-100:])\n",
    "    max_reward = max(rewards)\n",
    "    print(f\"{name:20s} - Avg (last 100): {final_avg:6.2f} | Max: {max_reward:6.1f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'rewards' is the list returned by your train_dqn function\n",
    "# If you used the variable name 'rewards_ddqn_per', replace 'rewards' below with that.\n",
    "\n",
    "# 1. Highest Single Episode Score (The \"High Score\")\n",
    "max_score = max(rewards)\n",
    "max_episode = np.argmax(rewards)\n",
    "print(f\"üöÄ Highest Single Episode Score: {max_score} (Achieved at Episode {max_episode})\")\n",
    "\n",
    "# 2. Best Average Score (The most stable policy)\n",
    "# Calculate moving average over 100 episodes\n",
    "window = 100\n",
    "if len(rewards) >= window:\n",
    "    moving_avgs = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    best_avg_score = np.max(moving_avgs)\n",
    "    best_avg_episode = np.argmax(moving_avgs) + window # Adjustment for window offset\n",
    "    print(f\"üèÜ Best 100-Episode Average: {best_avg_score:.2f} (Achieved around Episode {best_avg_episode})\")\n",
    "else:\n",
    "    print(\"Not enough episodes for 100-episode average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rewards, alpha=0.4, color='gray', label='Episode Reward') # Faint raw scores\n",
    "if len(rewards) >= 100:\n",
    "    plt.plot(range(99, len(rewards)), moving_avgs, color='blue', linewidth=2, label='100-Ep Moving Avg')\n",
    "    # Mark the best point\n",
    "    plt.scatter(best_avg_episode-100, best_avg_score, color='red', s=100, zorder=5, label='Best Average')\n",
    "\n",
    "plt.axhline(y=500, color='green', linestyle='--', label='Target: 500')\n",
    "plt.title(\"Training Progress: Double DQN + PER\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
