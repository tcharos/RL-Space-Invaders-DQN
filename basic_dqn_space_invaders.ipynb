{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DQN for Space Invaders\n",
    "\n",
    "Minimal implementation of Deep Q-Network for ALE/SpaceInvaders-v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari,accept-rom-license]\n",
    "!pip install ale-py\n",
    "!pip install torch scipy numpy\n",
    "\n",
    "# Import and verify ALE is available\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "\n",
    "# Register ALE environments\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DQN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert frame to grayscale, resize to 84x84, and normalize\"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = np.dot(frame[..., :3], [0.299, 0.587, 0.114])\n",
    "    # Resize to 84x84\n",
    "    resized = zoom(gray, (84/210, 84/160), order=1)\n",
    "    # Normalize\n",
    "    normalized = resized / 255.0\n",
    "    return normalized.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Dictionary\n",
    "CONFIG = {\n",
    "    # Environment\n",
    "    'ENV_ID': 'ALE/SpaceInvaders-v5',\n",
    "    'SEED': 7,\n",
    "    \n",
    "    # Network\n",
    "    'N_FRAMES': 4,\n",
    "    'N_ACTIONS': 6,\n",
    "    \n",
    "    # Training\n",
    "    'N_EPISODES': 1000,\n",
    "    'LEARNING_RATE': 0.00025,\n",
    "    'GAMMA': 0.99,\n",
    "    'BATCH_SIZE': 32,\n",
    "    \n",
    "    # Exploration\n",
    "    'EPSILON_START': 1.0,\n",
    "    'EPSILON_END': 0.1,\n",
    "    'EPSILON_DECAY': 10000,\n",
    "    \n",
    "    # Memory\n",
    "    'BUFFER_SIZE': 10000,\n",
    "    'TARGET_UPDATE': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment and Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net = DQN((CONFIG['N_FRAMES'], 84, 84), CONFIG['N_ACTIONS']).to(device)\n",
    "target_net = DQN((CONFIG['N_FRAMES'], 84, 84), CONFIG['N_ACTIONS']).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "replay_buffer = ReplayBuffer(CONFIG['BUFFER_SIZE'])\n",
    "\n",
    "print(f\"Environment: {CONFIG['ENV_ID']}\")\n",
    "print(f\"Action space: {CONFIG['N_ACTIONS']}\")\n",
    "print(f\"Seed: {CONFIG['SEED']}\")\n",
    "print(f\"Policy Network initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, policy_net, n_actions, device):\n",
    "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "def optimize_model(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device):\n",
    "    \"\"\"Perform one step of optimization\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Next Q values from target network\n",
    "    next_q = target_net(next_states).max(1)[0].detach()\n",
    "    target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Loss\n",
    "    loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(config, policy_net, target_net, optimizer, replay_buffer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generic DQN training function that works with different DQN variants.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary with all hyperparameters\n",
    "        policy_net: Policy network (DQN, Double DQN, Dueling DQN, etc.)\n",
    "        target_net: Target network\n",
    "        optimizer: Optimizer for policy network\n",
    "        replay_buffer: Experience replay buffer\n",
    "        device: Device to run on ('cpu' or 'cuda')\n",
    "    \n",
    "    Returns:\n",
    "        episode_rewards: List of rewards per episode\n",
    "    \"\"\"\n",
    "    import psutil\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(config['ENV_ID'])\n",
    "    if config.get('SEED') is not None:\n",
    "        env.reset(seed=config['SEED'])\n",
    "    \n",
    "    n_actions = config['N_ACTIONS']\n",
    "    episode_rewards = []\n",
    "    steps = 0\n",
    "    \n",
    "    for episode in range(config['N_EPISODES']):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_frame(state)\n",
    "        state_stack = deque([state] * config['N_FRAMES'], maxlen=config['N_FRAMES'])\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon decay\n",
    "            epsilon = config['EPSILON_END'] + (config['EPSILON_START'] - config['EPSILON_END']) * \\\n",
    "                      np.exp(-1. * steps / config['EPSILON_DECAY'])\n",
    "            \n",
    "            # Select action\n",
    "            state_array = np.array(state_stack)\n",
    "            action = select_action(state_array, epsilon, policy_net, n_actions, device)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_state = preprocess_frame(next_state)\n",
    "            next_state_stack = state_stack.copy()\n",
    "            next_state_stack.append(next_state)\n",
    "            \n",
    "            # Store transition\n",
    "            replay_buffer.push(\n",
    "                np.array(state_stack),\n",
    "                action,\n",
    "                reward,\n",
    "                np.array(next_state_stack),\n",
    "                float(done)\n",
    "            )\n",
    "            \n",
    "            state_stack = next_state_stack\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Optimize\n",
    "            optimize_model(policy_net, target_net, optimizer, replay_buffer, \n",
    "                          config['BATCH_SIZE'], config['GAMMA'], device)\n",
    "            \n",
    "            # Update target network\n",
    "            if steps % config['TARGET_UPDATE'] == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            \n",
    "            # Get memory info\n",
    "            mem = psutil.virtual_memory()\n",
    "            gpu_mem = 0.0\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
    "            \n",
    "            print(f'Episode {episode}\\tScore: {episode_reward:.1f}\\tAvg: {avg_score:.2f}\\tEps: {epsilon:.3f}\\tSteps: {steps}')\n",
    "            print(f'RAM: {mem.percent:.1f}% | GPU: {gpu_mem:.2f}GB | Buffer: {len(replay_buffer)}/{config[\"BUFFER_SIZE\"]}')\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\nTraining completed!\")\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DQN\n",
    "episode_rewards = train_dqn(\n",
    "    config=CONFIG,\n",
    "    policy_net=policy_net,\n",
    "    target_net=target_net,\n",
    "    optimizer=optimizer,\n",
    "    replay_buffer=replay_buffer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidated Results Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consolidated_results(results_dict, window=100, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Plot consolidated training progress for multiple DQN runs.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary mapping run names to episode rewards lists\n",
    "                     e.g., {'DQN_basic': [rewards], 'DoubleDQN': [rewards]}\n",
    "        window: Window size for moving average\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    for idx, (name, rewards) in enumerate(results_dict.items()):\n",
    "        # Calculate moving average\n",
    "        if len(rewards) >= window:\n",
    "            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            episodes = range(window-1, len(rewards))\n",
    "            final_avg = np.mean(rewards[-100:])\n",
    "            \n",
    "            # Plot moving average\n",
    "            plt.plot(episodes, moving_avg, \n",
    "                    label=f'{name} (Avg={final_avg:.2f})',\n",
    "                    color=colors[idx % len(colors)],\n",
    "                    linewidth=2)\n",
    "    \n",
    "    # Add goal lines\n",
    "    plt.axhline(y=500, color='green', linestyle='--', linewidth=2, label='Goal: 500', alpha=0.7)\n",
    "    plt.axhline(y=400, color='red', linestyle='--', linewidth=2, label='Goal: 400', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Episode #', fontsize=12)\n",
    "    plt.ylabel(f'Average Score ({window}-Game Window)', fontsize=12)\n",
    "    plt.title(f'Consolidated DQN Training Progress ({window}-Episode Moving Average)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Storage for multiple runs\n",
    "all_results = {}\n",
    "\n",
    "# Store current run\n",
    "run_name = 'DQN_basic'  # Change this for each variant\n",
    "all_results[run_name] = episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Current Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode rewards for current run\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "\n",
    "# Calculate moving average\n",
    "window = 100\n",
    "moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, len(episode_rewards)), moving_avg, label=f'Moving Average ({window})', linewidth=2)\n",
    "\n",
    "plt.axhline(y=500, color='r', linestyle='--', label='Target (500)')\n",
    "plt.axhline(y=400, color='orange', linestyle='--', label='Minimum (400)')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title(f'{run_name} - Training Progress on Space Invaders')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "final_avg = np.mean(episode_rewards[-100:])\n",
    "print(f\"\\nFinal average reward (last 100 episodes): {final_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Consolidated Results (All Runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot consolidated results comparing all runs\n",
    "plot_consolidated_results(all_results, window=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with more DQN variants\n",
    "\n",
    "# Run 1: Basic DQN\n",
    "run_name = 'DQN_basic'\n",
    "episode_rewards_1 = train_dqn(CONFIG, policy_net, target_net, optimizer, replay_buffer, device)\n",
    "all_results[run_name] = episode_rewards_1\n",
    "\n",
    "# Run 2: Lower Learning Rate\n",
    "CONFIG_v2 = CONFIG.copy()\n",
    "CONFIG_v2['LEARNING_RATE'] = 0.0001\n",
    "CONFIG_v2['SEED'] = 42\n",
    "\n",
    "policy_net_2 = DQN((CONFIG['N_FRAMES'], 84, 84), CONFIG['N_ACTIONS']).to(device)\n",
    "target_net_2 = DQN((CONFIG['N_FRAMES'], 84, 84), CONFIG['N_ACTIONS']).to(device)\n",
    "optimizer_2 = optim.Adam(policy_net_2.parameters(), lr=CONFIG_v2['LEARNING_RATE'])\n",
    "replay_buffer_2 = ReplayBuffer(CONFIG['BUFFER_SIZE'])\n",
    "\n",
    "run_name = 'DQN_lower_lr'\n",
    "episode_rewards_2 = train_dqn(CONFIG_v2, policy_net_2, target_net_2, optimizer_2, replay_buffer_2, device)\n",
    "all_results[run_name] = episode_rewards_2\n",
    "\n",
    "# Run 3: Larger Buffer\n",
    "CONFIG_v3 = CONFIG.copy()\n",
    "CONFIG_v3['BUFFER_SIZE'] = 50000\n",
    "CONFIG_v3['SEED'] = 123\n",
    "\n",
    "policy_net_3 = DQN((CONFIG['N_FRAMES'], 84, 84), CONFIG['N_ACTIONS']).to(device)\n",
    "target_net_3 = DQN((CONFIG['N_FRAMES'], 84, 84), CONFIG['N_ACTIONS']).to(device)\n",
    "optimizer_3 = optim.Adam(policy_net_3.parameters(), lr=CONFIG_v3['LEARNING_RATE'])\n",
    "replay_buffer_3 = ReplayBuffer(CONFIG_v3['BUFFER_SIZE'])\n",
    "\n",
    "run_name = 'DQN_large_buffer'\n",
    "episode_rewards_3 = train_dqn(CONFIG_v3, policy_net_3, target_net_3, optimizer_3, replay_buffer_3, device)\n",
    "all_results[run_name] = episode_rewards_3\n",
    "\n",
    "# Plot all three together\n",
    "plot_consolidated_results(all_results, window=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and all results\n",
    "torch.save({\n",
    "    'config': CONFIG,\n",
    "    'policy_net_state_dict': policy_net.state_dict(),\n",
    "    'target_net_state_dict': target_net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'all_results': all_results,\n",
    "}, f'dqn_space_invaders_{run_name}.pth')\n",
    "\n",
    "print(f\"Model saved successfully as 'dqn_space_invaders_{run_name}.pth'!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
