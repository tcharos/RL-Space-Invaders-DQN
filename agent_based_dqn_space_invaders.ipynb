{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent-Based DQN for Space Invaders\n",
    "\n",
    "Implementation of DQN variants using Agent class with hard target updates:\n",
    "- Standard DQN\n",
    "- Double DQN\n",
    "- Duelling DQN\n",
    "- Prioritized Experience Replay (PER)\n",
    "- Learning Rate Scheduling\n",
    "- Graceful Interruption (Ctrl+C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari,accept-rom-license]\n",
    "!pip install ale-py\n",
    "!pip install torch scipy numpy psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and verify ALE is available\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (MPS for Mac, CUDA for GPU, CPU otherwise)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GDRIVE = False  # Set to True to enable Google Drive integration\n",
    "\n",
    "if USE_GDRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/DQN_SpaceInvaders_Checkpoints'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    BEST_MODELS = './best_models'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(BEST_MODELS, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved locally to: {CHECKPOINT_DIR}\")\n",
    "    print(f\"Best models will be saved locally to: {BEST_MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Standard DQN network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class DuellingDQN(nn.Module):\n",
    "    \"\"\"Duelling DQN network - separates value and advantage streams.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuellingDQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        \n",
    "        # Combine value and advantage: Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Standard uniform replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized Experience Replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size, alpha=0.6, beta_start=0.4, beta_frames=100000, epsilon=1e-6):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.priorities = deque(maxlen=buffer_size)\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.epsilon = epsilon\n",
    "        self.frame = 0\n",
    "        self.max_priority = 1.0\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        \"\"\"Linearly increase beta from beta_start to 1.0.\"\"\"\n",
    "        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(self.max_priority)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        self.frame += 1\n",
    "        beta = self.beta_schedule()\n",
    "        \n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=False)\n",
    "        \n",
    "        # Importance sampling weights\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "            indices,\n",
    "            weights.astype(np.float32)\n",
    "        )\n",
    "    \n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"Update priorities based on TD errors.\"\"\"\n",
    "        for idx, error in zip(indices, td_errors):\n",
    "            priority = (abs(error) + self.epsilon) ** self.alpha\n",
    "            self.priorities[idx] = priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocess Space Invaders frame: grayscale + resize to 84x84.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    normalized = resized / 255.0\n",
    "    return normalized.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class with Hard Target Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with hard target network updates.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize DQN Agent.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Configuration dictionary\n",
    "            device (str): Device to use ('cpu', 'cuda', 'mps')\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.n_actions = config['N_ACTIONS']\n",
    "        \n",
    "        # Create networks\n",
    "        input_shape = (config['N_FRAMES'], 84, 84)\n",
    "        if config['DQN_TYPE'] == 'DuellingDQN':\n",
    "            self.policy_net = DuellingDQN(input_shape, self.n_actions).to(device)\n",
    "            self.target_net = DuellingDQN(input_shape, self.n_actions).to(device)\n",
    "        else:\n",
    "            self.policy_net = DQN(input_shape, self.n_actions).to(device)\n",
    "            self.target_net = DQN(input_shape, self.n_actions).to(device)\n",
    "        \n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=config['LEARNING_RATE'])\n",
    "        \n",
    "        # LR Scheduler (optional)\n",
    "        self.scheduler = None\n",
    "        if config.get('USE_LR_SCHEDULER', False):\n",
    "            self.scheduler = ExponentialLR(self.optimizer, gamma=config.get('LR_GAMMA', 0.9995))\n",
    "        \n",
    "        # Replay buffer\n",
    "        if config.get('USE_PER', False):\n",
    "            self.memory = PrioritizedReplayBuffer(\n",
    "                config['BUFFER_SIZE'],\n",
    "                alpha=config.get('PER_ALPHA', 0.6),\n",
    "                beta_start=config.get('PER_BETA_START', 0.4),\n",
    "                beta_frames=config.get('PER_BETA_FRAMES', 100000),\n",
    "                epsilon=config.get('PER_EPSILON', 1e-6)\n",
    "            )\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(config['BUFFER_SIZE'])\n",
    "        \n",
    "        self.steps = 0\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience and learn.\"\"\"\n",
    "        # Store experience\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Learn from experience\n",
    "        if len(self.memory) >= self.config['BATCH_SIZE']:\n",
    "            self.learn()\n",
    "        \n",
    "        # Hard update target network\n",
    "        if self.steps % self.config['TARGET_UPDATE'] == 0:\n",
    "            self.update_target_network()\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Update policy network using batch of experiences.\"\"\"\n",
    "        # Sample from memory\n",
    "        use_per = self.config.get('USE_PER', False)\n",
    "        \n",
    "        if use_per:\n",
    "            states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(self.config['BATCH_SIZE'])\n",
    "        else:\n",
    "            states, actions, rewards, next_states, dones = self.memory.sample(self.config['BATCH_SIZE'])\n",
    "            weights = np.ones(self.config['BATCH_SIZE'])\n",
    "            indices = None\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # Get current Q values\n",
    "        current_q_values = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Compute target Q values\n",
    "        with torch.no_grad():\n",
    "            if self.config['DQN_TYPE'] == 'DoubleDQN':\n",
    "                # Double DQN: use policy net to select actions, target net to evaluate\n",
    "                next_actions = self.policy_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            else:\n",
    "                # Standard DQN (or Duelling DQN)\n",
    "                next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "            \n",
    "            target_q_values = rewards + (1 - dones) * self.config['GAMMA'] * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        td_errors = target_q_values - current_q_values\n",
    "        loss = (td_errors.pow(2) * weights).mean()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=10.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities if using PER\n",
    "        if use_per and indices is not None:\n",
    "            self.memory.update_priorities(indices, td_errors.detach().cpu().numpy())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Hard update: copy policy network weights to target network.\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def get_epsilon(self, episode_steps):\n",
    "        \"\"\"Calculate current epsilon based on decay schedule.\"\"\"\n",
    "        epsilon = self.config['EPSILON_END'] + (self.config['EPSILON_START'] - self.config['EPSILON_END']) * \\\n",
    "                  np.exp(-1. * episode_steps / self.config['EPSILON_DECAY'])\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(config, agent, device='cpu'):\n",
    "    \"\"\"Train DQN agent with graceful interruption support.\"\"\"\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(config['ENV_ID'])\n",
    "    if config.get('SEED') is not None:\n",
    "        env.reset(seed=config['SEED'])\n",
    "        random.seed(config['SEED'])\n",
    "        np.random.seed(config['SEED'])\n",
    "        torch.manual_seed(config['SEED'])\n",
    "    \n",
    "    episode_rewards = []\n",
    "    best_avg_score = -float('inf')\n",
    "    \n",
    "    # Print configuration\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  DQN TYPE: {config['DQN_TYPE']}\")\n",
    "    if config.get('USE_PER', False):\n",
    "        print(\"  Using Prioritized Experience Replay (PER)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"-\"*70)\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if agent.scheduler:\n",
    "        print(f\"\\nâœ… LR Scheduler enabled: ExponentialLR (gamma={config.get('LR_GAMMA', 0.9995)})\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Press Ctrl+C anytime to stop training and save progress\\n\")\n",
    "    print(f\"--- TRAINING STARTED: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\\n\")\n",
    "    \n",
    "    try:\n",
    "        for episode in range(config['N_EPISODES']):\n",
    "            state, _ = env.reset()\n",
    "            state = preprocess_frame(state)\n",
    "            state_stack = deque([state] * config['N_FRAMES'], maxlen=config['N_FRAMES'])\n",
    "            \n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # Get epsilon and select action\n",
    "                epsilon = agent.get_epsilon(agent.steps)\n",
    "                state_array = np.array(state_stack)\n",
    "                action = agent.act(state_array, epsilon)\n",
    "                \n",
    "                # Take step\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                next_state = preprocess_frame(next_state)\n",
    "                next_state_stack = state_stack.copy()\n",
    "                next_state_stack.append(next_state)\n",
    "                \n",
    "                # Agent step (store and learn)\n",
    "                agent.step(\n",
    "                    np.array(state_stack),\n",
    "                    action,\n",
    "                    reward,\n",
    "                    np.array(next_state_stack),\n",
    "                    float(done)\n",
    "                )\n",
    "                \n",
    "                state_stack = next_state_stack\n",
    "                episode_reward += reward\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            # Step LR scheduler\n",
    "            if agent.scheduler:\n",
    "                agent.scheduler.step()\n",
    "            \n",
    "            # Print progress\n",
    "            if episode % 10 == 0:\n",
    "                avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "                current_lr = agent.optimizer.param_groups[0]['lr']\n",
    "                epsilon = agent.get_epsilon(agent.steps)\n",
    "                \n",
    "                print(f'Episode {episode} | Score: {episode_reward:.1f} | Avg: {avg_score:.2f} | '\n",
    "                      f'Eps: {epsilon:.3f} | LR: {current_lr:.6f} | Steps: {agent.steps}')\n",
    "                \n",
    "                # Save best model\n",
    "                if avg_score > best_avg_score:\n",
    "                    best_avg_score = avg_score\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    filename = f'{config[\"DQN_TYPE\"]}_{timestamp}_best.pth'\n",
    "                    torch.save({\n",
    "                        'episode': episode,\n",
    "                        'config': config,\n",
    "                        'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "                        'target_net_state_dict': agent.target_net.state_dict(),\n",
    "                        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': agent.scheduler.state_dict() if agent.scheduler else None,\n",
    "                        'episode_rewards': episode_rewards,\n",
    "                        'avg_score': avg_score,\n",
    "                    }, filename)\n",
    "                    print(f\"\\t\\tNew best avg: {avg_score:.2f} - saved to {filename}\")\n",
    "            \n",
    "            # System stats\n",
    "            if episode % 40 == 0:\n",
    "                mem = psutil.virtual_memory()\n",
    "                gpu_str = \"N/A\"\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_str = f\"{torch.cuda.memory_allocated() / 1024**3:.2f}GB\"\n",
    "                elif torch.backends.mps.is_available() and device.type == 'mps':\n",
    "                    gpu_str = \"Active\"\n",
    "                print(f\"\\t\\tRAM: {mem.percent:.1f}% | GPU: {gpu_str} | Buffer: {len(agent.memory)}/{config['BUFFER_SIZE']}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"âš ï¸  TRAINING INTERRUPTED BY USER\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if len(episode_rewards) > 0:\n",
    "            current_episode = len(episode_rewards) - 1\n",
    "            avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Training Statistics at Interruption:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"  Episodes completed: {current_episode + 1} / {config['N_EPISODES']}\")\n",
    "            print(f\"  Total steps: {agent.steps:,}\")\n",
    "            print(f\"  Last episode score: {episode_rewards[-1]:.1f}\")\n",
    "            print(f\"  Average score (last {min(100, len(episode_rewards))} episodes): {avg_score:.2f}\")\n",
    "            print(f\"  Best average score: {best_avg_score:.2f}\")\n",
    "            print(f\"  Max episode score: {max(episode_rewards):.1f}\")\n",
    "            print(f\"  Min episode score: {min(episode_rewards):.1f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f'interrupted_{config[\"DQN_TYPE\"]}_ep{current_episode}_{timestamp}.pth'\n",
    "            torch.save({\n",
    "                'episode': current_episode,\n",
    "                'config': config,\n",
    "                'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "                'target_net_state_dict': agent.target_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': agent.scheduler.state_dict() if agent.scheduler else None,\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'avg_score': avg_score,\n",
    "            }, filename)\n",
    "            print(f\"\\nâœ… Progress saved: {filename}\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "    \n",
    "    if len(episode_rewards) == config['N_EPISODES']:\n",
    "        print(\"\\nâœ… Training completed successfully!\")\n",
    "        print(f\"Best average score: {best_avg_score:.2f}\\n\")\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Configuration\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    'ENV_ID': 'ALE/SpaceInvaders-v5',\n",
    "    'SEED': 18,\n",
    "    'N_FRAMES': 4,\n",
    "    'N_ACTIONS': 6,\n",
    "    'N_EPISODES': 4000,\n",
    "    'LEARNING_RATE': 0.00001,\n",
    "    'GAMMA': 0.99,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPSILON_START': 1.0,\n",
    "    'EPSILON_END': 0.1,\n",
    "    'EPSILON_DECAY': 100000,\n",
    "    'BUFFER_SIZE': 100000,\n",
    "    'TARGET_UPDATE': 5000,\n",
    "    'CHECKPOINT_EVERY': 400,\n",
    "    'USE_GDRIVE': USE_GDRIVE,\n",
    "    'CHECKPOINT_DIR': CHECKPOINT_DIR,\n",
    "    'BEST_MODELS_DIR': BEST_MODELS,\n",
    "    'DQN_TYPE': 'DQN',  # Options: 'DQN', 'DoubleDQN', 'DuellingDQN'\n",
    "    'USE_PER': False, \n",
    "    'LR_SCHEDULER': True,\n",
    "    'LR_GAMMA': 0.999, \n",
    "    \n",
    "    # PER hyperparameters (if USE_PER=True)\n",
    "    'PER_ALPHA': 0.6,\n",
    "    'PER_BETA_START': 0.4,\n",
    "    'PER_BETA_FRAMES': 100000,\n",
    "    'PER_EPSILON': 1e-6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple DQN agent\n",
    "\n",
    "CONFIG_DQN = BASE_CONFIG.copy()\n",
    "CONFIG_DQN['DQN_TYPE'] = 'DQN'\n",
    "CONFIG_DQN['USE_PER'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = DQNAgent(CONFIG_DQN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = train_agent(CONFIG_DQN, agent, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Double DQN agent\n",
    "\n",
    "CONFIG_DDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DDQN['DQN_TYPE'] = 'DoubleDQN'\n",
    "CONFIG_DDQN['USE_PER'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_ddqn = DQNAgent(CONFIG_DDQN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_ddqn = train_agent(CONFIG_DDQN, agent_ddqn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Duelling DQN agent\n",
    "\n",
    "CONFIG_DuelDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DuelDQN['DQN_TYPE'] = 'DuellingDQN'\n",
    "CONFIG_DuelDQN['USE_PER'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dueldqn = DQNAgent(CONFIG_DuelDQN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_dueldqn = train_agent(CONFIG_DuelDQN, agent_dueldqn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, window=100):\n",
    "    \"\"\"Plot training rewards with moving average.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    \n",
    "    if len(rewards) >= window:\n",
    "        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(range(window-1, len(rewards)), moving_avg, \n",
    "                label=f'Moving Avg ({window})', linewidth=2, color='red')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 'rewards' is the list returned by your train_dqn function\n",
    "# If you used the variable name 'rewards_ddqn_per', replace 'rewards' below with that.\n",
    "\n",
    "# 1. Highest Single Episode Score (The \"High Score\")\n",
    "max_score = max(rewards)\n",
    "max_episode = np.argmax(rewards)\n",
    "print(f\"ðŸš€ Highest Single Episode Score: {max_score} (Achieved at Episode {max_episode})\")\n",
    "\n",
    "# 2. Best Average Score (The most stable policy)\n",
    "# Calculate moving average over 100 episodes\n",
    "window = 100\n",
    "if len(rewards) >= window:\n",
    "    moving_avgs = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    best_avg_score = np.max(moving_avgs)\n",
    "    best_avg_episode = np.argmax(moving_avgs) + window # Adjustment for window offset\n",
    "    print(f\"ðŸ† Best 100-Episode Average: {best_avg_score:.2f} (Achieved around Episode {best_avg_episode})\")\n",
    "else:\n",
    "    print(\"Not enough episodes for 100-episode average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rewards, alpha=0.4, color='gray', label='Episode Reward') # Faint raw scores\n",
    "if len(rewards) >= 100:\n",
    "    plt.plot(range(99, len(rewards)), moving_avgs, color='blue', linewidth=2, label='100-Ep Moving Avg')\n",
    "    # Mark the best point\n",
    "    plt.scatter(best_avg_episode-100, best_avg_score, color='red', s=100, zorder=5, label='Best Average')\n",
    "\n",
    "plt.axhline(y=500, color='green', linestyle='--', label='Target: 500')\n",
    "plt.title(\"Training Progress: Double DQN + PER\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used code from https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google\n",
    "# for the video saving and display\n",
    "\n",
    "before_training = \"trained.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "state = env.reset()\n",
    "score = 0\n",
    "for i in range(1000):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  action = agent.act(state)\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  score += reward\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "video.close()\n",
    "env.close()\n",
    "\n",
    "print(\"Total score was\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "def render_mp4(videopath: str) -> str:\n",
    "  \"\"\"\n",
    "  Gets a string containing a b4-encoded version of the MP4 video\n",
    "  at the specified path.\n",
    "  \"\"\"\n",
    "  mp4 = open(videopath, 'rb').read()\n",
    "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "html = render_mp4(before_training)\n",
    "HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
