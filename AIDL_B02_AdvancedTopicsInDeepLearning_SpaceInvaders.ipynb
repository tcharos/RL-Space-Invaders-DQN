{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/AIDL_B02_AdvancedTopicsInDeepLearning_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkZJMnJp0uUq"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/AIDL_B02_AdvancedTopicsInDeepLearning_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# üöÄ Project Base: DQN Variants for ALE/SpaceInvaders-v5\n",
        "\n",
        "This notebook strictly implements the project's requirements for the **`ALE/SpaceInvaders-v5`** environment with 4-frame stacking and CNN architecture.\n",
        "\n",
        "**Key Requirements Met:**\n",
        "* **Environment:** `ALE/SpaceInvaders-v5` [cite: 11]\n",
        "* **Action Space:** 6 actions [cite: 13, 21]\n",
        "* **State:** 4 stacked input frames [cite: 19]\n",
        "\n",
        "**To run an implementation:**\n",
        "1.  Change the `CONFIG['MODE']` variable below to one of: **`SimpleDQN`**, **`DoubleDQN`**, or **`DuelingDQN`**.\n",
        "2.  Adjust hyperparameters (`LR`, `EPS_DECAY`, etc.) in the `CONFIG` dictionary if needed.\n",
        "3.  Run all cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03689557-f6a5-4dce-e9e9-188530309fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari,other] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (0.0.4)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (1.0.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (4.12.0.88)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (2.9.0.post0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (0.6.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn>=0.13->gymnasium[accept-rom-license,atari,other]) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[accept-rom-license,atari,other]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[accept-rom-license,atari,other]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (2025.11.12)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.12/dist-packages (3.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+5build2).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: shimmy in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from shimmy) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.12/dist-packages (from shimmy) (1.2.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gymnasium[atari,accept-rom-license,other]\" ale-py\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install shimmy imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "from gymnasium.wrappers import AtariPreprocessing\n",
        "from gymnasium.wrappers import FrameStackObservation\n",
        "import ale_py\n",
        "\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Tools for video display\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- GLOBAL CONFIGURATION -----------------\n",
        "CONFIG = {\n",
        "    \"ENV_ID\": 'ALE/SpaceInvaders-v5',\n",
        "    \"SEED\": 7,\n",
        "    \"MODE\": \"SimpleDQN\", # Choice --> 'SimpleDQN', 'DoubleDQN', 'DuelingDQN'\n",
        "    \"INPUT_SHAPE\": (4, 84, 84), # 4 stacked frames, resized to 84x84\n",
        "    \"BUFFER_SIZE\": int(5e3), # from 1e5\n",
        "    \"BATCH_SIZE\": 32, # Reduced batch size (common practice for Atari, hinted in PDF [cite: 37])\n",
        "    \"GAMMA\": 0.99, # Prioritizing long-term cumulative reward\n",
        "    \"TAU\": 1e-3, # Soft Update Rate\n",
        "    \"LR\": 1e-4, # Lower learning rate --> stable convergence\n",
        "    \"UPDATE_EVERY\": 4, # Learn frequency (standard for Atari DQN)\n",
        "    \"TARGET_UPDATE_FREQ\": 1000,\n",
        "    \"N_EPISODES\": 4000,\n",
        "    \"EPS_START\": 1.0, # Initial probability of choosing a random action (exploration) --> fully exploring the environment to gather initial experiences\n",
        "    \"EPS_END\": 0.01, # Minimum probability of choosing a random action.\n",
        "    \"EPS_DECAY\": 0.999, # Exploration rate decays very slowly, allowing the agent to explore over a large number of episodes\n",
        "    \"USE_GOOGLE_DRIVE\": True,\n",
        "    \"CHECKPOINT_FREQ\": 400,\n",
        "    \"GOAL_SCORE\": 400.0\n",
        "}\n",
        "# --------------------------------------------------------\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random.seed(CONFIG['SEED'])\n",
        "np.random.seed(CONFIG['SEED'])\n",
        "torch.manual_seed(CONFIG['SEED'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(CONFIG['SEED'])\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Current DQN Mode: {CONFIG['MODE']}\")"
      ],
      "metadata": {
        "id": "rSTjoTix3Zdq",
        "outputId": "7f890202-07d8-4cda-b990-441f1819e600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Current DQN Mode: SimpleDQN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG['USE_GOOGLE_DRIVE']:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    checkpoint_dir = '/content/drive/MyDrive/DQN_Checkpoints'\n",
        "    print(\"‚úì Google Drive mounted - checkpoints will be saved to Drive\")\n",
        "else:\n",
        "    checkpoint_dir = './checkpoints'  # Local directory\n",
        "    print(\"‚úì Using local storage - checkpoints will be saved locally\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "7_nOa0elCNZL",
        "outputId": "6927446f-9a58-4dc4-cc0b-bbc7e436ca94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úì Google Drive mounted - checkpoints will be saved to Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_init"
      },
      "source": [
        "## 2. Environment Initialization\n",
        "We use **`AtariPreprocessing`** to handle resizing/cropping to 84x84 and grayscale conversion. **`FrameStack`** then stacks 4 consecutive frames, fulfilling the requirements for the state space[cite: 19, 20]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "env_run",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff461d9-aabc-4213-9098-7cba66e33ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final State shape (Stacked Frames): (4, 84, 84)\n",
            "Number of available actions (SpaceInvaders): 6\n"
          ]
        }
      ],
      "source": [
        "def make_atari_env(env_id, seed):\n",
        "    \"\"\"Creates and wraps the Atari environment with standard preprocessing and 4-frame stacking.\"\"\"\n",
        "    # 1. Base Environment (Using the required ID [cite: 11])\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    # 2. Atari Preprocessing: Resizes to 84x84, grayscale, handles max-pooling/skip.\n",
        "    # Frame skip is set to 1 here because the ALE/SpaceInvaders-v5 environment generally handles skips\n",
        "    # implicitly, or we rely on the standard wrappers' internal logic for compatibility.\n",
        "    env = AtariPreprocessing(env, grayscale_obs=True, terminal_on_life_loss=True, frame_skip=1, screen_size=84)\n",
        "\n",
        "    # 3. Frame Stacking (Creates the (4, 84, 84) state [cite: 19])\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "    # Set seed on the final environment\n",
        "    if seed is not None:\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "\n",
        "    return env\n",
        "\n",
        "env = make_atari_env(CONFIG['ENV_ID'], CONFIG['SEED'])\n",
        "action_size = env.action_space.n\n",
        "state_shape = env.observation_space.shape\n",
        "\n",
        "print(f'Final State shape (Stacked Frames): {state_shape}')\n",
        "print(f'Number of available actions (SpaceInvaders): {action_size}') # Confirms 6 actions [cite: 13, 21]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "network_arch"
      },
      "source": [
        "## 3. Q-Network Architecture\n",
        "The network uses a CNN architecture  to process the high-dimensional image input, supporting Dueling components via a flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QNetwork_code"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"CNN-based Q-Network Model supporting Standard and Dueling structures.\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_size, seed, dueling=False):\n",
        "        \"\"\"Initializes the shared CNN layers and splits into Value/Advantage streams if Dueling is enabled.\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.dueling = dueling\n",
        "        in_channels = state_shape[0] # 4 stacked frames\n",
        "\n",
        "        # --- Shared CNN Layers (Original DQN architecture) ---\n",
        "        # Layers extract features from the 4 stacked 84x84 input images.\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        # --- Dynamic Calculation of fc_input_size ---\n",
        "        # This prevents the network from breaking if the input image size changes.\n",
        "        # 1. Create a dummy input tensor based on state_shape (e.g., (1, 4, 84, 84))\n",
        "        dummy_input = torch.zeros(1, *state_shape)\n",
        "\n",
        "        # 2. Pass the dummy input through the convolutional layers\n",
        "        x = self._forward_conv(dummy_input)\n",
        "\n",
        "        # 3. Calculate the flattened feature size (e.g., 7*7*64 = 3136)\n",
        "        self.fc_input_size = x.view(1, -1).size(1)\n",
        "\n",
        "        # --- Fully Connected Layers ---\n",
        "        if self.dueling:\n",
        "            # Dueling Architecture: Split into Value (V) and Advantage (A) streams\n",
        "            self.fc_v1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc_a1 = nn.Linear(self.fc_input_size, 512)\n",
        "\n",
        "            self.fc_v2 = nn.Linear(512, 1) # Output V(s)\n",
        "            self.fc_a2 = nn.Linear(512, action_size) # Output A(s, a)\n",
        "        else:\n",
        "            # Standard DQN Architecture (Single Q-stream)\n",
        "            self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc2 = nn.Linear(512, action_size)\n",
        "\n",
        "    def _forward_conv(self, x):\n",
        "        \"\"\"Forward pass through convolutional layers only.\"\"\"\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        return x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Maps state (4, 84, 84) to action values (6).\"\"\"\n",
        "        # CNN forward pass\n",
        "        x = F.relu(self.conv1(state))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "\n",
        "        if self.dueling:\n",
        "        # Dueling Combination: Q(s,a) = V(s) + [A(s,a) - mean(A(s,a))]\n",
        "            v = F.relu(self.fc_v1(x))\n",
        "            a = F.relu(self.fc_a1(x))\n",
        "            v = self.fc_v2(v)\n",
        "            a = self.fc_a2(a)\n",
        "            return v + a - a.mean(1).unsqueeze(1)\n",
        "        else:\n",
        "            # Standard Q-stream\n",
        "            x = F.relu(self.fc1(x))\n",
        "            return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buffer_agents"
      },
      "source": [
        "## 4. Replay Buffer and Agent Implementations\n",
        "The **Replay Buffer** (PER is an optional extension [cite: 27]) is crucial for breaking correlation in experience samples. The **AgentBase** handles common functions; specialized classes implement the specific Q-learning update rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ReplayBuffer_code"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Memory-efficient replay buffer using uint8 for frames.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store frames as uint8 to save memory (4x compression).\"\"\"\n",
        "        # Convert float32 [0,1] to uint8 [0,255]\n",
        "        if state.dtype == np.float32 or state.dtype == np.float64:\n",
        "            state = (state * 255).astype(np.uint8)\n",
        "        if next_state.dtype == np.float32 or next_state.dtype == np.float64:\n",
        "            next_state = (next_state * 255).astype(np.uint8)\n",
        "\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Sample and convert back to float32 [0,1].\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.stack([e.state for e in experiences])).float().to(device) / 255.0\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences])).float().to(device) / 255.0\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Agent_init",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e972b2c0-b5a1-4195-c908-d39d04ff5fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized agent: SimpleDQNAgent with learning mode: SimpleDQN\n"
          ]
        }
      ],
      "source": [
        "class AgentBase:\n",
        "    \"\"\"Base class for all DQN agents, handling shared components and target network logic.\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_size, seed, mode, dueling):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.mode = mode\n",
        "\n",
        "        # Initialize Q-Networks\n",
        "        self.qnetwork_local = QNetwork(state_shape, action_size, seed, dueling=dueling).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_shape, action_size, seed, dueling=dueling).to(device)\n",
        "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=CONFIG['LR'])\n",
        "        self.memory = ReplayBuffer(action_size, CONFIG['BUFFER_SIZE'], CONFIG['BATCH_SIZE'], seed)\n",
        "\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "      # Convert LazyFrames to numpy if needed\n",
        "      if hasattr(state, '__array__'):\n",
        "          state = np.array(state)\n",
        "      if hasattr(next_state, '__array__'):\n",
        "          next_state = np.array(next_state)\n",
        "\n",
        "      # Save experience\n",
        "      self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "      # Learn every UPDATE_EVERY steps\n",
        "      self.t_step = (self.t_step + 1) % CONFIG['UPDATE_EVERY']\n",
        "      if self.t_step == 0:\n",
        "          if len(self.memory) > CONFIG['BATCH_SIZE']:\n",
        "              experiences = self.memory.sample()\n",
        "              self.learn(experiences, CONFIG['GAMMA'])\n",
        "              # CRITICAL: Delete experiences tuple to free memory\n",
        "              del experiences\n",
        "\n",
        "      # Hard update the target network periodically\n",
        "      # FIX: This condition was wrong - it would almost never trigger\n",
        "      if (self.t_step + 1) % CONFIG['TARGET_UPDATE_FREQ'] == 0:\n",
        "          self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "      \"\"\"Returns action based on epsilon-greedy policy.\"\"\"\n",
        "      # Convert LazyFrames to numpy if needed\n",
        "      if hasattr(state, '__array__'):\n",
        "          state = np.array(state)\n",
        "\n",
        "      state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "      self.qnetwork_local.eval()\n",
        "      with torch.no_grad():\n",
        "          action_values = self.qnetwork_local(state)\n",
        "      self.qnetwork_local.train()\n",
        "\n",
        "      if random.random() > eps:\n",
        "          return np.argmax(action_values.cpu().data.numpy())\n",
        "      else:\n",
        "          return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        # Placeholder, implemented by child classes\n",
        "        pass\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- üí• DQN Variant 1: Simple DQN (Original Target Calculation) ---\n",
        "# ----------------------------------------------------------------\n",
        "class SimpleDQNAgent(AgentBase):\n",
        "    \"\"\"Implements the original DQN learning step: Target Q = R + gamma * max_a Q_target(s', a).\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Standard QNetwork (dueling=False)\n",
        "        super().__init__(state_shape, action_size, seed, mode='SimpleDQN', dueling=False)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Ensure states have the right shape: (batch, 4, 84, 84)\n",
        "        if states.dim() == 5:  # If shape is (batch, 4, 1, 84, 84)\n",
        "            states = states.squeeze(2)\n",
        "        if next_states.dim() == 5:\n",
        "            next_states = next_states.squeeze(2)\n",
        "\n",
        "        # Target Q calculation uses the max Q-value from the target network directly.\n",
        "        with torch.no_grad():  # ‚Üê CRITICAL: Prevent gradient tracking for target\n",
        "            Q_targets_next = self.qnetwork_target(next_states).max(1)[0].unsqueeze(1)\n",
        "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # CRITICAL: Clean up to prevent memory leak\n",
        "        del states, actions, rewards, next_states, dones, Q_targets, Q_expected, loss\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- üí• DQN Variant 2: Double DQN (Decoupled Target Calculation) ---\n",
        "# ----------------------------------------------------------------\n",
        "class DoubleDQNAgent(AgentBase):\n",
        "    \"\"\"Implements the Double DQN learning step.\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        super().__init__(state_shape, action_size, seed, mode='DoubleDQN', dueling=False)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Ensure states have the right shape\n",
        "        if states.dim() == 5:\n",
        "            states = states.squeeze(2)\n",
        "        if next_states.dim() == 5:\n",
        "            next_states = next_states.squeeze(2)\n",
        "\n",
        "        # Double DQN: Select actions with local, evaluate with target\n",
        "        with torch.no_grad():  # ‚Üê CRITICAL: No gradient tracking\n",
        "            # 1. Action selection from LOCAL network\n",
        "            Q_local_next = self.qnetwork_local(next_states)\n",
        "            best_actions = Q_local_next.max(1)[1].unsqueeze(1)\n",
        "\n",
        "            # 2. Value estimation from TARGET network\n",
        "            Q_targets_next = self.qnetwork_target(next_states).gather(1, best_actions)\n",
        "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Clean up\n",
        "        del states, actions, rewards, next_states, dones, Q_targets, Q_expected, loss\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- üí• DQN Variant 3: Dueling DQN (Dueling Architecture + Double Learning Rule) ---\n",
        "# ----------------------------------------------------------------\n",
        "class DuelingDQNAgent(DoubleDQNAgent):\n",
        "    \"\"\"Dueling DQN uses the Dueling architecture and the Double DQN learning rule for stability.\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Dueling QNetwork (dueling=True)\n",
        "        AgentBase.__init__(self, state_shape, action_size, seed, mode='DuelingDQN', dueling=True)\n",
        "\n",
        "    # Inherits the Double DQN learn() method for stability\n",
        "\n",
        "\n",
        "# --- Agent Initialization based on global CONFIG['MODE'] ---\n",
        "if CONFIG['MODE'] == \"SimpleDQN\":\n",
        "    agent = SimpleDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "elif CONFIG['MODE'] == \"DoubleDQN\":\n",
        "    agent = DoubleDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "elif CONFIG['MODE'] == \"DuelingDQN\":\n",
        "    agent = DuelingDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "else:\n",
        "    raise ValueError(\"Invalid MODE specified in CONFIG.\")\n",
        "\n",
        "print(f\"Initialized agent: {type(agent).__name__} with learning mode: {agent.mode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_loop"
      },
      "source": [
        "## 5. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "training_code"
      },
      "outputs": [],
      "source": [
        "def dqn_train(n_episodes=CONFIG['N_EPISODES'], max_t=10000, eps_start=CONFIG['EPS_START'], eps_end=CONFIG['EPS_END'], eps_decay=CONFIG['EPS_DECAY'], checkpoint_dir='/content/drive/MyDrive/DQN_Checkpoints'):\n",
        "    \"\"\"Deep Q-Learning with crash debugging.\"\"\"\n",
        "\n",
        "    import traceback\n",
        "\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "\n",
        "    global ALL_SCORES\n",
        "\n",
        "    print(f\"\\nStarting training for {agent.mode}...\")\n",
        "    print(f\"Checkpoints will be saved to: {checkpoint_dir}\")\n",
        "\n",
        "    try:\n",
        "        for i_episode in range(1, n_episodes + 1):\n",
        "            try:\n",
        "                # Monitor memory at start of episode\n",
        "                if i_episode % 10 == 0:\n",
        "                    mem = psutil.virtual_memory()\n",
        "                    gpu_mem = torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0\n",
        "                    print(f'\\n[Ep {i_episode}] RAM: {mem.percent:.1f}% | GPU: {gpu_mem:.2f}GB | Buffer: {len(agent.memory)}/{CONFIG[\"BUFFER_SIZE\"]}')\n",
        "\n",
        "                state, info = env.reset(seed=CONFIG['SEED'] if i_episode == 1 else None)\n",
        "\n",
        "                # Check state validity\n",
        "                if state is None:\n",
        "                    print(f\"ERROR: Episode {i_episode} - env.reset() returned None!\")\n",
        "                    continue\n",
        "\n",
        "                state = np.array(state)\n",
        "\n",
        "                # Verify state shape\n",
        "                if state.shape != (4, 84, 84):\n",
        "                    print(f\"ERROR: Episode {i_episode} - Invalid state shape: {state.shape}\")\n",
        "                    continue\n",
        "\n",
        "                score = 0\n",
        "\n",
        "                for t in range(max_t):\n",
        "                    action = agent.act(state, eps)\n",
        "\n",
        "                    next_state_raw, reward, terminated, truncated, info = env.step(action)\n",
        "                    done = terminated or truncated\n",
        "\n",
        "                    # Check for NaN or invalid values\n",
        "                    if np.isnan(reward) or np.isinf(reward):\n",
        "                        print(f\"WARNING: Episode {i_episode}, step {t} - Invalid reward: {reward}\")\n",
        "                        reward = 0.0\n",
        "\n",
        "                    next_state = np.array(next_state_raw)\n",
        "\n",
        "                    # Verify next_state\n",
        "                    if next_state.shape != (4, 84, 84):\n",
        "                        print(f\"ERROR: Episode {i_episode}, step {t} - Invalid next_state shape: {next_state.shape}\")\n",
        "                        break\n",
        "\n",
        "                    reward_np = np.array([reward]).astype(np.float32)\n",
        "                    done_np = np.array([done]).astype(np.uint8)\n",
        "\n",
        "                    agent.step(state, action, reward_np, next_state, done_np)\n",
        "                    state = next_state\n",
        "                    score += reward\n",
        "\n",
        "                    if done:\n",
        "                        break\n",
        "\n",
        "                scores_window.append(score)\n",
        "                scores.append(score)\n",
        "                eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "                avg_score = np.mean(scores_window)\n",
        "\n",
        "                # Print progress\n",
        "                if i_episode % 1 == 0:  # Print every episode for debugging\n",
        "                    print(f'\\rEpisode {i_episode}\\tScore: {score:.1f}\\tAvg: {avg_score:.2f}\\tEps: {eps:.3f}\\tSteps: {t+1}', end=\"\")\n",
        "\n",
        "                # Frequent checkpoints while debugging\n",
        "                if i_episode % CONFIG['CHECKPOINT_FREQ'] == 0:\n",
        "                    print(f'\\n[CHECKPOINT] Episode {i_episode}\\tAverage Score: {avg_score:.2f}')\n",
        "\n",
        "                    checkpoint = {\n",
        "                        'episode': i_episode,\n",
        "                        'model_state_dict': agent.qnetwork_local.state_dict(),\n",
        "                        'target_state_dict': agent.qnetwork_target.state_dict(),\n",
        "                        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "                        'scores': scores,\n",
        "                        'eps': eps,\n",
        "                        'mode': agent.mode\n",
        "                    }\n",
        "                    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_{agent.mode}_ep{i_episode}.pth')\n",
        "                    torch.save(checkpoint, checkpoint_path)\n",
        "                    print(f\"‚úì Checkpoint saved\")\n",
        "\n",
        "                    # Aggressive memory cleanup\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                    mem = psutil.virtual_memory()\n",
        "                    print(f\"RAM: {mem.percent}% ({mem.available / (1024**3):.1f}GB free)\")\n",
        "\n",
        "                # In your training loop, add this after episode 10:\n",
        "                if i_episode == 10:\n",
        "                    print(\"\\n\" + \"=\"*60)\n",
        "                    print(\"MEMORY LEAK INVESTIGATION\")\n",
        "                    print(\"=\"*60)\n",
        "\n",
        "                    # Check what's in the buffer\n",
        "                    if len(agent.memory) > 0:\n",
        "                        sample_exp = list(agent.memory.memory)[0]\n",
        "                        print(f\"State dtype in buffer: {sample_exp.state.dtype}\")\n",
        "                        print(f\"State shape: {sample_exp.state.shape}\")\n",
        "                        print(f\"State memory: {sample_exp.state.nbytes / 1024:.1f} KB\")\n",
        "\n",
        "                        # Calculate expected buffer size\n",
        "                        bytes_per_exp = (sample_exp.state.nbytes + sample_exp.next_state.nbytes + 20)\n",
        "                        expected_mb = (len(agent.memory) * bytes_per_exp) / (1024**2)\n",
        "\n",
        "                        print(f\"\\nBuffer has {len(agent.memory)} experiences\")\n",
        "                        print(f\"Expected buffer size: {expected_mb:.1f} MB\")\n",
        "\n",
        "                        # Check actual RAM\n",
        "                        mem = psutil.virtual_memory()\n",
        "                        print(f\"Actual RAM used: {mem.used / (1024**3):.2f} GB ({mem.percent:.1f}%)\")\n",
        "\n",
        "                        print(\"\\n‚ö†Ô∏è If RAM is much higher than expected buffer size,\")\n",
        "                        print(\"   there's a memory leak outside the buffer!\")\n",
        "\n",
        "                    print(\"=\"*60)\n",
        "\n",
        "                # Check for goal\n",
        "                if avg_score >= CONFIG['GOAL_SCORE']:\n",
        "                    print(f'\\nüéâ Goal Reached in {i_episode} episodes!')\n",
        "                    torch.save(agent.qnetwork_local.state_dict(), os.path.join(checkpoint_dir, f'{agent.mode}_solved_{i_episode}.pth'))\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå ERROR in Episode {i_episode}:\")\n",
        "                print(f\"Exception: {type(e).__name__}: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "                # Save emergency checkpoint\n",
        "                print(\"Saving emergency checkpoint...\")\n",
        "                torch.save({\n",
        "                    'episode': i_episode,\n",
        "                    'model_state_dict': agent.qnetwork_local.state_dict(),\n",
        "                    'scores': scores,\n",
        "                    'eps': eps,\n",
        "                }, os.path.join(checkpoint_dir, f'emergency_ep{i_episode}.pth'))\n",
        "\n",
        "                # Try to continue or break\n",
        "                user_input = input(\"Continue training? (y/n): \")\n",
        "                if user_input.lower() != 'y':\n",
        "                    break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n‚ö†Ô∏è  Training interrupted by user\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\n‚ùå FATAL ERROR:\")\n",
        "        print(f\"Exception: {type(e).__name__}: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    finally:\n",
        "        # Always save what we have\n",
        "        print(\"\\n\\nSaving final results...\")\n",
        "        ALL_SCORES[agent.mode] = scores\n",
        "        np.save(os.path.join(checkpoint_dir, 'dqn_project_scores.npy'), ALL_SCORES)\n",
        "        print(f\"‚úì Saved {len(scores)} episodes\")\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ThBy0kRg0uUx"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to hold scores from all runs\n",
        "ALL_SCORES = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resuming from checkpoint if it exists\n",
        "# Auto-resume from latest checkpoint\n",
        "\n",
        "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, f'checkpoint_{CONFIG[\"MODE\"]}_ep*.pth'))\n",
        "\n",
        "if checkpoint_files:\n",
        "    # Sort to get the latest checkpoint\n",
        "    checkpoint_files.sort(key=lambda x: int(x.split('_ep')[-1].split('.')[0]))\n",
        "    latest_checkpoint = checkpoint_files[-1]\n",
        "\n",
        "    print(f\"\\nüîÑ Found existing checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
        "    print(f\"üìÅ Location: {checkpoint_dir}\")\n",
        "\n",
        "    # Option to resume or start fresh\n",
        "    resume = input(\"Resume from checkpoint? (y/n): \").lower() == 'y'\n",
        "\n",
        "    if resume:\n",
        "        print(f\"Loading checkpoint...\")\n",
        "        checkpoint = torch.load(latest_checkpoint)\n",
        "\n",
        "        agent.qnetwork_local.load_state_dict(checkpoint['model_state_dict'])\n",
        "        agent.qnetwork_target.load_state_dict(checkpoint['target_state_dict'])\n",
        "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Load previous scores\n",
        "        scores_path = os.path.join(checkpoint_dir, 'dqn_project_scores.npy')\n",
        "        if os.path.exists(scores_path):\n",
        "            ALL_SCORES = np.load(scores_path, allow_pickle=True).item()\n",
        "\n",
        "        start_episode = checkpoint['episode']\n",
        "        start_eps = checkpoint['eps']\n",
        "\n",
        "        print(f\"‚úì Resumed from episode {start_episode}\")\n",
        "        print(f\"‚úì Previous scores loaded: {len(checkpoint['scores'])} episodes\")\n",
        "        print(f\"‚úì Epsilon: {start_eps:.4f}\")\n",
        "        if len(checkpoint['scores']) >= 100:\n",
        "            print(f\"‚úì Last 100 episodes avg: {np.mean(checkpoint['scores'][-100:]):.2f}\")\n",
        "\n",
        "        # Update CONFIG to continue from where we left off\n",
        "        CONFIG['EPS_START'] = start_eps\n",
        "    else:\n",
        "        print(\"Starting fresh training...\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting fresh training...\")"
      ],
      "metadata": {
        "id": "HTnFGVp3Bp9c",
        "outputId": "b62a7645-1c52-4aa4-9b4b-3739e3f48820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Found existing checkpoint: checkpoint_SimpleDQN_ep25.pth\n",
            "üìÅ Location: /content/drive/MyDrive/DQN_Checkpoints\n",
            "Resume from checkpoint? (y/n): n\n",
            "Starting fresh training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PRE-TRAINING VERIFICATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SYSTEM CHECK BEFORE TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Check RAM\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"\\nüìä RAM Status:\")\n",
        "print(f\"   Total: {mem.total / (1024**3):.2f} GB\")\n",
        "print(f\"   Available: {mem.available / (1024**3):.2f} GB\")\n",
        "print(f\"   Used: {mem.used / (1024**3):.2f} GB ({mem.percent}%)\")\n",
        "\n",
        "if mem.available / (1024**3) < 2.0:\n",
        "    print(\"   ‚ö†Ô∏è  WARNING: Less than 2GB RAM available!\")\n",
        "else:\n",
        "    print(\"   ‚úì RAM looks good\")\n",
        "\n",
        "# 2. Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nüéÆ GPU Status:\")\n",
        "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Total Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "    print(f\"   Allocated: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
        "    print(f\"   Cached: {torch.cuda.memory_reserved() / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No GPU detected - training will be VERY slow!\")\n",
        "\n",
        "# 3. Verify networks are on GPU\n",
        "print(f\"\\nüß† Network Status:\")\n",
        "print(f\"   Local network device: {next(agent.qnetwork_local.parameters()).device}\")\n",
        "print(f\"   Target network device: {next(agent.qnetwork_target.parameters()).device}\")\n",
        "\n",
        "# 4. Check buffer configuration\n",
        "print(f\"\\nüíæ Replay Buffer:\")\n",
        "print(f\"   Max size: {CONFIG['BUFFER_SIZE']:,}\")\n",
        "print(f\"   Current size: {len(agent.memory)}\")\n",
        "print(f\"   Batch size: {CONFIG['BATCH_SIZE']}\")\n",
        "\n",
        "# Estimate memory usage\n",
        "frames_per_exp = 2 * 4 * 84 * 84  # state + next_state, 4 frames each\n",
        "if hasattr(agent.memory.memory, 'maxlen') and len(agent.memory) > 0:\n",
        "    # Check if using uint8 (1 byte) or float32 (4 bytes)\n",
        "    sample_exp = list(agent.memory.memory)[0]\n",
        "    dtype_size = 1 if sample_exp.state.dtype == np.uint8 else 4\n",
        "    estimated_mb = (CONFIG['BUFFER_SIZE'] * frames_per_exp * dtype_size) / (1024**2)\n",
        "    print(f\"   Data type: {sample_exp.state.dtype}\")\n",
        "    print(f\"   Estimated buffer memory: {estimated_mb:.0f} MB ({estimated_mb/1024:.2f} GB)\")\n",
        "\n",
        "    if dtype_size == 4:\n",
        "        print(\"   ‚ö†Ô∏è  WARNING: Using float32! Switch to uint8 to save 75% memory\")\n",
        "    else:\n",
        "        print(\"   ‚úì Using uint8 compression\")\n",
        "else:\n",
        "    # Empty buffer estimation\n",
        "    estimated_mb_uint8 = (CONFIG['BUFFER_SIZE'] * frames_per_exp * 1) / (1024**2)\n",
        "    estimated_mb_float32 = (CONFIG['BUFFER_SIZE'] * frames_per_exp * 4) / (1024**2)\n",
        "    print(f\"   Estimated memory (uint8): {estimated_mb_uint8:.0f} MB ({estimated_mb_uint8/1024:.2f} GB)\")\n",
        "    print(f\"   Estimated memory (float32): {estimated_mb_float32:.0f} MB ({estimated_mb_float32/1024:.2f} GB)\")\n",
        "\n",
        "# 5. Config summary\n",
        "print(f\"\\n‚öôÔ∏è  Training Config:\")\n",
        "print(f\"   Mode: {CONFIG['MODE']}\")\n",
        "print(f\"   Episodes: {CONFIG['N_EPISODES']}\")\n",
        "print(f\"   Buffer size: {CONFIG['BUFFER_SIZE']:,}\")\n",
        "print(f\"   Epsilon decay: {CONFIG['EPS_DECAY']}\")\n",
        "print(f\"   Checkpoint dir: {checkpoint_dir}\")\n",
        "\n",
        "# 6. Final recommendation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "total_estimated_gb = estimated_mb_uint8 / 1024 if 'estimated_mb_uint8' in locals() else 1.5\n",
        "ram_available = mem.available / (1024**3)\n",
        "\n",
        "if ram_available > total_estimated_gb + 2:  # +2GB safety margin\n",
        "    print(\"‚úÖ READY TO TRAIN - Sufficient resources available\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  RISK OF CRASH - Consider:\")\n",
        "    print(f\"   - Reduce BUFFER_SIZE to {int(CONFIG['BUFFER_SIZE'] * 0.5):,}\")\n",
        "    print(\"   - Make sure uint8 ReplayBuffer is being used\")\n",
        "    print(\"   - Close other applications\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Force garbage collection before starting\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "KdgHNirnEtOY",
        "outputId": "e9dae0f1-4b6a-4d16-97ca-29f8c3818e7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SYSTEM CHECK BEFORE TRAINING\n",
            "============================================================\n",
            "\n",
            "üìä RAM Status:\n",
            "   Total: 12.67 GB\n",
            "   Available: 10.87 GB\n",
            "   Used: 1.48 GB (14.2%)\n",
            "   ‚úì RAM looks good\n",
            "\n",
            "üéÆ GPU Status:\n",
            "   Device: Tesla T4\n",
            "   Total Memory: 14.74 GB\n",
            "   Allocated: 0.01 GB\n",
            "   Cached: 0.02 GB\n",
            "\n",
            "üß† Network Status:\n",
            "   Local network device: cuda:0\n",
            "   Target network device: cuda:0\n",
            "\n",
            "üíæ Replay Buffer:\n",
            "   Max size: 5,000\n",
            "   Current size: 0\n",
            "   Batch size: 32\n",
            "   Estimated memory (uint8): 269 MB (0.26 GB)\n",
            "   Estimated memory (float32): 1077 MB (1.05 GB)\n",
            "\n",
            "‚öôÔ∏è  Training Config:\n",
            "   Mode: SimpleDQN\n",
            "   Episodes: 4000\n",
            "   Buffer size: 5,000\n",
            "   Epsilon decay: 0.999\n",
            "   Checkpoint dir: /content/drive/MyDrive/DQN_Checkpoints\n",
            "\n",
            "============================================================\n",
            "‚úÖ READY TO TRAIN - Sufficient resources available\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this right after creating the agent\n",
        "print(\"\\nüîç Testing ReplayBuffer compression:\")\n",
        "\n",
        "# Create a test state\n",
        "test_state = np.random.rand(4, 84, 84).astype(np.float32)\n",
        "test_action = 0\n",
        "test_reward = 1.0\n",
        "test_next_state = np.random.rand(4, 84, 84).astype(np.float32)\n",
        "test_done = False\n",
        "\n",
        "# Add to buffer\n",
        "agent.memory.add(test_state, test_action, test_reward, test_next_state, test_done)\n",
        "\n",
        "# Check what was stored\n",
        "if len(agent.memory) > 0:\n",
        "    stored_exp = list(agent.memory.memory)[0]\n",
        "    print(f\"Stored state dtype: {stored_exp.state.dtype}\")\n",
        "    print(f\"Stored state shape: {stored_exp.state.shape}\")\n",
        "\n",
        "    if stored_exp.state.dtype == np.uint8:\n",
        "        print(\"‚úÖ ReplayBuffer IS using uint8 compression\")\n",
        "    else:\n",
        "        print(\"‚ùå ReplayBuffer NOT using uint8! This will cause crashes!\")\n",
        "        print(\"You need to use the uint8 ReplayBuffer implementation!\")"
      ],
      "metadata": {
        "id": "Ja7B1cHcGLiI",
        "outputId": "eb15b8b5-2814-4ffd-9b08-5585aaaff9d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Testing ReplayBuffer compression:\n",
            "Stored state dtype: uint8\n",
            "Stored state shape: (4, 84, 84)\n",
            "‚úÖ ReplayBuffer IS using uint8 compression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P87GiE140uUx",
        "outputId": "6c8dd50c-193a-4b24-8df4-d3d1f5901357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training for SimpleDQN...\n",
            "Checkpoints will be saved to: /content/drive/MyDrive/DQN_Checkpoints\n",
            "Episode 9\tScore: 5.0\tAvg: 21.67\tEps: 0.991\tSteps: 139\n",
            "[Ep 10] RAM: 22.1% | GPU: 0.05GB | Buffer: 1182/5000\n",
            "Episode 10\tScore: 125.0\tAvg: 32.00\tEps: 0.990\tSteps: 370\n",
            "============================================================\n",
            "MEMORY LEAK INVESTIGATION\n",
            "============================================================\n",
            "State dtype in buffer: uint8\n",
            "State shape: (4, 84, 84)\n",
            "State memory: 27.6 KB\n",
            "\n",
            "Buffer has 1552 experiences\n",
            "Expected buffer size: 83.6 MB\n",
            "Actual RAM used: 2.77 GB (24.4%)\n",
            "\n",
            "‚ö†Ô∏è If RAM is much higher than expected buffer size,\n",
            "   there's a memory leak outside the buffer!\n",
            "============================================================\n",
            "Episode 19\tScore: 25.0\tAvg: 56.05\tEps: 0.981\tSteps: 121\n",
            "[Ep 20] RAM: 38.4% | GPU: 0.05GB | Buffer: 3736/5000\n",
            "Episode 25\tScore: 0.0\tAvg: 60.80\tEps: 0.975\tSteps: 68\n",
            "[CHECKPOINT] Episode 25\tAverage Score: 60.80\n",
            "‚úì Checkpoint saved\n",
            "RAM: 45.5% (6.9GB free)\n",
            "Episode 29\tScore: 160.0\tAvg: 61.38\tEps: 0.971\tSteps: 466\n",
            "[Ep 30] RAM: 46.7% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 39\tScore: 0.0\tAvg: 66.92\tEps: 0.962\tSteps: 118\n",
            "[Ep 40] RAM: 47.2% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 49\tScore: 75.0\tAvg: 72.45\tEps: 0.952\tSteps: 173\n",
            "[Ep 50] RAM: 46.8% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 50\tScore: 5.0\tAvg: 71.10\tEps: 0.951\tSteps: 110\n",
            "[CHECKPOINT] Episode 50\tAverage Score: 71.10\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.8% (6.7GB free)\n",
            "Episode 59\tScore: 35.0\tAvg: 67.54\tEps: 0.943\tSteps: 113\n",
            "[Ep 60] RAM: 46.8% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 69\tScore: 75.0\tAvg: 72.54\tEps: 0.933\tSteps: 140\n",
            "[Ep 70] RAM: 46.8% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 75\tScore: 105.0\tAvg: 71.07\tEps: 0.928\tSteps: 286\n",
            "[CHECKPOINT] Episode 75\tAverage Score: 71.07\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.8% (6.7GB free)\n",
            "Episode 79\tScore: 30.0\tAvg: 69.62\tEps: 0.924\tSteps: 135\n",
            "[Ep 80] RAM: 46.7% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 89\tScore: 55.0\tAvg: 69.66\tEps: 0.915\tSteps: 266\n",
            "[Ep 90] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 99\tScore: 15.0\tAvg: 66.16\tEps: 0.906\tSteps: 174\n",
            "[Ep 100] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 100\tScore: 135.0\tAvg: 66.85\tEps: 0.905\tSteps: 286\n",
            "[CHECKPOINT] Episode 100\tAverage Score: 66.85\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.5% (6.8GB free)\n",
            "Episode 109\tScore: 0.0\tAvg: 71.40\tEps: 0.897\tSteps: 85\n",
            "[Ep 110] RAM: 46.7% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 119\tScore: 20.0\tAvg: 69.40\tEps: 0.888\tSteps: 132\n",
            "[Ep 120] RAM: 46.7% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 125\tScore: 80.0\tAvg: 66.30\tEps: 0.882\tSteps: 164\n",
            "[CHECKPOINT] Episode 125\tAverage Score: 66.30\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.7% (6.8GB free)\n",
            "Episode 129\tScore: 135.0\tAvg: 65.95\tEps: 0.879\tSteps: 473\n",
            "[Ep 130] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 139\tScore: 0.0\tAvg: 62.65\tEps: 0.870\tSteps: 109\n",
            "[Ep 140] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 149\tScore: 320.0\tAvg: 62.40\tEps: 0.862\tSteps: 369\n",
            "[Ep 150] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 150\tScore: 60.0\tAvg: 62.95\tEps: 0.861\tSteps: 298\n",
            "[CHECKPOINT] Episode 150\tAverage Score: 62.95\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.5% (6.8GB free)\n",
            "Episode 159\tScore: 20.0\tAvg: 64.90\tEps: 0.853\tSteps: 160\n",
            "[Ep 160] RAM: 46.4% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 169\tScore: 50.0\tAvg: 57.10\tEps: 0.844\tSteps: 176\n",
            "[Ep 170] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 175\tScore: 0.0\tAvg: 55.85\tEps: 0.839\tSteps: 70\n",
            "[CHECKPOINT] Episode 175\tAverage Score: 55.85\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "Episode 179\tScore: 35.0\tAvg: 56.95\tEps: 0.836\tSteps: 187\n",
            "[Ep 180] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 189\tScore: 75.0\tAvg: 61.15\tEps: 0.828\tSteps: 159\n",
            "[Ep 190] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 199\tScore: 30.0\tAvg: 66.90\tEps: 0.819\tSteps: 305\n",
            "[Ep 200] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 200\tScore: 105.0\tAvg: 66.60\tEps: 0.819\tSteps: 183\n",
            "[CHECKPOINT] Episode 200\tAverage Score: 66.60\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "Episode 209\tScore: 0.0\tAvg: 66.25\tEps: 0.811\tSteps: 75\n",
            "[Ep 210] RAM: 46.4% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 219\tScore: 50.0\tAvg: 69.35\tEps: 0.803\tSteps: 217\n",
            "[Ep 220] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 225\tScore: 80.0\tAvg: 70.60\tEps: 0.798\tSteps: 200\n",
            "[CHECKPOINT] Episode 225\tAverage Score: 70.60\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.5% (6.8GB free)\n",
            "Episode 229\tScore: 70.0\tAvg: 70.30\tEps: 0.795\tSteps: 379\n",
            "[Ep 230] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 239\tScore: 135.0\tAvg: 70.15\tEps: 0.787\tSteps: 296\n",
            "[Ep 240] RAM: 46.7% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 249\tScore: 55.0\tAvg: 66.20\tEps: 0.779\tSteps: 171\n",
            "[Ep 250] RAM: 46.7% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 250\tScore: 0.0\tAvg: 65.60\tEps: 0.779\tSteps: 76\n",
            "[CHECKPOINT] Episode 250\tAverage Score: 65.60\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "Episode 259\tScore: 0.0\tAvg: 61.35\tEps: 0.772\tSteps: 57\n",
            "[Ep 260] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 269\tScore: 50.0\tAvg: 63.65\tEps: 0.764\tSteps: 111\n",
            "[Ep 270] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 275\tScore: 35.0\tAvg: 65.05\tEps: 0.759\tSteps: 145\n",
            "[CHECKPOINT] Episode 275\tAverage Score: 65.05\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "Episode 279\tScore: 0.0\tAvg: 64.90\tEps: 0.756\tSteps: 97\n",
            "[Ep 280] RAM: 46.7% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 289\tScore: 275.0\tAvg: 59.70\tEps: 0.749\tSteps: 292\n",
            "[Ep 290] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 299\tScore: 55.0\tAvg: 55.20\tEps: 0.741\tSteps: 145\n",
            "[Ep 300] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 300\tScore: 285.0\tAvg: 57.00\tEps: 0.741\tSteps: 375\n",
            "[CHECKPOINT] Episode 300\tAverage Score: 57.00\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.7% (6.8GB free)\n",
            "Episode 309\tScore: 35.0\tAvg: 54.30\tEps: 0.734\tSteps: 152\n",
            "[Ep 310] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 319\tScore: 80.0\tAvg: 49.30\tEps: 0.727\tSteps: 178\n",
            "[Ep 320] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 325\tScore: 65.0\tAvg: 48.40\tEps: 0.722\tSteps: 136\n",
            "[CHECKPOINT] Episode 325\tAverage Score: 48.40\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "Episode 329\tScore: 100.0\tAvg: 48.70\tEps: 0.720\tSteps: 342\n",
            "[Ep 330] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 339\tScore: 0.0\tAvg: 47.90\tEps: 0.712\tSteps: 80\n",
            "[Ep 340] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 349\tScore: 105.0\tAvg: 46.15\tEps: 0.705\tSteps: 290\n",
            "[Ep 350] RAM: 46.4% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 350\tScore: 25.0\tAvg: 46.40\tEps: 0.705\tSteps: 101\n",
            "[CHECKPOINT] Episode 350\tAverage Score: 46.40\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.5% (6.8GB free)\n",
            "Episode 359\tScore: 10.0\tAvg: 54.90\tEps: 0.698\tSteps: 89\n",
            "[Ep 360] RAM: 46.4% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 369\tScore: 0.0\tAvg: 52.95\tEps: 0.691\tSteps: 87\n",
            "[Ep 370] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 375\tScore: 35.0\tAvg: 53.40\tEps: 0.687\tSteps: 151\n",
            "[CHECKPOINT] Episode 375\tAverage Score: 53.40\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.5% (6.8GB free)\n",
            "Episode 379\tScore: 0.0\tAvg: 51.15\tEps: 0.684\tSteps: 91\n",
            "[Ep 380] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 389\tScore: 55.0\tAvg: 49.65\tEps: 0.678\tSteps: 159\n",
            "[Ep 390] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 399\tScore: 305.0\tAvg: 53.85\tEps: 0.671\tSteps: 391\n",
            "[Ep 400] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 400\tScore: 35.0\tAvg: 51.35\tEps: 0.670\tSteps: 206\n",
            "[CHECKPOINT] Episode 400\tAverage Score: 51.35\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.5% (6.8GB free)\n",
            "Episode 409\tScore: 55.0\tAvg: 51.15\tEps: 0.664\tSteps: 191\n",
            "[Ep 410] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 419\tScore: 55.0\tAvg: 52.05\tEps: 0.658\tSteps: 176\n",
            "[Ep 420] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 425\tScore: 20.0\tAvg: 55.30\tEps: 0.654\tSteps: 149\n",
            "[CHECKPOINT] Episode 425\tAverage Score: 55.30\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "Episode 429\tScore: 15.0\tAvg: 54.40\tEps: 0.651\tSteps: 248\n",
            "[Ep 430] RAM: 46.4% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 439\tScore: 5.0\tAvg: 52.10\tEps: 0.645\tSteps: 126\n",
            "[Ep 440] RAM: 46.4% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 449\tScore: 80.0\tAvg: 54.25\tEps: 0.638\tSteps: 185\n",
            "[Ep 450] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 450\tScore: 10.0\tAvg: 54.10\tEps: 0.637\tSteps: 122\n",
            "[CHECKPOINT] Episode 450\tAverage Score: 54.10\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.5% (6.8GB free)\n",
            "Episode 459\tScore: 105.0\tAvg: 49.05\tEps: 0.632\tSteps: 346\n",
            "[Ep 460] RAM: 46.3% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 469\tScore: 105.0\tAvg: 51.85\tEps: 0.625\tSteps: 166\n",
            "[Ep 470] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 475\tScore: 50.0\tAvg: 52.95\tEps: 0.622\tSteps: 158\n",
            "[CHECKPOINT] Episode 475\tAverage Score: 52.95\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "Episode 479\tScore: 55.0\tAvg: 55.10\tEps: 0.619\tSteps: 151\n",
            "[Ep 480] RAM: 46.4% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 489\tScore: 105.0\tAvg: 58.65\tEps: 0.613\tSteps: 239\n",
            "[Ep 490] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 499\tScore: 25.0\tAvg: 53.05\tEps: 0.607\tSteps: 138\n",
            "[Ep 500] RAM: 46.5% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 500\tScore: 30.0\tAvg: 53.00\tEps: 0.606\tSteps: 112\n",
            "[CHECKPOINT] Episode 500\tAverage Score: 53.00\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "Episode 509\tScore: 100.0\tAvg: 55.15\tEps: 0.601\tSteps: 178\n",
            "[Ep 510] RAM: 46.7% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 519\tScore: 25.0\tAvg: 53.10\tEps: 0.595\tSteps: 128\n",
            "[Ep 520] RAM: 46.6% | GPU: 0.05GB | Buffer: 5000/5000\n",
            "Episode 525\tScore: 75.0\tAvg: 50.15\tEps: 0.591\tSteps: 162\n",
            "[CHECKPOINT] Episode 525\tAverage Score: 50.15\n",
            "‚úì Checkpoint saved\n",
            "RAM: 46.6% (6.8GB free)\n",
            "\n",
            "\n",
            "‚ö†Ô∏è  Training interrupted by user\n",
            "\n",
            "\n",
            "Saving final results...\n",
            "‚úì Saved 525 episodes\n"
          ]
        }
      ],
      "source": [
        "# Run training\n",
        "scores = dqn_train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU5ktOld0uUy"
      },
      "outputs": [],
      "source": [
        "def plot_all_dqn_scores(all_scores_dict, window=100):\n",
        "    \"\"\"\n",
        "    Loads scores for all DQN variants and plots their moving average on a single graph.\n",
        "\n",
        "    Args:\n",
        "        all_scores_dict (dict): Dictionary mapping mode names ('SimpleDQN', etc.) to lists of episode scores.\n",
        "        window (int): The window size for the moving average.\n",
        "    \"\"\"\n",
        "    if not all_scores_dict:\n",
        "        print(\"No scores available to plot. Please run training for at least one agent.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for mode, scores in all_scores_dict.items():\n",
        "        if len(scores) >= window:\n",
        "            # Calculate 100-episode moving average\n",
        "            moving_avg = np.convolve(scores, np.ones(window)/window, mode='valid')\n",
        "\n",
        "            # The x-axis should start at the window size, as the moving average starts there\n",
        "            x_axis = np.arange(len(moving_avg)) + window\n",
        "\n",
        "            plt.plot(x_axis, moving_avg, label=f'{mode} (Avg={moving_avg[-1]:.2f})')\n",
        "        else:\n",
        "            print(f\"Not enough data to calculate moving average for {mode}.\")\n",
        "\n",
        "    # Add score targets as horizontal lines, similar to the presentation graph\n",
        "    plt.axhline(y=400, color='r', linestyle='--', linewidth=1, label='Goal: 400')\n",
        "    plt.axhline(y=500, color='g', linestyle='--', linewidth=1, label='Goal: 500')\n",
        "\n",
        "    plt.title('Consolidated DQN Training Progress (100-Episode Moving Average)')\n",
        "    plt.ylabel('Average Score (100-Game Window)')\n",
        "    plt.xlabel('Episode #')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('all_dqn_scores.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jC9Nqis0uUy"
      },
      "outputs": [],
      "source": [
        "# Run run AFTER you have completed at least one training run\n",
        "try:\n",
        "    # Load saved scores (if they exist from previous runs)\n",
        "    loaded_scores = np.load('dqn_project_scores.npy', allow_pickle=True).item()\n",
        "    plot_all_dqn_scores(loaded_scores)\n",
        "except FileNotFoundError:\n",
        "    print(\"Scores file not found. Run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video_display"
      },
      "source": [
        "## 6. Video Visualization Utility\n",
        "This function is provided for optional video recording using a trained model's weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "video_code"
      },
      "outputs": [],
      "source": [
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"Gets a string containing a b64-encoded version of the MP4 video.\"\"\"\n",
        "  import os\n",
        "  if not os.path.exists(videopath):\n",
        "      return f\"<p>Video file not found at {videopath}. Run a test episode first.</p>\"\n",
        "\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "\n",
        "def run_and_record(env_id, weights_path, mode, seed=CONFIG['SEED'], num_episodes=1):\n",
        "    \"\"\"Runs a specified agent on the environment and records the interaction.\"\"\"\n",
        "\n",
        "    # 1. Setup Environment\n",
        "    # Use the same wrapper stack as training for consistent state representation\n",
        "    env_render = make_atari_env(env_id, seed=seed)\n",
        "\n",
        "    # 2. Setup Agent\n",
        "    action_size = env_render.action_space.n\n",
        "    state_shape = env_render.observation_space.shape\n",
        "\n",
        "    # Use the appropriate Agent class\n",
        "    if mode == \"SimpleDQN\":\n",
        "        test_agent = SimpleDQNAgent(state_shape, action_size, seed)\n",
        "    elif mode == \"DoubleDQN\":\n",
        "        test_agent = DoubleDQNAgent(state_shape, action_size, seed)\n",
        "    elif mode == \"DuelingDQN\":\n",
        "        test_agent = DuelingDQNAgent(state_shape, action_size, seed)\n",
        "    else:\n",
        "        return f\"<p>Invalid MODE specified for testing: {mode}</p>\"\n",
        "\n",
        "    # 3. Load Weights\n",
        "    try:\n",
        "        test_agent.qnetwork_local.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "        test_agent.qnetwork_local.eval()\n",
        "        print(f\"Successfully loaded {mode} weights from {weights_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Checkpoint file {weights_path} not found. Agent will use random weights.\")\n",
        "        return\n",
        "\n",
        "    # 4. Record Episodes\n",
        "    video_path = f'{mode}_{env_id.split(\"/\")[-1]}_test.mp4'\n",
        "    frames = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env_render.reset(seed=seed)\n",
        "        score = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # The FrameStack wrapper returns a LazyFrame, convert to NumPy array\n",
        "            state_np = np.array(state)\n",
        "            action = test_agent.act(state_np, eps=0.0)\n",
        "\n",
        "            # Capture frame (convert to RGB before saving)\n",
        "            frames.append(env_render.render())\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env_render.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "        print(f\"Test Episode {episode+1} score: {score:.2f}\")\n",
        "\n",
        "    env_render.close()\n",
        "\n",
        "    # Save video\n",
        "    imageio.mimsave(video_path, frames, fps=30)\n",
        "\n",
        "    # Display video\n",
        "    html = render_mp4(video_path)\n",
        "    ipythondisplay.display(ipythondisplay.HTML(html))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FJMkbNG0uUy"
      },
      "outputs": [],
      "source": [
        "# Example usage (Uncomment and update weights_path after training):\n",
        "# run_and_record(CONFIG['ENV_ID'], 'SimpleDQN_5000.pth', 'SimpleDQN', num_episodes=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "AIDL_B02_DQN_SpaceInvaders_Variants_Clean.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}