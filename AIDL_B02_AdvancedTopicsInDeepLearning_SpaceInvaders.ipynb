{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/AIDL_B02_AdvancedTopicsInDeepLearning_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkZJMnJp0uUq"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/AIDL_B02_AdvancedTopicsInDeepLearning_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# ðŸš€ Project Base: DQN Variants for ALE/SpaceInvaders-v5\n",
        "\n",
        "This notebook strictly implements the project's requirements for the **`ALE/SpaceInvaders-v5`** environment with 4-frame stacking and CNN architecture.\n",
        "\n",
        "**Key Requirements Met:**\n",
        "* **Environment:** `ALE/SpaceInvaders-v5` [cite: 11]\n",
        "* **Action Space:** 6 actions [cite: 13, 21]\n",
        "* **State:** 4 stacked input frames [cite: 19]\n",
        "\n",
        "**To run an implementation:**\n",
        "1.  Change the `CONFIG['MODE']` variable below to one of: **`SimpleDQN`**, **`DoubleDQN`**, or **`DuelingDQN`**.\n",
        "2.  Adjust hyperparameters (`LR`, `EPS_DECAY`, etc.) in the `CONFIG` dictionary if needed.\n",
        "3.  Run all cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install",
        "outputId": "49b1070e-f6ab-4b6a-a7d0-b60d4a42faff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari,other] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (0.0.4)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (1.0.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (4.12.0.88)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,other]) (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (2.9.0.post0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (0.6.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn>=0.13->gymnasium[accept-rom-license,atari,other]) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[accept-rom-license,atari,other]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[accept-rom-license,atari,other]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[accept-rom-license,atari,other]) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,other]) (2025.11.12)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\n",
            "The following additional packages will be installed:\n",
            "  libxcomposite1 libxtst6 libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxcomposite1 libxtst6 libxxf86dga1 x11-utils\n",
            "0 upgraded, 4 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 239 kB of archives.\n",
            "After this operation, 852 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Fetched 239 kB in 1s (250 kB/s)\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gymnasium[atari,accept-rom-license,other]\" ale-py\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y xvfb x11-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "config",
        "outputId": "13c5cb9f-a88e-4537-c97d-8068ebdb2fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gymnasium.wrappers.frame_stack'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3806314425.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#from gymnasium.wrappers import AtariPreprocessing, FrameStack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari_preprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAtariPreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_stack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFrameStack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Tools for video display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium.wrappers.frame_stack'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "#from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
        "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
        "from gymnasium.wrappers.frame_stack import FrameStack\n",
        "\n",
        "# Tools for video display\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from base64 import b64encode\n",
        "\n",
        "# ----------------- GLOBAL CONFIGURATION -----------------\n",
        "CONFIG = {\n",
        "    \"ENV_ID\": 'ALE/SpaceInvaders-v5',\n",
        "    \"SEED\": 7,\n",
        "    \"MODE\": \"SimpleDQN\", # Choice --> 'SimpleDQN', 'DoubleDQN', 'DuelingDQN'\n",
        "    \"INPUT_SHAPE\": (4, 84, 84), # 4 stacked frames, resized to 84x84\n",
        "    \"BUFFER_SIZE\": int(1e5),\n",
        "    \"BATCH_SIZE\": 32, # Reduced batch size (common practice for Atari, hinted in PDF [cite: 37])\n",
        "    \"GAMMA\": 0.99, # Prioritizing long-term cumulative reward\n",
        "    \"TAU\": 1e-3, # Soft Update Rate\n",
        "    \"LR\": 1e-4, # Lower learning rate --> stable convergence\n",
        "    \"UPDATE_EVERY\": 4, # Learn frequency (standard for Atari DQN)\n",
        "    \"TARGET_UPDATE_FREQ\": 1000,\n",
        "    \"N_EPISODES\": 5000,\n",
        "    \"EPS_START\": 1.0, # Initial probability of choosing a random action (exploration) --> fully exploring the environment to gather initial experiences\n",
        "    \"EPS_END\": 0.01, # Minimum probability of choosing a random action.\n",
        "    \"EPS_DECAY\": 0.999 # Exploration rate decays very slowly, allowing the agent to explore over a large number of episodes\n",
        "}\n",
        "# --------------------------------------------------------\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random.seed(CONFIG['SEED'])\n",
        "np.random.seed(CONFIG['SEED'])\n",
        "torch.manual_seed(CONFIG['SEED'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(CONFIG['SEED'])\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Current DQN Mode: {CONFIG['MODE']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_init"
      },
      "source": [
        "## 2. Environment Initialization\n",
        "We use **`AtariPreprocessing`** to handle resizing/cropping to 84x84 and grayscale conversion. **`FrameStack`** then stacks 4 consecutive frames, fulfilling the requirements for the state space[cite: 19, 20]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_run"
      },
      "outputs": [],
      "source": [
        "def make_atari_env(env_id, seed):\n",
        "    \"\"\"Creates and wraps the Atari environment with standard preprocessing and 4-frame stacking.\"\"\"\n",
        "    # 1. Base Environment (Using the required ID [cite: 11])\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    # 2. Atari Preprocessing: Resizes to 84x84, grayscale, handles max-pooling/skip.\n",
        "    # Frame skip is set to 1 here because the ALE/SpaceInvaders-v5 environment generally handles skips\n",
        "    # implicitly, or we rely on the standard wrappers' internal logic for compatibility.\n",
        "    env = AtariPreprocessing(env, grayscale_obs=True, terminal_on_life_loss=True, frame_skip=1, screen_size=84)\n",
        "\n",
        "    # 3. Frame Stacking (Creates the (4, 84, 84) state [cite: 19])\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "\n",
        "    # Set seed on the final environment\n",
        "    if seed is not None:\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "\n",
        "    return env\n",
        "\n",
        "env = make_atari_env(CONFIG['ENV_ID'], CONFIG['SEED'])\n",
        "action_size = env.action_space.n\n",
        "state_shape = env.observation_space.shape\n",
        "\n",
        "print(f'Final State shape (Stacked Frames): {state_shape}')\n",
        "print(f'Number of available actions (SpaceInvaders): {action_size}') # Confirms 6 actions [cite: 13, 21]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "network_arch"
      },
      "source": [
        "## 3. Q-Network Architecture\n",
        "The network uses a CNN architecture  to process the high-dimensional image input, supporting Dueling components via a flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNetwork_code"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"CNN-based Q-Network Model supporting Standard and Dueling structures.\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_size, seed, dueling=False):\n",
        "        \"\"\"Initializes the shared CNN layers and splits into Value/Advantage streams if Dueling is enabled.\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.dueling = dueling\n",
        "        in_channels = state_shape[0] # 4 stacked frames\n",
        "\n",
        "        # --- Shared CNN Layers (Original DQN architecture) ---\n",
        "        # Layers extract features from the 4 stacked 84x84 input images.\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        # --- Dynamic Calculation of fc_input_size ---\n",
        "        # This prevents the network from breaking if the input image size changes.\n",
        "        # 1. Create a dummy input tensor based on state_shape (e.g., (1, 4, 84, 84))\n",
        "        dummy_input = torch.zeros(1, *state_shape)\n",
        "\n",
        "        # 2. Pass the dummy input through the convolutional layers\n",
        "        x = self._forward_conv(dummy_input)\n",
        "\n",
        "        # 3. Calculate the flattened feature size (e.g., 7*7*64 = 3136)\n",
        "        self.fc_input_size = x.view(1, -1).size(1)\n",
        "\n",
        "        # --- Fully Connected Layers ---\n",
        "        if self.dueling:\n",
        "            # Dueling Architecture: Split into Value (V) and Advantage (A) streams\n",
        "            self.fc_v1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc_a1 = nn.Linear(self.fc_input_size, 512)\n",
        "\n",
        "            self.fc_v2 = nn.Linear(512, 1) # Output V(s)\n",
        "            self.fc_a2 = nn.Linear(512, action_size) # Output A(s, a)\n",
        "        else:\n",
        "            # Standard DQN Architecture (Single Q-stream)\n",
        "            self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc2 = nn.Linear(512, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Maps state (4, 84, 84) to action values (6).\"\"\"\n",
        "        # CNN forward pass\n",
        "        x = F.relu(self.conv1(state))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "\n",
        "        if self.dueling:\n",
        "        # Dueling Combination: Q(s,a) = V(s) + [A(s,a) - mean(A(s,a))]\n",
        "            v = F.relu(self.fc_v1(x))\n",
        "            a = F.relu(self.fc_a1(x))\n",
        "            v = self.fc_v2(v)\n",
        "            a = self.fc_a2(a)\n",
        "            return v + a - a.mean(1).unsqueeze(1)\n",
        "        else:\n",
        "            # Standard Q-stream\n",
        "            x = F.relu(self.fc1(x))\n",
        "            return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buffer_agents"
      },
      "source": [
        "## 4. Replay Buffer and Agent Implementations\n",
        "The **Replay Buffer** (PER is an optional extension [cite: 27]) is crucial for breaking correlation in experience samples. The **AgentBase** handles common functions; specialized classes implement the specific Q-learning update rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReplayBuffer_code"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples, essential for DQN.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initializes the ReplayBuffer.\"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Adds a new experience (s, a, r, s', done) to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly samples a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        # Convert FrameStack/NumPy data into required Torch tensor shape (B, C, H, W)\n",
        "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.array([e.action for e in experiences if e is not None])).long().unsqueeze(1).to(device)\n",
        "        rewards = torch.from_numpy(np.array([e.reward for e in experiences if e is not None])).float().unsqueeze(1).to(device)\n",
        "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.array([e.done for e in experiences if e is not None]).astype(np.uint8)).float().unsqueeze(1).to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agent_init"
      },
      "outputs": [],
      "source": [
        "class AgentBase:\n",
        "    \"\"\"Base class for all DQN agents, handling shared components and target network logic.\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_size, seed, mode, dueling):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.mode = mode\n",
        "\n",
        "        # Initialize Q-Networks\n",
        "        self.qnetwork_local = QNetwork(state_shape, action_size, seed, dueling=dueling).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_shape, action_size, seed, dueling=dueling).to(device)\n",
        "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=CONFIG['LR'])\n",
        "        self.memory = ReplayBuffer(action_size, CONFIG['BUFFER_SIZE'], CONFIG['BATCH_SIZE'], seed)\n",
        "\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY steps\n",
        "        self.t_step = (self.t_step + 1) % CONFIG['UPDATE_EVERY']\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > CONFIG['BATCH_SIZE']:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, CONFIG['GAMMA'])\n",
        "\n",
        "        # Hard update the target network periodically (standard for Atari)\n",
        "        if self.t_step % CONFIG['TARGET_UPDATE_FREQ'] == 0:\n",
        "           self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Returns action based on epsilon-greedy policy.\"\"\"\n",
        "        state = torch.from_numpy(np.copy(state)).float().unsqueeze(0).to(device)\n",
        "\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        # Placeholder, implemented by child classes\n",
        "        pass\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 1: Simple DQN (Original Target Calculation) ---\n",
        "# ----------------------------------------------------------------\n",
        "class SimpleDQNAgent(AgentBase):\n",
        "    \"\"\"Implements the original DQN learning step: Target Q = R + gamma * max_a Q_target(s', a).\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Standard QNetwork (dueling=False)\n",
        "        super().__init__(state_shape, action_size, seed, mode='SimpleDQN', dueling=False)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Target Q calculation uses the max Q-value from the target network directly.\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 2: Double DQN (Decoupled Target Calculation) ---\n",
        "# ----------------------------------------------------------------\n",
        "class DoubleDQNAgent(AgentBase):\n",
        "    \"\"\"Implements the Double DQN learning step: Target Q = R + gamma * Q_target(s', argmax_a Q_local(s', a)).\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Standard QNetwork (dueling=False)\n",
        "        super().__init__(state_shape, action_size, seed, mode='DoubleDQN', dueling=False)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # 1. Action selection from LOCAL network (argmax_a Q_local(s', a))\n",
        "        Q_local_next = self.qnetwork_local(next_states).detach()\n",
        "        best_actions = Q_local_next.max(1)[1].unsqueeze(1)\n",
        "\n",
        "        # 2. Value estimation from TARGET network (Q_target(s', best_actions))\n",
        "        Q_targets_next = self.qnetwork_target(next_states).gather(1, best_actions).detach()\n",
        "\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 3: Dueling DQN (Dueling Architecture + Double Learning Rule) ---\n",
        "# ----------------------------------------------------------------\n",
        "class DuelingDQNAgent(DoubleDQNAgent):\n",
        "    \"\"\"Dueling DQN uses the Dueling architecture and the Double DQN learning rule for stability.\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Dueling QNetwork (dueling=True)\n",
        "        AgentBase.__init__(self, state_shape, action_size, seed, mode='DuelingDQN', dueling=True)\n",
        "\n",
        "    # Inherits the Double DQN learn() method for stability\n",
        "\n",
        "\n",
        "# --- Agent Initialization based on global CONFIG['MODE'] ---\n",
        "if CONFIG['MODE'] == \"SimpleDQN\":\n",
        "    agent = SimpleDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "elif CONFIG['MODE'] == \"DoubleDQN\":\n",
        "    agent = DoubleDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "elif CONFIG['MODE'] == \"DuelingDQN\":\n",
        "    agent = DuelingDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "else:\n",
        "    raise ValueError(\"Invalid MODE specified in CONFIG.\")\n",
        "\n",
        "print(f\"Initialized agent: {type(agent).__name__} with learning mode: {agent.mode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_loop"
      },
      "source": [
        "## 5. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_code"
      },
      "outputs": [],
      "source": [
        "def dqn(n_episodes=CONFIG['N_EPISODES'], max_t=10000, eps_start=CONFIG['EPS_START'], eps_end=CONFIG['EPS_END'], eps_decay=CONFIG['EPS_DECAY']):\n",
        "    \"\"\"Deep Q-Learning training function, modified to save scores.\"\"\"\n",
        "\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "\n",
        "    GOAL_SCORE = 400.0\n",
        "\n",
        "    # Check for existing scores to prevent overwriting if continuing a run\n",
        "    global ALL_SCORES\n",
        "\n",
        "    print(f\"\\nStarting training for {agent.mode}...\")\n",
        "\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        state, info = env.reset(seed=CONFIG['SEED'] if i_episode == 1 else None)\n",
        "        state = np.array(state)\n",
        "        score = 0\n",
        "\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "\n",
        "            next_state_raw, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            next_state = np.array(next_state_raw)\n",
        "            reward_np = np.array([reward]).astype(np.float32)\n",
        "            done_np = np.array([done]).astype(np.uint8)\n",
        "\n",
        "            agent.step(state, action, reward_np, next_state, done_np)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "        avg_score = np.mean(scores_window)\n",
        "\n",
        "        print(f'\\rEpisode {i_episode}\\tAverage Score: {avg_score:.2f}\\tEpsilon: {eps:.4f}', end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {avg_score:.2f}\\tEpsilon: {eps:.4f}')\n",
        "\n",
        "        if avg_score >= GOAL_SCORE:\n",
        "            print(f'\\n{agent.mode} Goal Reached in {i_episode-100} episodes!\\\\tAverage Score: {avg_score:.2f}')\n",
        "            torch.save(agent.qnetwork_local.state_dict(), f'{agent.mode}_{i_episode}.pth')\n",
        "            break\n",
        "\n",
        "    # Save the scores of the completed run\n",
        "    ALL_SCORES[agent.mode] = scores\n",
        "    np.save('dqn_project_scores.npy', ALL_SCORES)\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThBy0kRg0uUx"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to hold scores from all runs\n",
        "ALL_SCORES = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P87GiE140uUx"
      },
      "outputs": [],
      "source": [
        "# Run training\n",
        "scores = dqn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU5ktOld0uUy"
      },
      "outputs": [],
      "source": [
        "def plot_all_dqn_scores(all_scores_dict, window=100):\n",
        "    \"\"\"\n",
        "    Loads scores for all DQN variants and plots their moving average on a single graph.\n",
        "\n",
        "    Args:\n",
        "        all_scores_dict (dict): Dictionary mapping mode names ('SimpleDQN', etc.) to lists of episode scores.\n",
        "        window (int): The window size for the moving average.\n",
        "    \"\"\"\n",
        "    if not all_scores_dict:\n",
        "        print(\"No scores available to plot. Please run training for at least one agent.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for mode, scores in all_scores_dict.items():\n",
        "        if len(scores) >= window:\n",
        "            # Calculate 100-episode moving average\n",
        "            moving_avg = np.convolve(scores, np.ones(window)/window, mode='valid')\n",
        "\n",
        "            # The x-axis should start at the window size, as the moving average starts there\n",
        "            x_axis = np.arange(len(moving_avg)) + window\n",
        "\n",
        "            plt.plot(x_axis, moving_avg, label=f'{mode} (Avg={moving_avg[-1]:.2f})')\n",
        "        else:\n",
        "            print(f\"Not enough data to calculate moving average for {mode}.\")\n",
        "\n",
        "    # Add score targets as horizontal lines, similar to the presentation graph\n",
        "    plt.axhline(y=400, color='r', linestyle='--', linewidth=1, label='Goal: 400')\n",
        "    plt.axhline(y=500, color='g', linestyle='--', linewidth=1, label='Goal: 500')\n",
        "\n",
        "    plt.title('Consolidated DQN Training Progress (100-Episode Moving Average)')\n",
        "    plt.ylabel('Average Score (100-Game Window)')\n",
        "    plt.xlabel('Episode #')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('all_dqn_scores.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jC9Nqis0uUy"
      },
      "outputs": [],
      "source": [
        "# Run run AFTER you have completed at least one training run\n",
        "try:\n",
        "    # Load saved scores (if they exist from previous runs)\n",
        "    loaded_scores = np.load('dqn_project_scores.npy', allow_pickle=True).item()\n",
        "    plot_all_dqn_scores(loaded_scores)\n",
        "except FileNotFoundError:\n",
        "    print(\"Scores file not found. Run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video_display"
      },
      "source": [
        "## 6. Video Visualization Utility\n",
        "This function is provided for optional video recording using a trained model's weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "video_code"
      },
      "outputs": [],
      "source": [
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"Gets a string containing a b64-encoded version of the MP4 video.\"\"\"\n",
        "  import os\n",
        "  if not os.path.exists(videopath):\n",
        "      return f\"<p>Video file not found at {videopath}. Run a test episode first.</p>\"\n",
        "\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "\n",
        "def run_and_record(env_id, weights_path, mode, seed=CONFIG['SEED'], num_episodes=1):\n",
        "    \"\"\"Runs a specified agent on the environment and records the interaction.\"\"\"\n",
        "\n",
        "    # 1. Setup Environment\n",
        "    # Use the same wrapper stack as training for consistent state representation\n",
        "    env_render = make_atari_env(env_id, seed=seed)\n",
        "\n",
        "    # 2. Setup Agent\n",
        "    action_size = env_render.action_space.n\n",
        "    state_shape = env_render.observation_space.shape\n",
        "\n",
        "    # Use the appropriate Agent class\n",
        "    if mode == \"SimpleDQN\":\n",
        "        test_agent = SimpleDQNAgent(state_shape, action_size, seed)\n",
        "    elif mode == \"DoubleDQN\":\n",
        "        test_agent = DoubleDQNAgent(state_shape, action_size, seed)\n",
        "    elif mode == \"DuelingDQN\":\n",
        "        test_agent = DuelingDQNAgent(state_shape, action_size, seed)\n",
        "    else:\n",
        "        return f\"<p>Invalid MODE specified for testing: {mode}</p>\"\n",
        "\n",
        "    # 3. Load Weights\n",
        "    try:\n",
        "        test_agent.qnetwork_local.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "        test_agent.qnetwork_local.eval()\n",
        "        print(f\"Successfully loaded {mode} weights from {weights_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Checkpoint file {weights_path} not found. Agent will use random weights.\")\n",
        "        return\n",
        "\n",
        "    # 4. Record Episodes\n",
        "    video_path = f'{mode}_{env_id.split(\"/\")[-1]}_test.mp4'\n",
        "    frames = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env_render.reset(seed=seed)\n",
        "        score = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # The FrameStack wrapper returns a LazyFrame, convert to NumPy array\n",
        "            state_np = np.array(state)\n",
        "            action = test_agent.act(state_np, eps=0.0)\n",
        "\n",
        "            # Capture frame (convert to RGB before saving)\n",
        "            frames.append(env_render.render())\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env_render.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "        print(f\"Test Episode {episode+1} score: {score:.2f}\")\n",
        "\n",
        "    env_render.close()\n",
        "\n",
        "    # Save video\n",
        "    imageio.mimsave(video_path, frames, fps=30)\n",
        "\n",
        "    # Display video\n",
        "    html = render_mp4(video_path)\n",
        "    ipythondisplay.display(ipythondisplay.HTML(html))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FJMkbNG0uUy"
      },
      "outputs": [],
      "source": [
        "# Example usage (Uncomment and update weights_path after training):\n",
        "# run_and_record(CONFIG['ENV_ID'], 'SimpleDQN_5000.pth', 'SimpleDQN', num_episodes=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "AIDL_B02_DQN_SpaceInvaders_Variants_Clean.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}