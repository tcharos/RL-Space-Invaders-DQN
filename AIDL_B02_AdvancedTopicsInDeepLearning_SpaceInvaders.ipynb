{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://github.com/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/f5f7e5f32210d47889e1d0b09b0c67fcd6edb951/AIDL_B02_AdvancedTopicsInDeepLearning_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# ðŸš€ Project Base: DQN Variants for ALE/SpaceInvaders-v5\n",
        "\n",
        "This notebook strictly implements the project's requirements for the **`ALE/SpaceInvaders-v5`** environment with 4-frame stacking and CNN architecture.\n",
        "\n",
        "**Key Requirements Met:**\n",
        "* **Environment:** `ALE/SpaceInvaders-v5` [cite: 11]\n",
        "* **Action Space:** 6 actions [cite: 13, 21]\n",
        "* **State:** 4 stacked input frames [cite: 19]\n",
        "\n",
        "**To run an implementation:**\n",
        "1.  Change the `CONFIG['MODE']` variable below to one of: **`SimpleDQN`**, **`DoubleDQN`**, or **`DuelingDQN`**.\n",
        "2.  Adjust hyperparameters (`LR`, `EPS_DECAY`, etc.) in the `CONFIG` dictionary if needed.\n",
        "3.  Run all cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install \"gymnasium[atari,accept-rom-license,other]\" ale-py\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y xvfb x11-utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
        "\n",
        "# Tools for video display\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from base64 import b64encode\n",
        "\n",
        "# ----------------- GLOBAL CONFIGURATION -----------------\n",
        "CONFIG = {\n",
        "    \"ENV_ID\": 'ALE/SpaceInvaders-v5',\n",
        "    \"SEED\": 7,\n",
        "    \"MODE\": \"SimpleDQN\", # Choice --> 'SimpleDQN', 'DoubleDQN', 'DuelingDQN'\n",
        "    \"INPUT_SHAPE\": (4, 84, 84), # 4 stacked frames, resized to 84x84\n",
        "    \"BUFFER_SIZE\": int(1e5), \n",
        "    \"BATCH_SIZE\": 32, # Reduced batch size (common practice for Atari, hinted in PDF [cite: 37])\n",
        "    \"GAMMA\": 0.99, # Prioritizing long-term cumulative reward\n",
        "    \"TAU\": 1e-3, # Soft Update Rate\n",
        "    \"LR\": 1e-4, # Lower learning rate --> stable convergence\n",
        "    \"UPDATE_EVERY\": 4, # Learn frequency (standard for Atari DQN)\n",
        "    \"TARGET_UPDATE_FREQ\": 1000, \n",
        "    \"N_EPISODES\": 5000, \n",
        "    \"EPS_START\": 1.0, # Initial probability of choosing a random action (exploration) --> fully exploring the environment to gather initial experiences\n",
        "    \"EPS_END\": 0.01, # Minimum probability of choosing a random action.\n",
        "    \"EPS_DECAY\": 0.999 # Exploration rate decays very slowly, allowing the agent to explore over a large number of episodes\n",
        "}\n",
        "# --------------------------------------------------------\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random.seed(CONFIG['SEED'])\n",
        "np.random.seed(CONFIG['SEED'])\n",
        "torch.manual_seed(CONFIG['SEED'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(CONFIG['SEED'])\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Current DQN Mode: {CONFIG['MODE']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_init"
      },
      "source": [
        "## 2. Environment Initialization\n",
        "We use **`AtariPreprocessing`** to handle resizing/cropping to 84x84 and grayscale conversion. **`FrameStack`** then stacks 4 consecutive frames, fulfilling the requirements for the state space[cite: 19, 20]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_run"
      },
      "outputs": [],
      "source": [
        "def make_atari_env(env_id, seed):\n",
        "    \"\"\"Creates and wraps the Atari environment with standard preprocessing and 4-frame stacking.\"\"\"\n",
        "    # 1. Base Environment (Using the required ID [cite: 11])\n",
        "    env = gym.make(env_id)\n",
        "    \n",
        "    # 2. Atari Preprocessing: Resizes to 84x84, grayscale, handles max-pooling/skip.\n",
        "    # Frame skip is set to 1 here because the ALE/SpaceInvaders-v5 environment generally handles skips \n",
        "    # implicitly, or we rely on the standard wrappers' internal logic for compatibility.\n",
        "    env = AtariPreprocessing(env, grayscale_obs=True, terminal_on_life_loss=True, frame_skip=1, screen_size=84)\n",
        "    \n",
        "    # 3. Frame Stacking (Creates the (4, 84, 84) state [cite: 19])\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "    \n",
        "    # Set seed on the final environment\n",
        "    if seed is not None:\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "        \n",
        "    return env\n",
        "\n",
        "env = make_atari_env(CONFIG['ENV_ID'], CONFIG['SEED'])\n",
        "action_size = env.action_space.n \n",
        "state_shape = env.observation_space.shape \n",
        "\n",
        "print(f'Final State shape (Stacked Frames): {state_shape}')\n",
        "print(f'Number of available actions (SpaceInvaders): {action_size}') # Confirms 6 actions [cite: 13, 21]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "network_arch"
      },
      "source": [
        "## 3. Q-Network Architecture\n",
        "The network uses a CNN architecture  to process the high-dimensional image input, supporting Dueling components via a flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNetwork_code"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"CNN-based Q-Network Model supporting Standard and Dueling structures.\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_size, seed, dueling=False):\n",
        "        \"\"\"Initializes the shared CNN layers and splits into Value/Advantage streams if Dueling is enabled.\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.dueling = dueling\n",
        "        in_channels = state_shape[0] # 4 stacked frames\n",
        "        \n",
        "        # --- Shared CNN Layers (Original DQN architecture) ---\n",
        "        # Layers extract features from the 4 stacked 84x84 input images.\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        \n",
        "        # --- Dynamic Calculation of fc_input_size ---\n",
        "        # This prevents the network from breaking if the input image size changes.\n",
        "        # 1. Create a dummy input tensor based on state_shape (e.g., (1, 4, 84, 84))\n",
        "        dummy_input = torch.zeros(1, *state_shape)\n",
        "        \n",
        "        # 2. Pass the dummy input through the convolutional layers\n",
        "        x = self._forward_conv(dummy_input)\n",
        "        \n",
        "        # 3. Calculate the flattened feature size (e.g., 7*7*64 = 3136)\n",
        "        self.fc_input_size = x.view(1, -1).size(1)\n",
        "        \n",
        "        # --- Fully Connected Layers ---\n",
        "        if self.dueling:\n",
        "            # Dueling Architecture: Split into Value (V) and Advantage (A) streams\n",
        "            self.fc_v1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc_a1 = nn.Linear(self.fc_input_size, 512)\n",
        "            \n",
        "            self.fc_v2 = nn.Linear(512, 1) # Output V(s)\n",
        "            self.fc_a2 = nn.Linear(512, action_size) # Output A(s, a)\n",
        "        else:\n",
        "            # Standard DQN Architecture (Single Q-stream)\n",
        "            self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc2 = nn.Linear(512, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Maps state (4, 84, 84) to action values (6).\"\"\"\n",
        "        # CNN forward pass\n",
        "        x = F.relu(self.conv1(state))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        \n",
        "        if self.dueling:\n",
        "        # Dueling Combination: Q(s,a) = V(s) + [A(s,a) - mean(A(s,a))]\n",
        "            v = F.relu(self.fc_v1(x))\n",
        "            a = F.relu(self.fc_a1(x))\n",
        "            v = self.fc_v2(v) \n",
        "            a = self.fc_a2(a) \n",
        "            return v + a - a.mean(1).unsqueeze(1)\n",
        "        else:\n",
        "            # Standard Q-stream\n",
        "            x = F.relu(self.fc1(x))\n",
        "            return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buffer_agents"
      },
      "source": [
        "## 4. Replay Buffer and Agent Implementations\n",
        "The **Replay Buffer** (PER is an optional extension [cite: 27]) is crucial for breaking correlation in experience samples. The **AgentBase** handles common functions; specialized classes implement the specific Q-learning update rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReplayBuffer_code"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples, essential for DQN.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initializes the ReplayBuffer.\"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Adds a new experience (s, a, r, s', done) to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly samples a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        # Convert FrameStack/NumPy data into required Torch tensor shape (B, C, H, W)\n",
        "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.array([e.action for e in experiences if e is not None])).long().unsqueeze(1).to(device)\n",
        "        rewards = torch.from_numpy(np.array([e.reward for e in experiences if e is not None])).float().unsqueeze(1).to(device)\n",
        "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.array([e.done for e in experiences if e is not None]).astype(np.uint8)).float().unsqueeze(1).to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agent_init"
      },
      "outputs": [],
      "source": [
        "class AgentBase:\n",
        "    \"\"\"Base class for all DQN agents, handling shared components and target network logic.\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_size, seed, mode, dueling):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.mode = mode\n",
        "\n",
        "        # Initialize Q-Networks \n",
        "        self.qnetwork_local = QNetwork(state_shape, action_size, seed, dueling=dueling).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_shape, action_size, seed, dueling=dueling).to(device)\n",
        "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=CONFIG['LR'])\n",
        "        self.memory = ReplayBuffer(action_size, CONFIG['BUFFER_SIZE'], CONFIG['BATCH_SIZE'], seed)\n",
        "        \n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY steps\n",
        "        self.t_step = (self.t_step + 1) % CONFIG['UPDATE_EVERY']\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > CONFIG['BATCH_SIZE']:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, CONFIG['GAMMA'])\n",
        "                \n",
        "        # Hard update the target network periodically (standard for Atari)\n",
        "        if self.t_step % CONFIG['TARGET_UPDATE_FREQ'] == 0:\n",
        "           self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Returns action based on epsilon-greedy policy.\"\"\"\n",
        "        state = torch.from_numpy(np.copy(state)).float().unsqueeze(0).to(device)\n",
        "        \n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        # Placeholder, implemented by child classes\n",
        "        pass\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 1: Simple DQN (Original Target Calculation) --- \n",
        "# ----------------------------------------------------------------\n",
        "class SimpleDQNAgent(AgentBase):\n",
        "    \"\"\"Implements the original DQN learning step: Target Q = R + gamma * max_a Q_target(s', a).\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Standard QNetwork (dueling=False)\n",
        "        super().__init__(state_shape, action_size, seed, mode='SimpleDQN', dueling=False)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Target Q calculation uses the max Q-value from the target network directly.\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 2: Double DQN (Decoupled Target Calculation) --- \n",
        "# ----------------------------------------------------------------\n",
        "class DoubleDQNAgent(AgentBase):\n",
        "    \"\"\"Implements the Double DQN learning step: Target Q = R + gamma * Q_target(s', argmax_a Q_local(s', a)).\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Standard QNetwork (dueling=False)\n",
        "        super().__init__(state_shape, action_size, seed, mode='DoubleDQN', dueling=False)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # 1. Action selection from LOCAL network (argmax_a Q_local(s', a))\n",
        "        Q_local_next = self.qnetwork_local(next_states).detach()\n",
        "        best_actions = Q_local_next.max(1)[1].unsqueeze(1)\n",
        "        \n",
        "        # 2. Value estimation from TARGET network (Q_target(s', best_actions))\n",
        "        Q_targets_next = self.qnetwork_target(next_states).gather(1, best_actions).detach()\n",
        "        \n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 3: Dueling DQN (Dueling Architecture + Double Learning Rule) --- \n",
        "# ----------------------------------------------------------------\n",
        "class DuelingDQNAgent(DoubleDQNAgent):\n",
        "    \"\"\"Dueling DQN uses the Dueling architecture and the Double DQN learning rule for stability.\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Dueling QNetwork (dueling=True)\n",
        "        AgentBase.__init__(self, state_shape, action_size, seed, mode='DuelingDQN', dueling=True)\n",
        "    \n",
        "    # Inherits the Double DQN learn() method for stability\n",
        "\n",
        "\n",
        "# --- Agent Initialization based on global CONFIG['MODE'] --- \n",
        "if CONFIG['MODE'] == \"SimpleDQN\":\n",
        "    agent = SimpleDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "elif CONFIG['MODE'] == \"DoubleDQN\":\n",
        "    agent = DoubleDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "elif CONFIG['MODE'] == \"DuelingDQN\":\n",
        "    agent = DuelingDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "else:\n",
        "    raise ValueError(\"Invalid MODE specified in CONFIG.\")\n",
        "\n",
        "print(f\"Initialized agent: {type(agent).__name__} with learning mode: {agent.mode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_loop"
      },
      "source": [
        "## 5. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_code"
      },
      "outputs": [],
      "source": [
        "def dqn(n_episodes=CONFIG['N_EPISODES'], max_t=10000, eps_start=CONFIG['EPS_START'], eps_end=CONFIG['EPS_END'], eps_decay=CONFIG['EPS_DECAY']):\n",
        "    \"\"\"Deep Q-Learning training function.\"\"\"\n",
        "        \n",
        "    scores = []                        \n",
        "    scores_window = deque(maxlen=100)  # Tracks the last 100 scores for the goal check [cite: 28]\n",
        "    eps = eps_start                   \n",
        "    \n",
        "    # Target score: one implementation needs 500+, two need 400+ [cite: 28]\n",
        "    GOAL_SCORE = 400.0 \n",
        "\n",
        "    print(f\"\\nStarting training for {agent.mode}...\")\n",
        "\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        state, info = env.reset(seed=CONFIG['SEED'] if i_episode == 1 else None) \n",
        "        state = np.array(state)\n",
        "        score = 0\n",
        "        \n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            \n",
        "            next_state_raw, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            next_state = np.array(next_state_raw) \n",
        "            reward_np = np.array([reward]).astype(np.float32)\n",
        "            done_np = np.array([done]).astype(np.uint8)\n",
        "            \n",
        "            agent.step(state, action, reward_np, next_state, done_np)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        scores_window.append(score)       \n",
        "        scores.append(score)              \n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "        \n",
        "        avg_score = np.mean(scores_window)\n",
        "\n",
        "        print(f'\\rEpisode {i_episode}\\tAverage Score: {avg_score:.2f}\\tEpsilon: {eps:.4f}', end=\"\")\n",
        "        \n",
        "        if i_episode % 100 == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {avg_score:.2f}\\tEpsilon: {eps:.4f}')\n",
        "            \n",
        "        if avg_score >= GOAL_SCORE:\n",
        "            print(f'\\n{agent.mode} Goal Reached in {i_episode-100} episodes!\\tAverage Score: {avg_score:.2f}')\n",
        "            # Save checkpoint for presentation [cite: 29]\n",
        "            torch.save(agent.qnetwork_local.state_dict(), f'{agent.mode}_{i_episode}.pth')\n",
        "            break\n",
        "            \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- UNCOMMENT TO RUN TRAINING ---\n",
        "# scores = dqn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fig = plt.figure(figsize=(10, 6))\n",
        "# ax = fig.add_subplot(111)\n",
        "# plt.plot(np.arange(len(scores)), scores, label=agent.mode)\n",
        "# plt.title(f'Training Scores ({agent.mode})')\n",
        "# plt.ylabel('Score')\n",
        "# plt.xlabel('Episode #')\n",
        "# plt.grid(True)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video_display"
      },
      "source": [
        "## 6. Video Visualization Utility\n",
        "This function is provided for optional video recording using a trained model's weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "video_code"
      },
      "outputs": [],
      "source": [
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"Gets a string containing a b64-encoded version of the MP4 video.\"\"\"\n",
        "  import os\n",
        "  if not os.path.exists(videopath):\n",
        "      return f\"<p>Video file not found at {videopath}. Run a test episode first.</p>\"\n",
        "      \n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "\n",
        "def run_and_record(env_id, weights_path, mode, seed=CONFIG['SEED'], num_episodes=1):\n",
        "    \"\"\"Runs a specified agent on the environment and records the interaction.\"\"\"\n",
        "    \n",
        "    # 1. Setup Environment\n",
        "    # Use the same wrapper stack as training for consistent state representation\n",
        "    env_render = make_atari_env(env_id, seed=seed)\n",
        "    \n",
        "    # 2. Setup Agent\n",
        "    action_size = env_render.action_space.n\n",
        "    state_shape = env_render.observation_space.shape\n",
        "    \n",
        "    # Use the appropriate Agent class\n",
        "    if mode == \"SimpleDQN\":\n",
        "        test_agent = SimpleDQNAgent(state_shape, action_size, seed)\n",
        "    elif mode == \"DoubleDQN\":\n",
        "        test_agent = DoubleDQNAgent(state_shape, action_size, seed)\n",
        "    elif mode == \"DuelingDQN\":\n",
        "        test_agent = DuelingDQNAgent(state_shape, action_size, seed)\n",
        "    else:\n",
        "        return f\"<p>Invalid MODE specified for testing: {mode}</p>\"\n",
        "    \n",
        "    # 3. Load Weights\n",
        "    try:\n",
        "        test_agent.qnetwork_local.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "        test_agent.qnetwork_local.eval()\n",
        "        print(f\"Successfully loaded {mode} weights from {weights_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Checkpoint file {weights_path} not found. Agent will use random weights.\")\n",
        "        return\n",
        "\n",
        "    # 4. Record Episodes\n",
        "    video_path = f'{mode}_{env_id.split(\"/\")[-1]}_test.mp4'\n",
        "    frames = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env_render.reset(seed=seed)\n",
        "        score = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            # The FrameStack wrapper returns a LazyFrame, convert to NumPy array\n",
        "            state_np = np.array(state)\n",
        "            action = test_agent.act(state_np, eps=0.0) \n",
        "            \n",
        "            # Capture frame (convert to RGB before saving)\n",
        "            frames.append(env_render.render())\n",
        "            \n",
        "            next_state, reward, terminated, truncated, info = env_render.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            \n",
        "        print(f\"Test Episode {episode+1} score: {score:.2f}\")\n",
        "\n",
        "    env_render.close()\n",
        "    \n",
        "    # Save video\n",
        "    imageio.mimsave(video_path, frames, fps=30)\n",
        "    \n",
        "    # Display video\n",
        "    html = render_mp4(video_path)\n",
        "    ipythondisplay.display(ipythondisplay.HTML(html))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage (Uncomment and update weights_path after training):\n",
        "# run_and_record(CONFIG['ENV_ID'], 'SimpleDQN_5000.pth', 'SimpleDQN', num_episodes=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "name": "AIDL_B02_DQN_SpaceInvaders_Variants_Clean.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
